{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "from utils import divide_on_feature, train_test_split, standardize, mean_squared_error\n",
    "from utils import calculate_entropy, accuracy_score, calculate_variance, Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i          # Index for the feature that is tested\n",
    "        self.threshold = threshold          # Threshold value for feature\n",
    "        self.value = value                  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch      # 'Left' subtree\n",
    "        self.false_branch = false_branch    # 'Right' subtree\n",
    "\n",
    "\n",
    "# Super class of RegressionTree and ClassificationTree\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    loss: function\n",
    "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        self._impurity_calculation = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        self._leaf_value_calculation = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None    # Feature index and threshold\n",
    "        best_sets = None        # Subsets of the data\n",
    "\n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
    "                                }\n",
    "\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostRegressionTree(DecisionTree):\n",
    "    \"\"\"\n",
    "    Regression tree for XGBoost\n",
    "    - Reference -\n",
    "    http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    \"\"\"\n",
    "\n",
    "    def _split(self, y):\n",
    "        \"\"\" y contains y_true in left half of the middle column and\n",
    "        y_pred in the right half. Split and return the two matrices \"\"\"\n",
    "        col = int(np.shape(y)[1]/2)\n",
    "        y, y_pred = y[:, :col], y[:, col:]\n",
    "        return y, y_pred\n",
    "\n",
    "    def _gain(self, y, y_pred):\n",
    "        nominator = np.power((y * self.loss.gradient(y, y_pred)).sum(), 2)\n",
    "        denominator = self.loss.hess(y, y_pred).sum()\n",
    "        return 0.5 * (nominator / denominator)\n",
    "\n",
    "    def _gain_by_taylor(self, y, y1, y2):\n",
    "        # Split\n",
    "        y, y_pred = self._split(y)\n",
    "        y1, y1_pred = self._split(y1)\n",
    "        y2, y2_pred = self._split(y2)\n",
    "\n",
    "        true_gain = self._gain(y1, y1_pred)\n",
    "        false_gain = self._gain(y2, y2_pred)\n",
    "        gain = self._gain(y, y_pred)\n",
    "        return true_gain + false_gain - gain\n",
    "\n",
    "    def _approximate_update(self, y):\n",
    "        # y split into y, y_pred\n",
    "        y, y_pred = self._split(y)\n",
    "        # Newton's Method\n",
    "        gradient = np.sum(y * self.loss.gradient(y, y_pred), axis=0)\n",
    "        hessian = np.sum(self.loss.hess(y, y_pred), axis=0)\n",
    "        update_approximation =  gradient / hessian\n",
    "\n",
    "        return update_approximation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._gain_by_taylor\n",
    "        self._leaf_value_calculation = self._approximate_update\n",
    "        super(XGBoostRegressionTree, self).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import progressbar\n",
    "\n",
    "from utils import train_test_split, standardize, to_categorical, normalize\n",
    "from utils import mean_squared_error, accuracy_score\n",
    "from utils import bar_widgets\n",
    "from utils import Plot\n",
    "\n",
    "class Sigmoid():\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "\n",
    "\n",
    "class LogisticLoss():\n",
    "    def __init__(self):\n",
    "        sigmoid = Sigmoid()\n",
    "        self.log_func = sigmoid\n",
    "        self.log_grad = sigmoid.gradient\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        p = self.log_func(y_pred)\n",
    "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
    "\n",
    "    # gradient w.r.t y_pred\n",
    "    def gradient(self, y, y_pred):\n",
    "        p = self.log_func(y_pred)\n",
    "        return -(y - p)\n",
    "\n",
    "    # w.r.t y_pred\n",
    "    def hess(self, y, y_pred):\n",
    "        p = self.log_func(y_pred)\n",
    "        return p * (1 - p)\n",
    "\n",
    "\n",
    "class XGBoost(object):\n",
    "    \"\"\"The XGBoost classifier.\n",
    "\n",
    "    Reference: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators: int\n",
    "        The number of classification trees that are used.\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further. \n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.001, min_samples_split=2,\n",
    "                 min_impurity=1e-7, max_depth=2):\n",
    "        self.n_estimators = n_estimators            # Number of trees\n",
    "        self.learning_rate = learning_rate          # Step size for weight update\n",
    "        self.min_samples_split = min_samples_split  # The minimum n of sampels to justify split\n",
    "        self.min_impurity = min_impurity              # Minimum variance reduction to continue\n",
    "        self.max_depth = max_depth                  # Maximum depth for tree\n",
    "\n",
    "        self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
    "        \n",
    "        # Log loss for classification\n",
    "        self.loss = LogisticLoss()\n",
    "\n",
    "        # Initialize regression trees\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = XGBoostRegressionTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    min_impurity=min_impurity,\n",
    "                    max_depth=self.max_depth,\n",
    "                    loss=self.loss)\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = to_categorical(y)\n",
    "\n",
    "        y_pred = np.zeros(np.shape(y))\n",
    "        for i in self.bar(range(self.n_estimators)):\n",
    "            tree = self.trees[i]\n",
    "            y_and_pred = np.concatenate((y, y_pred), axis=1)\n",
    "            tree.fit(X, y_and_pred)\n",
    "            update_pred = tree.predict(X)\n",
    "\n",
    "            y_pred -= np.multiply(self.learning_rate, update_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = None\n",
    "        # Make predictions\n",
    "        for tree in self.trees:\n",
    "            # Estimate gradient and update prediction\n",
    "            update_pred = tree.predict(X)\n",
    "            if y_pred is None:\n",
    "                y_pred = np.zeros_like(update_pred)\n",
    "            y_pred -= np.multiply(self.learning_rate, update_pred)\n",
    "\n",
    "        # Turn into probability distribution (Softmax)\n",
    "        y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)\n",
    "        # Set label to the value that maximizes probability\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2% [                                                ] ETA:  0:00:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- XGBoost --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% [------------------------------------------------] Time: 0:00:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print (\"-- XGBoost --\")\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, seed=2)  \n",
    "\n",
    "clf = XGBoost()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print (\"Accuracy:\", accuracy)\n",
    "\n",
    "Plot().plot_in_2d(X_test, y_pred, \n",
    "    title=\"XGBoost\", \n",
    "accuracy=accuracy, \n",
    "legend_labels=data.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
