{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import zipfile\n",
    "#filtering reshape warnings deprecation\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"titanic.zip\") as f:\n",
    "    f.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data and checking different variables and their types\n",
    "data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "data.columns\n",
    "len(data['Ticket'].unique())\n",
    "\n",
    "#Dropping id and not useful varibles\n",
    "data.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n",
    "test.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n",
    "data.describe(include='all')\n",
    "data.isnull().sum()\n",
    "trainy =data['Survived']\n",
    "data = data.drop(['Survived'],axis=1)\n",
    "data['Pclass']= pd.Categorical(data['Pclass'])\n",
    "test['Pclass']= pd.Categorical(test['Pclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cabin\n",
       "A10    1\n",
       "A14    1\n",
       "A16    1\n",
       "A19    1\n",
       "A20    1\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() \n",
    "data[data['Age'].isnull()].index\n",
    "data['Age']=data['Age'].fillna(data['Age'].mean())\n",
    "test['Age']=test['Age'].fillna(data['Age'].mean())\n",
    "trainy.groupby(data['Cabin']).count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7f4af97908>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extracting character cabin from string\n",
    "index=pd.Series(data['Cabin'].dropna()).str.slice(0,1).index\n",
    "data['Cabin'][index]=pd.Series(data['Cabin'].dropna()).str.slice(0,1)\n",
    "\n",
    "index=pd.Series(test['Cabin'].dropna()).str.slice(0,1).index\n",
    "test['Cabin'][index]=pd.Series(test['Cabin'].dropna()).str.slice(0,1)\n",
    "\n",
    "#nan behaves differently, more in number, making it a separate class\n",
    "trainy.groupby(data['Cabin'].fillna('dummy')).mean()\n",
    "\n",
    "#Visualization\n",
    "trainy.groupby(data['Sex']).mean().plot(kind='bar',color='green')\n",
    "trainy.groupby(data['Pclass']).mean().plot(kind='bar',color='green')\n",
    "trainy.groupby(data['Embarked']).mean().plot(kind='bar',color='green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing sparse categories\n",
    "data['Cabin']=data['Cabin'].replace(['T'],['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to one hot encoding of data\n",
    "data =pd.get_dummies(data,dummy_na=False,drop_first=True)\n",
    "test =pd.get_dummies(test,dummy_na=False,drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data for train test validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainx, testx, trainy, testy = train_test_split(data, trainy, test_size=0.3, random_state=42)\n",
    "\n",
    "#Data columns are at different scale, import to normalize before appling gradient descent optim \n",
    "def normalize_and_stackintercept(trainx,testx):\n",
    "    traindatax=trainx\n",
    "    testdatax=testx\n",
    "    mu= traindatax.mean(axis=0)\n",
    "    sigma= traindatax.std(axis=0)\n",
    "    traindatax= (traindatax-mu)/sigma\n",
    "    traindatax = np.array(traindatax)    \n",
    "    traindatax=np.hstack([np.ones((traindatax.shape[0],1)),traindatax])\n",
    "    testdatax = (testdatax-mu)/sigma\n",
    "    testdatax = np.array(testdatax)    \n",
    "    testdatax=np.hstack([np.ones((testdatax.shape[0],1)),testdatax])    \n",
    "    return traindatax,testdatax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data columns are at different scale, import to normalize before appling gradient descent optim\n",
    "#stacking 1 for intercept term\n",
    "trainx,testx= normalize_and_stackintercept(trainx,testx)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    prob=1.0/(1+np.exp(-x))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running loops for gradient descent optimization\n",
    "    \n",
    "def Gradient_Descent(trainx,trainy,testx,testy,lr=0.05,reg=0,iters=1000):\n",
    "    train_m = trainx.shape[0]\n",
    "    test_m = testx.shape[0]\n",
    "    trainy=trainy.reshape(train_m,1)\n",
    "    testy=testy.reshape(test_m,1)\n",
    "    n = trainx.shape[1]\n",
    "    coeff= np.zeros((n,1))\n",
    "    lr=lr\n",
    "    reg=reg\n",
    "    train_Loss=[]\n",
    "    test_Loss=[]\n",
    "\n",
    "    for iterations in range(iters):\n",
    "        train_prediction= sigmoid(trainx.dot(coeff))\n",
    "        test_prediction= sigmoid(testx.dot(coeff))\n",
    "        train_loss= -np.sum(trainy*np.log(train_prediction)+(1-trainy)*np.log(1-train_prediction))/train_m + 0.5*reg*np.sum(coeff**2)/train_m\n",
    "        test_loss=  -np.sum(testy*np.log(test_prediction)+(1-testy)*np.log(1-test_prediction))/test_m + 0.5*reg*np.sum(coeff**2)/test_m\n",
    "        print(\"iteration \"+str(iterations)+\"train_loss: \"+str(train_loss) +\" test_loss: \"+str(test_loss))\n",
    "        train_Loss.append(train_loss)\n",
    "        test_Loss.append(test_loss) \n",
    "        gradient=(trainx.T).dot(train_prediction-trainy)/train_m\n",
    "        coeff= coeff-lr*(gradient)-(reg*coeff/train_m)\n",
    "    Loss_dict=pd.DataFrame({'train_Loss':train_Loss,'test_Loss':test_Loss})   \n",
    "    return coeff,Loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0train_loss: 0.6931471805599454 test_loss: 0.6931471805599453\n",
      "iteration 1train_loss: 0.6856026094181301 test_loss: 0.6855731289907518\n",
      "iteration 2train_loss: 0.678349532635492 test_loss: 0.6783073760581364\n",
      "iteration 3train_loss: 0.6713751698895234 test_loss: 0.6713360930395187\n",
      "iteration 4train_loss: 0.664667203002512 test_loss: 0.6646459470192878\n",
      "iteration 5train_loss: 0.6582137838135371 test_loss: 0.6582241119226983\n",
      "iteration 6train_loss: 0.6520035372280184 test_loss: 0.6520582739474637\n",
      "iteration 7train_loss: 0.6460255601224087 test_loss: 0.6461366321403018\n",
      "iteration 8train_loss: 0.6402694167469588 test_loss: 0.6404478948368094\n",
      "iteration 9train_loss: 0.6347251312233464 test_loss: 0.6349812726391036\n",
      "iteration 10train_loss: 0.6293831776808037 test_loss: 0.6297264685517996\n",
      "iteration 11train_loss: 0.6242344685178158 test_loss: 0.6246736658373772\n",
      "iteration 12train_loss: 0.619270341219347 test_loss: 0.6198135140903277\n",
      "iteration 13train_loss: 0.614482544103986 test_loss: 0.6151371139682668\n",
      "iteration 14train_loss: 0.6098632213228615 test_loss: 0.6106360009594279\n",
      "iteration 15train_loss: 0.6054048973835918 test_loss: 0.6063021285108774\n",
      "iteration 16train_loss: 0.6011004614284347 test_loss: 0.6021278507912545\n",
      "iteration 17train_loss: 0.5969431514563909 test_loss: 0.5981059053162242\n",
      "iteration 18train_loss: 0.5929265386442513 test_loss: 0.5942293956242581\n",
      "iteration 19train_loss: 0.5890445118912688 test_loss: 0.5904917741547233\n",
      "iteration 20train_loss: 0.5852912626860072 test_loss: 0.5868868254493077\n",
      "iteration 21train_loss: 0.5816612703715726 test_loss: 0.583408649771205\n",
      "iteration 22train_loss: 0.578149287866535 test_loss: 0.5800516472138343\n",
      "iteration 23train_loss: 0.5747503278829694 test_loss: 0.5768105023517597\n",
      "iteration 24train_loss: 0.5714596496698504 test_loss: 0.5736801694705014\n",
      "iteration 25train_loss: 0.5682727462991377 test_loss: 0.5706558583987091\n",
      "iteration 26train_loss: 0.5651853325030083 test_loss: 0.5677330209553108\n",
      "iteration 27train_loss: 0.5621933330634994 test_loss: 0.5649073380154509\n",
      "iteration 28train_loss: 0.559292871750089 test_loss: 0.5621747071919736\n",
      "iteration 29train_loss: 0.5564802607962293 test_loss: 0.5595312311236178\n",
      "iteration 30train_loss: 0.5537519909023483 test_loss: 0.5569732063567627\n",
      "iteration 31train_loss: 0.5511047217502014 test_loss: 0.5544971128042507\n",
      "iteration 32train_loss: 0.5485352730115205 test_loss: 0.5520996037623891\n",
      "iteration 33train_loss: 0.5460406158325473 test_loss: 0.549777496465496\n",
      "iteration 34train_loss: 0.5436178647751737 test_loss: 0.5475277631562215\n",
      "iteration 35train_loss: 0.5412642701949018 test_loss: 0.5453475226492013\n",
      "iteration 36train_loss: 0.5389772110356645 test_loss: 0.5432340323653071\n",
      "iteration 37train_loss: 0.5367541880216035 test_loss: 0.5411846808137848\n",
      "iteration 38train_loss: 0.5345928172261559 test_loss: 0.539196980499813\n",
      "iteration 39train_loss: 0.5324908239992079 test_loss: 0.5372685612354589\n",
      "iteration 40train_loss: 0.5304460372335889 test_loss: 0.5353971638325908\n",
      "iteration 41train_loss: 0.5284563839527849 test_loss: 0.5335806341569888\n",
      "iteration 42train_loss: 0.5265198842024084 test_loss: 0.5318169175236483\n",
      "iteration 43train_loss: 0.5246346462286614 test_loss: 0.530104053414096\n",
      "iteration 44train_loss: 0.5227988619277611 test_loss: 0.5284401704973529\n",
      "iteration 45train_loss: 0.5210108025510208 test_loss: 0.5268234819370564\n",
      "iteration 46train_loss: 0.5192688146510271 test_loss: 0.5252522809680952\n",
      "iteration 47train_loss: 0.5175713162550775 test_loss: 0.5237249367269581\n",
      "iteration 48train_loss: 0.5159167932527559 test_loss: 0.5222398903208401\n",
      "iteration 49train_loss: 0.5143037959852289 test_loss: 0.5207956511213491\n",
      "iteration 50train_loss: 0.5127309360245147 test_loss: 0.5193907932694485\n",
      "iteration 51train_loss: 0.5111968831316291 test_loss: 0.5180239523790261\n",
      "iteration 52train_loss: 0.5097003623831431 test_loss: 0.5166938224272052\n",
      "iteration 53train_loss: 0.5082401514562783 test_loss: 0.5153991528202096\n",
      "iteration 54train_loss: 0.5068150780632469 test_loss: 0.5141387456242548\n",
      "iteration 55train_loss: 0.5054240175260731 test_loss: 0.5129114529515653\n",
      "iteration 56train_loss: 0.5040658904836658 test_loss: 0.5117161744922167\n",
      "iteration 57train_loss: 0.5027396607233842 test_loss: 0.5105518551830668\n",
      "iteration 58train_loss: 0.5014443331298162 test_loss: 0.5094174830055659\n",
      "iteration 59train_loss: 0.5001789517439136 test_loss: 0.5083120869047512\n",
      "iteration 60train_loss: 0.4989425979260515 test_loss: 0.507234734822194\n",
      "iteration 61train_loss: 0.49773438861695907 test_loss: 0.5061845318361209\n",
      "iteration 62train_loss: 0.4965534746908438 test_loss: 0.5051606184023479\n",
      "iteration 63train_loss: 0.4953990393953673 test_loss: 0.5041621686900607\n",
      "iteration 64train_loss: 0.49427029687346286 test_loss: 0.5031883890068454\n",
      "iteration 65train_loss: 0.49316649076228336 test_loss: 0.5022385163077211\n",
      "iteration 66train_loss: 0.49208689286485807 test_loss: 0.5013118167832515\n",
      "iteration 67train_loss: 0.4910308018903031 test_loss: 0.500407584522118\n",
      "iteration 68train_loss: 0.4899975422586852 test_loss: 0.4995251402438245\n",
      "iteration 69train_loss: 0.48898646296687004 test_loss: 0.49866383009747134\n",
      "iteration 70train_loss: 0.4879969365119162 test_loss: 0.4978230245227858\n",
      "iteration 71train_loss: 0.4870283578687746 test_loss: 0.4970021171698352\n",
      "iteration 72train_loss: 0.4860801435192545 test_loss: 0.49620052387406566\n",
      "iteration 73train_loss: 0.48515173052940136 test_loss: 0.49541768168351713\n",
      "iteration 74train_loss: 0.48424257567259554 test_loss: 0.4946530479352597\n",
      "iteration 75train_loss: 0.4833521545958504 test_loss: 0.4939060993782741\n",
      "iteration 76train_loss: 0.4824799610269346 test_loss: 0.4931763313401706\n",
      "iteration 77train_loss: 0.48162550602008386 test_loss: 0.4924632569352985\n",
      "iteration 78train_loss: 0.480788317238203 test_loss: 0.4917664063119461\n",
      "iteration 79train_loss: 0.47996793826958223 test_loss: 0.4910853259364702\n",
      "iteration 80train_loss: 0.47916392797726487 test_loss: 0.49041957791232355\n",
      "iteration 81train_loss: 0.47837585987931824 test_loss: 0.4897687393320731\n",
      "iteration 82train_loss: 0.47760332155835844 test_loss: 0.4891324016606091\n",
      "iteration 83train_loss: 0.47684591409877525 test_loss: 0.488510170147862\n",
      "iteration 84train_loss: 0.4761032515501969 test_loss: 0.4879016632694332\n",
      "iteration 85train_loss: 0.4753749604158142 test_loss: 0.4873065121936474\n",
      "iteration 86train_loss: 0.4746606791642674 test_loss: 0.4867243602736192\n",
      "iteration 87train_loss: 0.47396005776387085 test_loss: 0.48615486256300744\n",
      "iteration 88train_loss: 0.47327275723802015 test_loss: 0.4855976853542092\n",
      "iteration 89train_loss: 0.47259844924069405 test_loss: 0.4850525057378207\n",
      "iteration 90train_loss: 0.4719368156510241 test_loss: 0.4845190111822521\n",
      "iteration 91train_loss: 0.47128754818596064 test_loss: 0.48399689913245725\n",
      "iteration 92train_loss: 0.47065034803012223 test_loss: 0.4834858766267891\n",
      "iteration 93train_loss: 0.4700249254819637 test_loss: 0.4829856599310562\n",
      "iteration 94train_loss: 0.46941099961544536 test_loss: 0.4824959741888996\n",
      "iteration 95train_loss: 0.4688082979564347 test_loss: 0.482016553087668\n",
      "iteration 96train_loss: 0.46821655617311025 test_loss: 0.4815471385390073\n",
      "iteration 97train_loss: 0.4676355177796793 test_loss: 0.4810874803734293\n",
      "iteration 98train_loss: 0.4670649338527587 test_loss: 0.480637336048163\n",
      "iteration 99train_loss: 0.46650456275980257 test_loss: 0.48019647036763025\n",
      "iteration 100train_loss: 0.4659541698989937 test_loss: 0.47976465521592665\n",
      "iteration 101train_loss: 0.46541352745004816 test_loss: 0.4793416693007164\n",
      "iteration 102train_loss: 0.46488241413541054 test_loss: 0.4789272979079897\n",
      "iteration 103train_loss: 0.4643606149913462 test_loss: 0.47852133266715363\n",
      "iteration 104train_loss: 0.46384792114846113 test_loss: 0.4781235713259621\n",
      "iteration 105train_loss: 0.46334412962120836 test_loss: 0.47773381753481287\n",
      "iteration 106train_loss: 0.4628490431059579 test_loss: 0.4773518806399658\n",
      "iteration 107train_loss: 0.46236246978723405 test_loss: 0.47697757548526254\n",
      "iteration 108train_loss: 0.46188422315174155 test_loss: 0.4766107222219453\n",
      "iteration 109train_loss: 0.461414121809822 test_loss: 0.47625114612619907\n",
      "iteration 110train_loss: 0.4609519893240019 test_loss: 0.4758986774240552\n",
      "iteration 111train_loss: 0.46049765404430865 test_loss: 0.47555315112331925\n",
      "iteration 112train_loss: 0.460050948950049 test_loss: 0.4752144068521982\n",
      "iteration 113train_loss: 0.4596117114977595 test_loss: 0.4748822887043222\n",
      "iteration 114train_loss: 0.45917978347505184 test_loss: 0.47455664508987\n",
      "iteration 115train_loss: 0.45875501086009357 test_loss: 0.47423732859252266\n",
      "iteration 116train_loss: 0.4583372436864711 test_loss: 0.47392419583198314\n",
      "iteration 117train_loss: 0.45792633591320187 test_loss: 0.4736171073318148\n",
      "iteration 118train_loss: 0.4575221452996682 test_loss: 0.47331592739236034\n",
      "iteration 119train_loss: 0.45712453328526 test_loss: 0.4730205239685202\n",
      "iteration 120train_loss: 0.4567333648735219 test_loss: 0.47273076855217355\n",
      "iteration 121train_loss: 0.4563485085206113 test_loss: 0.47244653605904186\n",
      "iteration 122train_loss: 0.4559698360278834 test_loss: 0.4721677047198011\n",
      "iteration 123train_loss: 0.45559722243842654 test_loss: 0.4718941559752598\n",
      "iteration 124train_loss: 0.45523054593738177 test_loss: 0.47162577437542785\n",
      "iteration 125train_loss: 0.4548696877558868 test_loss: 0.4713624474823105\n",
      "iteration 126train_loss: 0.4545145320784929 test_loss: 0.47110406577626923\n",
      "iteration 127train_loss: 0.45416496595391 test_loss: 0.47085052256579996\n",
      "iteration 128train_loss: 0.45382087920894193 test_loss: 0.4706017139005828\n",
      "iteration 129train_loss: 0.4534821643654817 test_loss: 0.47035753848766976\n",
      "iteration 130train_loss: 0.45314871656043937 test_loss: 0.47011789761067846\n",
      "iteration 131train_loss: 0.4528204334684856 test_loss: 0.4698826950518677\n",
      "iteration 132train_loss: 0.4524972152274942 test_loss: 0.4696518370169773\n",
      "iteration 133train_loss: 0.4521789643665769 test_loss: 0.46942523206271985\n",
      "iteration 134train_loss: 0.45186558573660573 test_loss: 0.46920279102681406\n",
      "iteration 135train_loss: 0.4515569864431235 test_loss: 0.46898442696046144\n",
      "iteration 136train_loss: 0.4512530757815475 test_loss: 0.46877005506316405\n",
      "iteration 137train_loss: 0.4509537651745781 test_loss: 0.4685595926197918\n",
      "iteration 138train_loss: 0.4506589681117217 test_loss: 0.4683529589398101\n",
      "iteration 139train_loss: 0.45036860009084967 test_loss: 0.46815007529858194\n",
      "iteration 140train_loss: 0.4500825785617117 test_loss: 0.46795086488066223\n",
      "iteration 141train_loss: 0.4498008228713286 test_loss: 0.46775525272500845\n",
      "iteration 142train_loss: 0.4495232542111926 test_loss: 0.4675631656720311\n",
      "iteration 143train_loss: 0.44924979556620565 test_loss: 0.4673745323124135\n",
      "iteration 144train_loss: 0.44898037166529065 test_loss: 0.46718928293763345\n",
      "iteration 145train_loss: 0.44871490893361066 test_loss: 0.4670073494921206\n",
      "iteration 146train_loss: 0.44845333544633664 test_loss: 0.4668286655269884\n",
      "iteration 147train_loss: 0.4481955808839062 test_loss: 0.46665316615527935\n",
      "iteration 148train_loss: 0.44794157648871596 test_loss: 0.4664807880086684\n",
      "iteration 149train_loss: 0.4476912550231968 test_loss: 0.46631146919556804\n",
      "iteration 150train_loss: 0.4474445507292186 test_loss: 0.46614514926058537\n",
      "iteration 151train_loss: 0.4472013992887782 test_loss: 0.46598176914527695\n",
      "iteration 152train_loss: 0.44696173778592024 test_loss: 0.4658212711501576\n",
      "iteration 153train_loss: 0.44672550466985134 test_loss: 0.4656635988979151\n",
      "iteration 154train_loss: 0.446492639719198 test_loss: 0.4655086972977864\n",
      "iteration 155train_loss: 0.44626308400737263 test_loss: 0.46535651251105425\n",
      "iteration 156train_loss: 0.44603677986900564 test_loss: 0.46520699191762327\n",
      "iteration 157train_loss: 0.44581367086740503 test_loss: 0.4650600840836357\n",
      "iteration 158train_loss: 0.4455937017630085 test_loss: 0.4649157387300911\n",
      "iteration 159train_loss: 0.4453768184827927 test_loss: 0.464773906702434\n",
      "iteration 160train_loss: 0.44516296809060485 test_loss: 0.46463453994107284\n",
      "iteration 161train_loss: 0.4449520987583868 test_loss: 0.46449759145280145\n",
      "iteration 162train_loss: 0.4447441597382583 test_loss: 0.46436301528308704\n",
      "iteration 163train_loss: 0.4445391013354311 test_loss: 0.46423076648919837\n",
      "iteration 164train_loss: 0.44433687488192447 test_loss: 0.4641008011141409\n",
      "iteration 165train_loss: 0.4441374327110557 test_loss: 0.4639730761613754\n",
      "iteration 166train_loss: 0.44394072813267804 test_loss: 0.4638475495702884\n",
      "iteration 167train_loss: 0.4437467154091416 test_loss: 0.46372418019239237\n",
      "iteration 168train_loss: 0.443555349731952 test_loss: 0.46360292776822926\n",
      "iteration 169train_loss: 0.44336658719910427 test_loss: 0.46348375290495303\n",
      "iteration 170train_loss: 0.4431803847930688 test_loss: 0.4633666170545691\n",
      "iteration 171train_loss: 0.44299670035940747 test_loss: 0.46325148249280856\n",
      "iteration 172train_loss: 0.4428154925859997 test_loss: 0.46313831229861474\n",
      "iteration 173train_loss: 0.4426367209828576 test_loss: 0.46302707033422397\n",
      "iteration 174train_loss: 0.4424603458625121 test_loss: 0.46291772122581826\n",
      "iteration 175train_loss: 0.44228632832094894 test_loss: 0.46281023034473234\n",
      "iteration 176train_loss: 0.4421146302190801 test_loss: 0.46270456378919766\n",
      "iteration 177train_loss: 0.4419452141647301 test_loss: 0.4626006883666037\n",
      "iteration 178train_loss: 0.44177804349512306 test_loss: 0.4624985715762611\n",
      "iteration 179train_loss: 0.441613082259852 test_loss: 0.4623981815926507\n",
      "iteration 180train_loss: 0.44145029520431783 test_loss: 0.4622994872491412\n",
      "iteration 181train_loss: 0.4412896477536207 test_loss: 0.4622024580221624\n",
      "iteration 182train_loss: 0.44113110599688976 test_loss: 0.4621070640158172\n",
      "iteration 183train_loss: 0.44097463667204045 test_loss: 0.4620132759469205\n",
      "iteration 184train_loss: 0.44082020715093984 test_loss: 0.4619210651304503\n",
      "iteration 185train_loss: 0.4406677854249743 test_loss: 0.461830403465398\n",
      "iteration 186train_loss: 0.4405173400910017 test_loss: 0.46174126342100535\n",
      "iteration 187train_loss: 0.44036884033767876 test_loss: 0.4616536180233769\n",
      "iteration 188train_loss: 0.4402222559321517 test_loss: 0.46156744084245444\n",
      "iteration 189train_loss: 0.440077557207098 test_loss: 0.46148270597934427\n",
      "iteration 190train_loss: 0.4399347150481094 test_loss: 0.4613993880539844\n",
      "iteration 191train_loss: 0.4397937008814071 test_loss: 0.46131746219314296\n",
      "iteration 192train_loss: 0.43965448666187495 test_loss: 0.46123690401873607\n",
      "iteration 193train_loss: 0.4395170448614062 test_loss: 0.4611576896364568\n",
      "iteration 194train_loss: 0.4393813484575502 test_loss: 0.4610797956247053\n",
      "iteration 195train_loss: 0.43924737092245364 test_loss: 0.4610031990238098\n",
      "iteration 196train_loss: 0.4391150862120829 test_loss: 0.46092787732553225\n",
      "iteration 197train_loss: 0.43898446875572533 test_loss: 0.4608538084628474\n",
      "iteration 198train_loss: 0.438855493445755 test_loss: 0.4607809707999887\n",
      "iteration 199train_loss: 0.4387281356276596 test_loss: 0.46070934312275286\n",
      "iteration 200train_loss: 0.43860237109031913 test_loss: 0.46063890462905477\n",
      "iteration 201train_loss: 0.43847817605652867 test_loss: 0.4605696349197264\n",
      "iteration 202train_loss: 0.4383555271737585 test_loss: 0.46050151398955147\n",
      "iteration 203train_loss: 0.4382344015051456 test_loss: 0.4604345222185294\n",
      "iteration 204train_loss: 0.4381147765207092 test_loss: 0.4603686403633623\n",
      "iteration 205train_loss: 0.4379966300887818 test_loss: 0.4603038495491577\n",
      "iteration 206train_loss: 0.43787994046765505 test_loss: 0.46024013126134133\n",
      "iteration 207train_loss: 0.43776468629742715 test_loss: 0.46017746733777376\n",
      "iteration 208train_loss: 0.4376508465920529 test_loss: 0.46011583996106487\n",
      "iteration 209train_loss: 0.43753840073158523 test_loss: 0.4600552316510804\n",
      "iteration 210train_loss: 0.43742732845460713 test_loss: 0.45999562525763626\n",
      "iteration 211train_loss: 0.4373176098508446 test_loss: 0.45993700395337284\n",
      "iteration 212train_loss: 0.43720922535396 test_loss: 0.45987935122680723\n",
      "iteration 213train_loss: 0.4371021557345158 test_loss: 0.45982265087555546\n",
      "iteration 214train_loss: 0.4369963820931092 test_loss: 0.4597668869997225\n",
      "iteration 215train_loss: 0.43689188585366856 test_loss: 0.45971204399545335\n",
      "iteration 216train_loss: 0.4367886487569099 test_loss: 0.45965810654864164\n",
      "iteration 217train_loss: 0.43668665285394764 test_loss: 0.4596050596287918\n",
      "iteration 218train_loss: 0.43658588050005587 test_loss: 0.45955288848302933\n",
      "iteration 219train_loss: 0.43648631434857754 test_loss: 0.4595015786302559\n",
      "iteration 220train_loss: 0.43638793734497333 test_loss: 0.4594511158554459\n",
      "iteration 221train_loss: 0.4362907327210126 test_loss: 0.45940148620407856\n",
      "iteration 222train_loss: 0.436194683989096 test_loss: 0.4593526759767042\n",
      "iteration 223train_loss: 0.43609977493671226 test_loss: 0.45930467172363965\n",
      "iteration 224train_loss: 0.43600598962102055 test_loss: 0.45925746023978964\n",
      "iteration 225train_loss: 0.43591331236355896 test_loss: 0.45921102855959056\n",
      "iteration 226train_loss: 0.4358217277450731 test_loss: 0.45916536395207413\n",
      "iteration 227train_loss: 0.43573122060046293 test_loss: 0.45912045391604667\n",
      "iteration 228train_loss: 0.4356417760138455 test_loss: 0.45907628617538127\n",
      "iteration 229train_loss: 0.4355533793137269 test_loss: 0.4590328486744204\n",
      "iteration 230train_loss: 0.43546601606828644 test_loss: 0.45899012957348534\n",
      "iteration 231train_loss: 0.43537967208076445 test_loss: 0.4589481172444897\n",
      "iteration 232train_loss: 0.4352943333849549 test_loss: 0.45890680026665465\n",
      "iteration 233train_loss: 0.43520998624079765 test_loss: 0.4588661674223232\n",
      "iteration 234train_loss: 0.43512661713007 test_loss: 0.4588262076928701\n",
      "iteration 235train_loss: 0.43504421275217275 test_loss: 0.4587869102547058\n",
      "iteration 236train_loss: 0.4349627600200097 test_loss: 0.4587482644753722\n",
      "iteration 237train_loss: 0.43488224605595915 test_loss: 0.4587102599097267\n",
      "iteration 238train_loss: 0.4348026581879316 test_loss: 0.4586728862962124\n",
      "iteration 239train_loss: 0.434723983945516 test_loss: 0.4586361335532145\n",
      "iteration 240train_loss: 0.4346462110562083 test_loss: 0.4585999917754968\n",
      "iteration 241train_loss: 0.43456932744172366 test_loss: 0.4585644512307201\n",
      "iteration 242train_loss: 0.4344933212143872 test_loss: 0.45852950235603757\n",
      "iteration 243train_loss: 0.43441818067360305 test_loss: 0.45849513575476686\n",
      "iteration 244train_loss: 0.4343438943024003 test_loss: 0.45846134219313567\n",
      "iteration 245train_loss: 0.4342704507640516 test_loss: 0.4584281125971011\n",
      "iteration 246train_loss: 0.43419783889876573 test_loss: 0.45839543804923755\n",
      "iteration 247train_loss: 0.4341260477204489 test_loss: 0.458363309785695\n",
      "iteration 248train_loss: 0.43405506641353664 test_loss: 0.45833171919322335\n",
      "iteration 249train_loss: 0.4339848843298914 test_loss: 0.4583006578062624\n",
      "iteration 250train_loss: 0.43391549098576665 test_loss: 0.45827011730409584\n",
      "iteration 251train_loss: 0.4338468760588342 test_loss: 0.4582400895080672\n",
      "iteration 252train_loss: 0.4337790293852744 test_loss: 0.45821056637885543\n",
      "iteration 253train_loss: 0.4337119409569266 test_loss: 0.4581815400138119\n",
      "iteration 254train_loss: 0.43364560091849996 test_loss: 0.45815300264435244\n",
      "iteration 255train_loss: 0.43357999956484117 test_loss: 0.4581249466334077\n",
      "iteration 256train_loss: 0.43351512733825964 test_loss: 0.45809736447292704\n",
      "iteration 257train_loss: 0.4334509748259075 test_loss: 0.458070248781437\n",
      "iteration 258train_loss: 0.4333875327572134 test_loss: 0.45804359230165115\n",
      "iteration 259train_loss: 0.43332479200137003 test_loss: 0.4580173878981315\n",
      "iteration 260train_loss: 0.43326274356487127 test_loss: 0.4579916285549997\n",
      "iteration 261train_loss: 0.4332013785891013 test_loss: 0.4579663073736963\n",
      "iteration 262train_loss: 0.4331406883479718 test_loss: 0.4579414175707881\n",
      "iteration 263train_loss: 0.43308066424560765 test_loss: 0.45791695247582104\n",
      "iteration 264train_loss: 0.4330212978140786 test_loss: 0.4578929055292191\n",
      "iteration 265train_loss: 0.4329625807111778 test_loss: 0.4578692702802266\n",
      "iteration 266train_loss: 0.4329045047182441 test_loss: 0.45784604038489424\n",
      "iteration 267train_loss: 0.4328470617380279 test_loss: 0.45782320960410583\n",
      "iteration 268train_loss: 0.43279024379260095 test_loss: 0.45780077180164824\n",
      "iteration 269train_loss: 0.4327340430213055 test_loss: 0.45777872094231975\n",
      "iteration 270train_loss: 0.4326784516787472 test_loss: 0.4577570510900778\n",
      "iteration 271train_loss: 0.4326234621328247 test_loss: 0.4577357564062257\n",
      "iteration 272train_loss: 0.43256906686280033 test_loss: 0.45771483114763595\n",
      "iteration 273train_loss: 0.4325152584574077 test_loss: 0.45769426966501037\n",
      "iteration 274train_loss: 0.4324620296129967 test_loss: 0.4576740664011761\n",
      "iteration 275train_loss: 0.43240937313171535 test_loss: 0.457654215889416\n",
      "iteration 276train_loss: 0.432357281919726 test_loss: 0.4576347127518334\n",
      "iteration 277train_loss: 0.4323057489854576 test_loss: 0.4576155516977495\n",
      "iteration 278train_loss: 0.4322547674378908 test_loss: 0.45759672752213465\n",
      "iteration 279train_loss: 0.4322043304848765 test_loss: 0.45757823510406953\n",
      "iteration 280train_loss: 0.43215443143148785 test_loss: 0.4575600694052386\n",
      "iteration 281train_loss: 0.432105063678402 test_loss: 0.4575422254684537\n",
      "iteration 282train_loss: 0.43205622072031513 test_loss: 0.457524698416207\n",
      "iteration 283train_loss: 0.43200789614438595 test_loss: 0.4575074834492529\n",
      "iteration 284train_loss: 0.4319600836287099 test_loss: 0.45749057584521874\n",
      "iteration 285train_loss: 0.431912776940823 test_loss: 0.4574739709572425\n",
      "iteration 286train_loss: 0.4318659699362315 test_loss: 0.45745766421263795\n",
      "iteration 287train_loss: 0.43181965655697335 test_loss: 0.4574416511115865\n",
      "iteration 288train_loss: 0.4317738308302026 test_loss: 0.4574259272258544\n",
      "iteration 289train_loss: 0.4317284868668042 test_loss: 0.4574104881975353\n",
      "iteration 290train_loss: 0.43168361886003176 test_loss: 0.45739532973781816\n",
      "iteration 291train_loss: 0.4316392210841729 test_loss: 0.45738044762577856\n",
      "iteration 292train_loss: 0.4315952878932387 test_loss: 0.4573658377071941\n",
      "iteration 293train_loss: 0.43155181371967744 test_loss: 0.457351495893383\n",
      "iteration 294train_loss: 0.43150879307311235 test_loss: 0.45733741816006485\n",
      "iteration 295train_loss: 0.4314662205391032 test_loss: 0.4573236005462443\n",
      "iteration 296train_loss: 0.43142409077793 test_loss: 0.4573100391531156\n",
      "iteration 297train_loss: 0.43138239852339966 test_loss: 0.4572967301429886\n",
      "iteration 298train_loss: 0.43134113858167494 test_loss: 0.45728366973823587\n",
      "iteration 299train_loss: 0.431300305830123 test_loss: 0.45727085422025976\n",
      "iteration 300train_loss: 0.431259895216188 test_loss: 0.45725827992847873\n",
      "iteration 301train_loss: 0.43121990175628166 test_loss: 0.45724594325933426\n",
      "iteration 302train_loss: 0.43118032053469474 test_loss: 0.4572338406653163\n",
      "iteration 303train_loss: 0.43114114670252945 test_loss: 0.4572219686540066\n",
      "iteration 304train_loss: 0.4311023754766491 test_loss: 0.45721032378714127\n",
      "iteration 305train_loss: 0.43106400213864904 test_loss: 0.45719890267969043\n",
      "iteration 306train_loss: 0.43102602203384355 test_loss: 0.4571877019989558\n",
      "iteration 307train_loss: 0.43098843057027353 test_loss: 0.457176718463685\n",
      "iteration 308train_loss: 0.43095122321772966 test_loss: 0.45716594884320266\n",
      "iteration 309train_loss: 0.430914395506795 test_loss: 0.45715538995655775\n",
      "iteration 310train_loss: 0.43087794302790294 test_loss: 0.4571450386716873\n",
      "iteration 311train_loss: 0.4308418614304133 test_loss: 0.4571348919045953\n",
      "iteration 312train_loss: 0.43080614642170356 test_loss: 0.4571249466185474\n",
      "iteration 313train_loss: 0.43077079376627736 test_loss: 0.4571151998232802\n",
      "iteration 314train_loss: 0.430735799284888 test_loss: 0.4571056485742254\n",
      "iteration 315train_loss: 0.43070115885367694 test_loss: 0.45709628997174867\n",
      "iteration 316train_loss: 0.43066686840332885 test_loss: 0.45708712116040223\n",
      "iteration 317train_loss: 0.4306329239182406 test_loss: 0.4570781393281911\n",
      "iteration 318train_loss: 0.4305993214357038 test_loss: 0.45706934170585367\n",
      "iteration 319train_loss: 0.4305660570451037 test_loss: 0.4570607255661545\n",
      "iteration 320train_loss: 0.43053312688713025 test_loss: 0.4570522882231909\n",
      "iteration 321train_loss: 0.4305005271530035 test_loss: 0.4570440270317117\n",
      "iteration 322train_loss: 0.4304682540837131 test_loss: 0.45703593938644893\n",
      "iteration 323train_loss: 0.4304363039692688 test_loss: 0.45702802272146115\n",
      "iteration 324train_loss: 0.430404673147967 test_loss: 0.4570202745094893\n",
      "iteration 325train_loss: 0.43037335800566673 test_loss: 0.4570126922613236\n",
      "iteration 326train_loss: 0.4303423549750803 test_loss: 0.45700527352518305\n",
      "iteration 327train_loss: 0.4303116605350747 test_loss: 0.45699801588610467\n",
      "iteration 328train_loss: 0.43028127120998605 test_loss: 0.4569909169653445\n",
      "iteration 329train_loss: 0.4302511835689448 test_loss: 0.4569839744197903\n",
      "iteration 330train_loss: 0.43022139422521266 test_loss: 0.45697718594138265\n",
      "iteration 331train_loss: 0.4301918998355308 test_loss: 0.45697054925654873\n",
      "iteration 332train_loss: 0.4301626970994795 test_loss: 0.4569640621256449\n",
      "iteration 333train_loss: 0.4301337827588477 test_loss: 0.45695772234240917\n",
      "iteration 334train_loss: 0.4301051535970136 test_loss: 0.4569515277334241\n",
      "iteration 335train_loss: 0.43007680643833646 test_loss: 0.45694547615758924\n",
      "iteration 336train_loss: 0.43004873814755656 test_loss: 0.4569395655056021\n",
      "iteration 337train_loss: 0.4300209456292075 test_loss: 0.45693379369944903\n",
      "iteration 338train_loss: 0.42999342582703676 test_loss: 0.45692815869190556\n",
      "iteration 339train_loss: 0.42996617572343626 test_loss: 0.45692265846604396\n",
      "iteration 340train_loss: 0.42993919233888267 test_loss: 0.45691729103475126\n",
      "iteration 341train_loss: 0.4299124727313869 test_loss: 0.45691205444025446\n",
      "iteration 342train_loss: 0.42988601399595194 test_loss: 0.45690694675365484\n",
      "iteration 343train_loss: 0.42985981326404105 test_loss: 0.4569019660744701\n",
      "iteration 344train_loss: 0.42983386770305354 test_loss: 0.4568971105301843\n",
      "iteration 345train_loss: 0.42980817451580905 test_loss: 0.4568923782758063\n",
      "iteration 346train_loss: 0.4297827309400417 test_loss: 0.4568877674934351\n",
      "iteration 347train_loss: 0.4297575342479003 test_loss: 0.4568832763918334\n",
      "iteration 348train_loss: 0.42973258174545925 test_loss: 0.4568789032060082\n",
      "iteration 349train_loss: 0.42970787077223516 test_loss: 0.45687464619679835\n",
      "iteration 350train_loss: 0.4296833987007121 test_loss: 0.45687050365047027\n",
      "iteration 351train_loss: 0.4296591629358756 test_loss: 0.4568664738783193\n",
      "iteration 352train_loss: 0.4296351609147523 test_loss: 0.45686255521627916\n",
      "iteration 353train_loss: 0.4296113901059582 test_loss: 0.4568587460245373\n",
      "iteration 354train_loss: 0.42958784800925426 test_loss: 0.45685504468715665\n",
      "iteration 355train_loss: 0.4295645321551085 test_loss: 0.4568514496117051\n",
      "iteration 356train_loss: 0.42954144010426537 test_loss: 0.4568479592288899\n",
      "iteration 357train_loss: 0.42951856944732203 test_loss: 0.45684457199219897\n",
      "iteration 358train_loss: 0.4294959178043118 test_loss: 0.45684128637754823\n",
      "iteration 359train_loss: 0.4294734828242931 test_loss: 0.4568381008829352\n",
      "iteration 360train_loss: 0.4294512621849464 test_loss: 0.4568350140280977\n",
      "iteration 361train_loss: 0.4294292535921762 test_loss: 0.4568320243541789\n",
      "iteration 362train_loss: 0.4294074547797203 test_loss: 0.45682913042339834\n",
      "iteration 363train_loss: 0.42938586350876484 test_loss: 0.45682633081872775\n",
      "iteration 364train_loss: 0.4293644775675648 test_loss: 0.4568236241435725\n",
      "iteration 365train_loss: 0.4293432947710719 test_loss: 0.4568210090214589\n",
      "iteration 366train_loss: 0.42932231296056694 test_loss: 0.4568184840957263\n",
      "iteration 367train_loss: 0.42930153000329874 test_loss: 0.4568160480292242\n",
      "iteration 368train_loss: 0.4292809437921281 test_loss: 0.4568136995040153\n",
      "iteration 369train_loss: 0.42926055224517773 test_loss: 0.45681143722108203\n",
      "iteration 370train_loss: 0.42924035330548793 test_loss: 0.45680925990003945\n",
      "iteration 371train_loss: 0.4292203449406766 test_loss: 0.456807166278852\n",
      "iteration 372train_loss: 0.4292005251426055 test_loss: 0.4568051551135552\n",
      "iteration 373train_loss: 0.4291808919270508 test_loss: 0.4568032251779822\n",
      "iteration 374train_loss: 0.42916144333338013 test_loss: 0.4568013752634944\n",
      "iteration 375train_loss: 0.4291421774242326 test_loss: 0.45679960417871707\n",
      "iteration 376train_loss: 0.42912309228520545 test_loss: 0.4567979107492784\n",
      "iteration 377train_loss: 0.4291041860245448 test_loss: 0.456796293817554\n",
      "iteration 378train_loss: 0.42908545677284077 test_loss: 0.456794752242415\n",
      "iteration 379train_loss: 0.42906690268272796 test_loss: 0.45679328489898013\n",
      "iteration 380train_loss: 0.42904852192859044 test_loss: 0.4567918906783719\n",
      "iteration 381train_loss: 0.4290303127062703 test_loss: 0.45679056848747746\n",
      "iteration 382train_loss: 0.4290122732327822 test_loss: 0.45678931724871213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 383train_loss: 0.4289944017460301 test_loss: 0.45678813589978806\n",
      "iteration 384train_loss: 0.42897669650453113 test_loss: 0.4567870233934855\n",
      "iteration 385train_loss: 0.4289591557871405 test_loss: 0.45678597869742893\n",
      "iteration 386train_loss: 0.4289417778927829 test_loss: 0.45678500079386547\n",
      "iteration 387train_loss: 0.4289245611401869 test_loss: 0.45678408867944853\n",
      "iteration 388train_loss: 0.4289075038676236 test_loss: 0.456783241365023\n",
      "iteration 389train_loss: 0.42889060443264904 test_loss: 0.4567824578754159\n",
      "iteration 390train_loss: 0.4288738612118503 test_loss: 0.4567817372492289\n",
      "iteration 391train_loss: 0.42885727260059586 test_loss: 0.4567810785386346\n",
      "iteration 392train_loss: 0.4288408370127899 test_loss: 0.4567804808091766\n",
      "iteration 393train_loss: 0.4288245528806288 test_loss: 0.4567799431395724\n",
      "iteration 394train_loss: 0.4288084186543632 test_loss: 0.45677946462151864\n",
      "iteration 395train_loss: 0.4287924328020617 test_loss: 0.4567790443595014\n",
      "iteration 396train_loss: 0.42877659380937977 test_loss: 0.4567786814706071\n",
      "iteration 397train_loss: 0.4287609001793308 test_loss: 0.4567783750843386\n",
      "iteration 398train_loss: 0.4287453504320611 test_loss: 0.45677812434243303\n",
      "iteration 399train_loss: 0.4287299431046281 test_loss: 0.45677792839868214\n",
      "iteration 400train_loss: 0.42871467675078173 test_loss: 0.4567777864187572\n",
      "iteration 401train_loss: 0.42869954994074927 test_loss: 0.45677769758003467\n",
      "iteration 402train_loss: 0.42868456126102245 test_loss: 0.4567776610714258\n",
      "iteration 403train_loss: 0.4286697093141493 test_loss: 0.45677767609320874\n",
      "iteration 404train_loss: 0.42865499271852714 test_loss: 0.4567777418568631\n",
      "iteration 405train_loss: 0.4286404101082004 test_loss: 0.45677785758490747\n",
      "iteration 406train_loss: 0.4286259601326594 test_loss: 0.4567780225107385\n",
      "iteration 407train_loss: 0.42861164145664365 test_loss: 0.45677823587847416\n",
      "iteration 408train_loss: 0.428597452759948 test_loss: 0.4567784969427978\n",
      "iteration 409train_loss: 0.42858339273722973 test_loss: 0.4567788049688055\n",
      "iteration 410train_loss: 0.4285694600978208 test_loss: 0.45677915923185597\n",
      "iteration 411train_loss: 0.4285556535655412 test_loss: 0.4567795590174223\n",
      "iteration 412train_loss: 0.42854197187851556 test_loss: 0.4567800036209459\n",
      "iteration 413train_loss: 0.4285284137889922 test_loss: 0.4567804923476933\n",
      "iteration 414train_loss: 0.42851497806316513 test_loss: 0.4567810245126149\n",
      "iteration 415train_loss: 0.42850166348099783 test_loss: 0.45678159944020585\n",
      "iteration 416train_loss: 0.4284884688360509 test_loss: 0.45678221646436873\n",
      "iteration 417train_loss: 0.42847539293530995 test_loss: 0.4567828749282792\n",
      "iteration 418train_loss: 0.42846243459901884 test_loss: 0.456783574184253\n",
      "iteration 419train_loss: 0.4284495926605123 test_loss: 0.4567843135936155\n",
      "iteration 420train_loss: 0.428436865966053 test_loss: 0.45678509252657223\n",
      "iteration 421train_loss: 0.42842425337467016 test_loss: 0.4567859103620832\n",
      "iteration 422train_loss: 0.4284117537580007 test_loss: 0.4567867664877373\n",
      "iteration 423train_loss: 0.4283993660001319 test_loss: 0.4567876602996298\n",
      "iteration 424train_loss: 0.4283870889974472 test_loss: 0.4567885912022413\n",
      "iteration 425train_loss: 0.42837492165847363 test_loss: 0.4567895586083184\n",
      "iteration 426train_loss: 0.42836286290373127 test_loss: 0.4567905619387564\n",
      "iteration 427train_loss: 0.4283509116655859 test_loss: 0.45679160062248414\n",
      "iteration 428train_loss: 0.42833906688810147 test_loss: 0.45679267409634955\n",
      "iteration 429train_loss: 0.428327327526897 test_loss: 0.456793781805008\n",
      "iteration 430train_loss: 0.4283156925490045 test_loss: 0.45679492320081155\n",
      "iteration 431train_loss: 0.4283041609327278 test_loss: 0.4567960977437009\n",
      "iteration 432train_loss: 0.4282927316675057 test_loss: 0.45679730490109755\n",
      "iteration 433train_loss: 0.42828140375377455 test_loss: 0.4567985441477989\n",
      "iteration 434train_loss: 0.4282701762028344 test_loss: 0.4567998149658741\n",
      "iteration 435train_loss: 0.42825904803671627 test_loss: 0.4568011168445616\n",
      "iteration 436train_loss: 0.42824801828805154 test_loss: 0.45680244928016894\n",
      "iteration 437train_loss: 0.42823708599994253 test_loss: 0.45680381177597296\n",
      "iteration 438train_loss: 0.4282262502258358 test_loss: 0.4568052038421223\n",
      "iteration 439train_loss: 0.4282155100293966 test_loss: 0.4568066249955407\n",
      "iteration 440train_loss: 0.428204864484385 test_loss: 0.4568080747598329\n",
      "iteration 441train_loss: 0.4281943126745337 test_loss: 0.45680955266519047\n",
      "iteration 442train_loss: 0.42818385369342793 test_loss: 0.4568110582482998\n",
      "iteration 443train_loss: 0.4281734866443865 test_loss: 0.45681259105225214\n",
      "iteration 444train_loss: 0.42816321064034474 test_loss: 0.4568141506264532\n",
      "iteration 445train_loss: 0.4281530248037386 test_loss: 0.45681573652653584\n",
      "iteration 446train_loss: 0.42814292826639144 test_loss: 0.4568173483142734\n",
      "iteration 447train_loss: 0.4281329201694007 test_loss: 0.45681898555749356\n",
      "iteration 448train_loss: 0.42812299966302697 test_loss: 0.4568206478299951\n",
      "iteration 449train_loss: 0.4281131659065854 test_loss: 0.4568223347114643\n",
      "iteration 450train_loss: 0.4281034180683368 test_loss: 0.45682404578739344\n",
      "iteration 451train_loss: 0.4280937553253813 test_loss: 0.4568257806490005\n",
      "iteration 452train_loss: 0.42808417686355354 test_loss: 0.4568275388931497\n",
      "iteration 453train_loss: 0.42807468187731884 test_loss: 0.4568293201222732\n",
      "iteration 454train_loss: 0.4280652695696704 test_loss: 0.4568311239442939\n",
      "iteration 455train_loss: 0.4280559391520288 test_loss: 0.45683294997255075\n",
      "iteration 456train_loss: 0.4280466898441424 test_loss: 0.45683479782572234\n",
      "iteration 457train_loss: 0.42803752087398855 test_loss: 0.4568366671277544\n",
      "iteration 458train_loss: 0.42802843147767744 test_loss: 0.4568385575077867\n",
      "iteration 459train_loss: 0.428019420899355 test_loss: 0.4568404686000817\n",
      "iteration 460train_loss: 0.42801048839111006 test_loss: 0.456842400043954\n",
      "iteration 461train_loss: 0.4280016332128797 test_loss: 0.4568443514837008\n",
      "iteration 462train_loss: 0.4279928546323584 test_loss: 0.4568463225685336\n",
      "iteration 463train_loss: 0.4279841519249062 test_loss: 0.45684831295251044\n",
      "iteration 464train_loss: 0.4279755243734601 test_loss: 0.45685032229447003\n",
      "iteration 465train_loss: 0.42796697126844485 test_loss: 0.4568523502579654\n",
      "iteration 466train_loss: 0.42795849190768614 test_loss: 0.45685439651119997\n",
      "iteration 467train_loss: 0.42795008559632447 test_loss: 0.4568564607269634\n",
      "iteration 468train_loss: 0.4279417516467298 test_loss: 0.4568585425825694\n",
      "iteration 469train_loss: 0.42793348937841785 test_loss: 0.4568606417597936\n",
      "iteration 470train_loss: 0.42792529811796787 test_loss: 0.4568627579448124\n",
      "iteration 471train_loss: 0.4279171771989394 test_loss: 0.45686489082814336\n",
      "iteration 472train_loss: 0.42790912596179337 test_loss: 0.4568670401045859\n",
      "iteration 473train_loss: 0.42790114375381133 test_loss: 0.45686920547316273\n",
      "iteration 474train_loss: 0.4278932299290173 test_loss: 0.4568713866370622\n",
      "iteration 475train_loss: 0.4278853838481 test_loss: 0.45687358330358246\n",
      "iteration 476train_loss: 0.42787760487833615 test_loss: 0.45687579518407445\n",
      "iteration 477train_loss: 0.4278698923935155 test_loss: 0.45687802199388766\n",
      "iteration 478train_loss: 0.4278622457738653 test_loss: 0.4568802634523154\n",
      "iteration 479train_loss: 0.42785466440597725 test_loss: 0.4568825192825416\n",
      "iteration 480train_loss: 0.4278471476827345 test_loss: 0.45688478921158754\n",
      "iteration 481train_loss: 0.42783969500324004 test_loss: 0.4568870729702605\n",
      "iteration 482train_loss: 0.42783230577274567 test_loss: 0.456889370293102\n",
      "iteration 483train_loss: 0.427824979402582 test_loss: 0.45689168091833743\n",
      "iteration 484train_loss: 0.42781771531008944 test_loss: 0.4568940045878261\n",
      "iteration 485train_loss: 0.4278105129185499 test_loss: 0.4568963410470123\n",
      "iteration 486train_loss: 0.4278033716571196 test_loss: 0.45689869004487693\n",
      "iteration 487train_loss: 0.4277962909607622 test_loss: 0.45690105133388936\n",
      "iteration 488train_loss: 0.4277892702701838 test_loss: 0.4569034246699606\n",
      "iteration 489train_loss: 0.4277823090317675 test_loss: 0.4569058098123971\n",
      "iteration 490train_loss: 0.4277754066975093 test_loss: 0.4569082065238546\n",
      "iteration 491train_loss: 0.42776856272495567 test_loss: 0.4569106145702934\n",
      "iteration 492train_loss: 0.42776177657714043 test_loss: 0.45691303372093345\n",
      "iteration 493train_loss: 0.4277550477225235 test_loss: 0.45691546374821085\n",
      "iteration 494train_loss: 0.42774837563492957 test_loss: 0.45691790442773444\n",
      "iteration 495train_loss: 0.42774175979348855 test_loss: 0.4569203555382431\n",
      "iteration 496train_loss: 0.42773519968257623 test_loss: 0.4569228168615644\n",
      "iteration 497train_loss: 0.4277286947917551 test_loss: 0.45692528818257155\n",
      "iteration 498train_loss: 0.42772224461571695 test_loss: 0.4569277692891444\n",
      "iteration 499train_loss: 0.42771584865422574 test_loss: 0.456930259972128\n",
      "iteration 500train_loss: 0.4277095064120606 test_loss: 0.456932760025293\n",
      "iteration 501train_loss: 0.4277032173989612 test_loss: 0.4569352692452968\n",
      "iteration 502train_loss: 0.4276969811295715 test_loss: 0.45693778743164465\n",
      "iteration 503train_loss: 0.42769079712338576 test_loss: 0.4569403143866515\n",
      "iteration 504train_loss: 0.4276846649046955 test_loss: 0.4569428499154046\n",
      "iteration 505train_loss: 0.42767858400253556 test_loss: 0.45694539382572624\n",
      "iteration 506train_loss: 0.427672553950632 test_loss: 0.4569479459281375\n",
      "iteration 507train_loss: 0.4276665742873507 test_loss: 0.4569505060358219\n",
      "iteration 508train_loss: 0.42766064455564606 test_loss: 0.45695307396458984\n",
      "iteration 509train_loss: 0.42765476430301025 test_loss: 0.45695564953284423\n",
      "iteration 510train_loss: 0.42764893308142343 test_loss: 0.4569582325615447\n",
      "iteration 511train_loss: 0.42764315044730505 test_loss: 0.4569608228741747\n",
      "iteration 512train_loss: 0.42763741596146454 test_loss: 0.4569634202967073\n",
      "iteration 513train_loss: 0.4276317291890535 test_loss: 0.45696602465757175\n",
      "iteration 514train_loss: 0.4276260896995184 test_loss: 0.4569686357876215\n",
      "iteration 515train_loss: 0.42762049706655336 test_loss: 0.4569712535201014\n",
      "iteration 516train_loss: 0.4276149508680539 test_loss: 0.4569738776906161\n",
      "iteration 517train_loss: 0.4276094506860717 test_loss: 0.45697650813709867\n",
      "iteration 518train_loss: 0.4276039961067685 test_loss: 0.45697914469977957\n",
      "iteration 519train_loss: 0.4275985867203721 test_loss: 0.4569817872211563\n",
      "iteration 520train_loss: 0.427593222121132 test_loss: 0.4569844355459628\n",
      "iteration 521train_loss: 0.427587901907276 test_loss: 0.45698708952114053\n",
      "iteration 522train_loss: 0.42758262568096694 test_loss: 0.45698974899580863\n",
      "iteration 523train_loss: 0.42757739304826015 test_loss: 0.456992413821235\n",
      "iteration 524train_loss: 0.4275722036190614 test_loss: 0.4569950838508081\n",
      "iteration 525train_loss: 0.4275670570070855 test_loss: 0.45699775894000877\n",
      "iteration 526train_loss: 0.4275619528298155 test_loss: 0.4570004389463822\n",
      "iteration 527train_loss: 0.4275568907084614 test_loss: 0.45700312372951113\n",
      "iteration 528train_loss: 0.42755187026792096 test_loss: 0.45700581315098826\n",
      "iteration 529train_loss: 0.42754689113673944 test_loss: 0.45700850707439017\n",
      "iteration 530train_loss: 0.4275419529470709 test_loss: 0.45701120536525053\n",
      "iteration 531train_loss: 0.4275370553346394 test_loss: 0.45701390789103474\n",
      "iteration 532train_loss: 0.42753219793870095 test_loss: 0.4570166145211141\n",
      "iteration 533train_loss: 0.42752738040200544 test_loss: 0.45701932512674037\n",
      "iteration 534train_loss: 0.4275226023707598 test_loss: 0.45702203958102156\n",
      "iteration 535train_loss: 0.4275178634945909 test_loss: 0.4570247577588966\n",
      "iteration 536train_loss: 0.42751316342650914 test_loss: 0.4570274795371121\n",
      "iteration 537train_loss: 0.4275085018228726 test_loss: 0.45703020479419737\n",
      "iteration 538train_loss: 0.4275038783433515 test_loss: 0.45703293341044166\n",
      "iteration 539train_loss: 0.4274992926508927 test_loss: 0.4570356652678706\n",
      "iteration 540train_loss: 0.42749474441168556 test_loss: 0.45703840025022363\n",
      "iteration 541train_loss: 0.42749023329512725 test_loss: 0.45704113824293074\n",
      "iteration 542train_loss: 0.4274857589737887 test_loss: 0.457043879133091\n",
      "iteration 543train_loss: 0.42748132112338155 test_loss: 0.45704662280944974\n",
      "iteration 544train_loss: 0.42747691942272503 test_loss: 0.45704936916237726\n",
      "iteration 545train_loss: 0.42747255355371233 test_loss: 0.4570521180838472\n",
      "iteration 546train_loss: 0.42746822320127936 test_loss: 0.45705486946741597\n",
      "iteration 547train_loss: 0.4274639280533723 test_loss: 0.457057623208201\n",
      "iteration 548train_loss: 0.4274596678009158 test_loss: 0.45706037920286047\n",
      "iteration 549train_loss: 0.42745544213778214 test_loss: 0.45706313734957377\n",
      "iteration 550train_loss: 0.4274512507607601 test_loss: 0.4570658975480204\n",
      "iteration 551train_loss: 0.4274470933695246 test_loss: 0.45706865969936045\n",
      "iteration 552train_loss: 0.4274429696666062 test_loss: 0.4570714237062159\n",
      "iteration 553train_loss: 0.4274388793573619 test_loss: 0.4570741894726502\n",
      "iteration 554train_loss: 0.42743482214994494 test_loss: 0.45707695690415007\n",
      "iteration 555train_loss: 0.42743079775527637 test_loss: 0.45707972590760626\n",
      "iteration 556train_loss: 0.42742680588701576 test_loss: 0.45708249639129583\n",
      "iteration 557train_loss: 0.42742284626153293 test_loss: 0.4570852682648629\n",
      "iteration 558train_loss: 0.42741891859787967 test_loss: 0.4570880414393015\n",
      "iteration 559train_loss: 0.4274150226177622 test_loss: 0.4570908158269372\n",
      "iteration 560train_loss: 0.42741115804551355 test_loss: 0.45709359134141003\n",
      "iteration 561train_loss: 0.42740732460806624 test_loss: 0.4570963678976567\n",
      "iteration 562train_loss: 0.4274035220349256 test_loss: 0.4570991454118943\n",
      "iteration 563train_loss: 0.427399750058143 test_loss: 0.45710192380160247\n",
      "iteration 564train_loss: 0.4273960084122898 test_loss: 0.45710470298550765\n",
      "iteration 565train_loss: 0.4273922968344313 test_loss: 0.457107482883566\n",
      "iteration 566train_loss: 0.42738861506410136 test_loss: 0.4571102634169479\n",
      "iteration 567train_loss: 0.4273849628432761 test_loss: 0.4571130445080213\n",
      "iteration 568train_loss: 0.4273813399163501 test_loss: 0.45711582608033624\n",
      "iteration 569train_loss: 0.42737774603011064 test_loss: 0.4571186080586096\n",
      "iteration 570train_loss: 0.42737418093371365 test_loss: 0.45712139036870925\n",
      "iteration 571train_loss: 0.4273706443786592 test_loss: 0.457124172937639\n",
      "iteration 572train_loss: 0.4273671361187677 test_loss: 0.4571269556935243\n",
      "iteration 573train_loss: 0.4273636559101561 test_loss: 0.45712973856559613\n",
      "iteration 574train_loss: 0.42736020351121473 test_loss: 0.457132521484178\n",
      "iteration 575train_loss: 0.42735677868258376 test_loss: 0.45713530438067035\n",
      "iteration 576train_loss: 0.4273533811871303 test_loss: 0.4571380871875371\n",
      "iteration 577train_loss: 0.4273500107899262 test_loss: 0.4571408698382915\n",
      "iteration 578train_loss: 0.42734666725822495 test_loss: 0.4571436522674819\n",
      "iteration 579train_loss: 0.4273433503614405 test_loss: 0.45714643441067865\n",
      "iteration 580train_loss: 0.42734005987112406 test_loss: 0.4571492162044605\n",
      "iteration 581train_loss: 0.4273367955609439 test_loss: 0.457151997586401\n",
      "iteration 582train_loss: 0.4273335572066626 test_loss: 0.4571547784950559\n",
      "iteration 583train_loss: 0.4273303445861171 test_loss: 0.4571575588699499\n",
      "iteration 584train_loss: 0.42732715747919653 test_loss: 0.457160338651564\n",
      "iteration 585train_loss: 0.4273239956678226 test_loss: 0.4571631177813228\n",
      "iteration 586train_loss: 0.4273208589359284 test_loss: 0.4571658962015819\n",
      "iteration 587train_loss: 0.4273177470694386 test_loss: 0.4571686738556161\n",
      "iteration 588train_loss: 0.4273146598562491 test_loss: 0.45717145068760706\n",
      "iteration 589train_loss: 0.42731159708620725 test_loss: 0.4571742266426312\n",
      "iteration 590train_loss: 0.42730855855109257 test_loss: 0.45717700166664776\n",
      "iteration 591train_loss: 0.4273055440445971 test_loss: 0.4571797757064879\n",
      "iteration 592train_loss: 0.4273025533623065 test_loss: 0.45718254870984226\n",
      "iteration 593train_loss: 0.4272995863016805 test_loss: 0.45718532062525047\n",
      "iteration 594train_loss: 0.42729664266203504 test_loss: 0.457188091402089\n",
      "iteration 595train_loss: 0.4272937222445233 test_loss: 0.4571908609905609\n",
      "iteration 596train_loss: 0.42729082485211717 test_loss: 0.4571936293416846\n",
      "iteration 597train_loss: 0.4272879502895892 test_loss: 0.45719639640728343\n",
      "iteration 598train_loss: 0.4272850983634953 test_loss: 0.457199162139974\n",
      "iteration 599train_loss: 0.42728226888215615 test_loss: 0.4572019264931572\n",
      "iteration 600train_loss: 0.42727946165564007 test_loss: 0.45720468942100645\n",
      "iteration 601train_loss: 0.42727667649574574 test_loss: 0.4572074508784583\n",
      "iteration 602train_loss: 0.42727391321598496 test_loss: 0.45721021082120217\n",
      "iteration 603train_loss: 0.42727117163156547 test_loss: 0.4572129692056703\n",
      "iteration 604train_loss: 0.4272684515593747 test_loss: 0.4572157259890278\n",
      "iteration 605train_loss: 0.4272657528179627 test_loss: 0.4572184811291632\n",
      "iteration 606train_loss: 0.4272630752275254 test_loss: 0.4572212345846787\n",
      "iteration 607train_loss: 0.42726041860988956 test_loss: 0.45722398631488087\n",
      "iteration 608train_loss: 0.42725778278849547 test_loss: 0.4572267362797709\n",
      "iteration 609train_loss: 0.4272551675883817 test_loss: 0.457229484440036\n",
      "iteration 610train_loss: 0.42725257283616924 test_loss: 0.4572322307570398\n",
      "iteration 611train_loss: 0.4272499983600458 test_loss: 0.4572349751928134\n",
      "iteration 612train_loss: 0.4272474439897507 test_loss: 0.4572377177100469\n",
      "iteration 613train_loss: 0.4272449095565593 test_loss: 0.45724045827208043\n",
      "iteration 614train_loss: 0.4272423948932685 test_loss: 0.4572431968428952\n",
      "iteration 615train_loss: 0.42723989983418115 test_loss: 0.45724593338710534\n",
      "iteration 616train_loss: 0.42723742421509175 test_loss: 0.45724866786994944\n",
      "iteration 617train_loss: 0.4272349678732721 test_loss: 0.45725140025728206\n",
      "iteration 618train_loss: 0.4272325306474558 test_loss: 0.4572541305155656\n",
      "iteration 619train_loss: 0.42723011237782565 test_loss: 0.45725685861186255\n",
      "iteration 620train_loss: 0.42722771290599826 test_loss: 0.45725958451382676\n",
      "iteration 621train_loss: 0.42722533207501 test_loss: 0.45726230818969604\n",
      "iteration 622train_loss: 0.4272229697293044 test_loss: 0.4572650296082845\n",
      "iteration 623train_loss: 0.42722062571471725 test_loss: 0.45726774873897463\n",
      "iteration 624train_loss: 0.42721829987846394 test_loss: 0.45727046555170925\n",
      "iteration 625train_loss: 0.4272159920691252 test_loss: 0.457273180016985\n",
      "iteration 626train_loss: 0.42721370213663473 test_loss: 0.4572758921058441\n",
      "iteration 627train_loss: 0.42721142993226574 test_loss: 0.45727860178986746\n",
      "iteration 628train_loss: 0.42720917530861785 test_loss: 0.45728130904116726\n",
      "iteration 629train_loss: 0.4272069381196044 test_loss: 0.45728401383238015\n",
      "iteration 630train_loss: 0.42720471822043987 test_loss: 0.4572867161366597\n",
      "iteration 631train_loss: 0.4272025154676273 test_loss: 0.45728941592766975\n",
      "iteration 632train_loss: 0.4272003297189457 test_loss: 0.4572921131795777\n",
      "iteration 633train_loss: 0.4271981608334378 test_loss: 0.4572948078670476\n",
      "iteration 634train_loss: 0.4271960086713982 test_loss: 0.4572974999652333\n",
      "iteration 635train_loss: 0.4271938730943609 test_loss: 0.45730018944977197\n",
      "iteration 636train_loss: 0.42719175396508785 test_loss: 0.45730287629677785\n",
      "iteration 637train_loss: 0.42718965114755647 test_loss: 0.45730556048283544\n",
      "iteration 638train_loss: 0.4271875645069491 test_loss: 0.4573082419849935\n",
      "iteration 639train_loss: 0.4271854939096403 test_loss: 0.4573109207807581\n",
      "iteration 640train_loss: 0.427183439223186 test_loss: 0.45731359684808753\n",
      "iteration 641train_loss: 0.4271814003163126 test_loss: 0.45731627016538523\n",
      "iteration 642train_loss: 0.4271793770589048 test_loss: 0.4573189407114942\n",
      "iteration 643train_loss: 0.42717736932199546 test_loss: 0.457321608465691\n",
      "iteration 644train_loss: 0.42717537697775426 test_loss: 0.4573242734076796\n",
      "iteration 645train_loss: 0.42717339989947667 test_loss: 0.45732693551758635\n",
      "iteration 646train_loss: 0.42717143796157364 test_loss: 0.45732959477595314\n",
      "iteration 647train_loss: 0.42716949103956103 test_loss: 0.45733225116373255\n",
      "iteration 648train_loss: 0.4271675590100485 test_loss: 0.457334904662282\n",
      "iteration 649train_loss: 0.42716564175072985 test_loss: 0.4573375552533584\n",
      "iteration 650train_loss: 0.42716373914037226 test_loss: 0.45734020291911237\n",
      "iteration 651train_loss: 0.42716185105880644 test_loss: 0.45734284764208305\n",
      "iteration 652train_loss: 0.42715997738691625 test_loss: 0.457345489405193\n",
      "iteration 653train_loss: 0.42715811800662873 test_loss: 0.4573481281917426\n",
      "iteration 654train_loss: 0.4271562728009049 test_loss: 0.4573507639854052\n",
      "iteration 655train_loss: 0.4271544416537289 test_loss: 0.4573533967702217\n",
      "iteration 656train_loss: 0.4271526244500995 test_loss: 0.45735602653059565\n",
      "iteration 657train_loss: 0.42715082107601965 test_loss: 0.4573586532512885\n",
      "iteration 658train_loss: 0.42714903141848737 test_loss: 0.45736127691741413\n",
      "iteration 659train_loss: 0.4271472553654862 test_loss: 0.4573638975144346\n",
      "iteration 660train_loss: 0.4271454928059764 test_loss: 0.45736651502815506\n",
      "iteration 661train_loss: 0.42714374362988533 test_loss: 0.45736912944471897\n",
      "iteration 662train_loss: 0.42714200772809846 test_loss: 0.45737174075060355\n",
      "iteration 663train_loss: 0.4271402849924503 test_loss: 0.45737434893261497\n",
      "iteration 664train_loss: 0.4271385753157161 test_loss: 0.4573769539778843\n",
      "iteration 665train_loss: 0.42713687859160204 test_loss: 0.45737955587386225\n",
      "iteration 666train_loss: 0.4271351947147371 test_loss: 0.4573821546083155\n",
      "iteration 667train_loss: 0.42713352358066486 test_loss: 0.4573847501693218\n",
      "iteration 668train_loss: 0.4271318650858339 test_loss: 0.4573873425452656\n",
      "iteration 669train_loss: 0.42713021912759036 test_loss: 0.4573899317248343\n",
      "iteration 670train_loss: 0.42712858560416894 test_loss: 0.45739251769701367\n",
      "iteration 671train_loss: 0.42712696441468484 test_loss: 0.4573951004510835\n",
      "iteration 672train_loss: 0.42712535545912544 test_loss: 0.4573976799766141\n",
      "iteration 673train_loss: 0.4271237586383428 test_loss: 0.4574002562634613\n",
      "iteration 674train_loss: 0.42712217385404433 test_loss: 0.4574028293017637\n",
      "iteration 675train_loss: 0.4271206010087863 test_loss: 0.4574053990819371\n",
      "iteration 676train_loss: 0.427119040005965 test_loss: 0.45740796559467234\n",
      "iteration 677train_loss: 0.4271174907498093 test_loss: 0.4574105288309301\n",
      "iteration 678train_loss: 0.4271159531453727 test_loss: 0.4574130887819378\n",
      "iteration 679train_loss: 0.4271144270985264 test_loss: 0.4574156454391855\n",
      "iteration 680train_loss: 0.4271129125159507 test_loss: 0.45741819879442236\n",
      "iteration 681train_loss: 0.42711140930512803 test_loss: 0.4574207488396529\n",
      "iteration 682train_loss: 0.42710991737433596 test_loss: 0.4574232955671335\n",
      "iteration 683train_loss: 0.42710843663263925 test_loss: 0.4574258389693684\n",
      "iteration 684train_loss: 0.42710696698988265 test_loss: 0.4574283790391069\n",
      "iteration 685train_loss: 0.42710550835668415 test_loss: 0.45743091576933925\n",
      "iteration 686train_loss: 0.4271040606444273 test_loss: 0.4574334491532934\n",
      "iteration 687train_loss: 0.42710262376525454 test_loss: 0.4574359791844318\n",
      "iteration 688train_loss: 0.4271011976320602 test_loss: 0.4574385058564474\n",
      "iteration 689train_loss: 0.42709978215848343 test_loss: 0.4574410291632615\n",
      "iteration 690train_loss: 0.4270983772589016 test_loss: 0.4574435490990194\n",
      "iteration 691train_loss: 0.4270969828484231 test_loss: 0.4574460656580875\n",
      "iteration 692train_loss: 0.42709559884288123 test_loss: 0.4574485788350502\n",
      "iteration 693train_loss: 0.42709422515882695 test_loss: 0.45745108862470674\n",
      "iteration 694train_loss: 0.427092861713523 test_loss: 0.4574535950220682\n",
      "iteration 695train_loss: 0.42709150842493676 test_loss: 0.45745609802235426\n",
      "iteration 696train_loss: 0.4270901652117342 test_loss: 0.45745859762099\n",
      "iteration 697train_loss: 0.4270888319932732 test_loss: 0.45746109381360334\n",
      "iteration 698train_loss: 0.4270875086895976 test_loss: 0.4574635865960219\n",
      "iteration 699train_loss: 0.42708619522143065 test_loss: 0.45746607596426964\n",
      "iteration 700train_loss: 0.42708489151016915 test_loss: 0.45746856191456503\n",
      "iteration 701train_loss: 0.4270835974778768 test_loss: 0.457471044443317\n",
      "iteration 702train_loss: 0.42708231304727856 test_loss: 0.45747352354712306\n",
      "iteration 703train_loss: 0.4270810381417546 test_loss: 0.45747599922276594\n",
      "iteration 704train_loss: 0.42707977268533437 test_loss: 0.45747847146721116\n",
      "iteration 705train_loss: 0.4270785166026903 test_loss: 0.4574809402776043\n",
      "iteration 706train_loss: 0.42707726981913263 test_loss: 0.4574834056512682\n",
      "iteration 707train_loss: 0.4270760322606031 test_loss: 0.45748586758570037\n",
      "iteration 708train_loss: 0.42707480385366947 test_loss: 0.4574883260785706\n",
      "iteration 709train_loss: 0.4270735845255199 test_loss: 0.45749078112771807\n",
      "iteration 710train_loss: 0.42707237420395694 test_loss: 0.457493232731149\n",
      "iteration 711train_loss: 0.4270711728173928 test_loss: 0.45749568088703413\n",
      "iteration 712train_loss: 0.42706998029484294 test_loss: 0.4574981255937062\n",
      "iteration 713train_loss: 0.4270687965659213 test_loss: 0.4575005668496574\n",
      "iteration 714train_loss: 0.42706762156083433 test_loss: 0.4575030046535375\n",
      "iteration 715train_loss: 0.4270664552103763 test_loss: 0.4575054390041506\n",
      "iteration 716train_loss: 0.4270652974459235 test_loss: 0.45750786990045367\n",
      "iteration 717train_loss: 0.4270641481994293 test_loss: 0.45751029734155346\n",
      "iteration 718train_loss: 0.4270630074034186 test_loss: 0.4575127213267047\n",
      "iteration 719train_loss: 0.4270618749909834 test_loss: 0.4575151418553078\n",
      "iteration 720train_loss: 0.4270607508957767 test_loss: 0.4575175589269066\n",
      "iteration 721train_loss: 0.4270596350520086 test_loss: 0.45751997254118576\n",
      "iteration 722train_loss: 0.4270585273944402 test_loss: 0.4575223826979691\n",
      "iteration 723train_loss: 0.42705742785837963 test_loss: 0.45752478939721736\n",
      "iteration 724train_loss: 0.4270563363796764 test_loss: 0.4575271926390259\n",
      "iteration 725train_loss: 0.42705525289471724 test_loss: 0.45752959242362284\n",
      "iteration 726train_loss: 0.4270541773404208 test_loss: 0.4575319887513665\n",
      "iteration 727train_loss: 0.4270531096542328 test_loss: 0.45753438162274407\n",
      "iteration 728train_loss: 0.427052049774122 test_loss: 0.4575367710383692\n",
      "iteration 729train_loss: 0.42705099763857474 test_loss: 0.4575391569989798\n",
      "iteration 730train_loss: 0.4270499531865912 test_loss: 0.4575415395054367\n",
      "iteration 731train_loss: 0.4270489163576799 test_loss: 0.45754391855872095\n",
      "iteration 732train_loss: 0.42704788709185354 test_loss: 0.45754629415993264\n",
      "iteration 733train_loss: 0.42704686532962505 test_loss: 0.45754866631028857\n",
      "iteration 734train_loss: 0.4270458510120024 test_loss: 0.4575510350111203\n",
      "iteration 735train_loss: 0.4270448440804844 test_loss: 0.45755340026387287\n",
      "iteration 736train_loss: 0.4270438444770566 test_loss: 0.45755576207010223\n",
      "iteration 737train_loss: 0.42704285214418697 test_loss: 0.45755812043147404\n",
      "iteration 738train_loss: 0.427041867024821 test_loss: 0.4575604753497616\n",
      "iteration 739train_loss: 0.42704088906237836 test_loss: 0.4575628268268444\n",
      "iteration 740train_loss: 0.4270399182007482 test_loss: 0.45756517486470577\n",
      "iteration 741train_loss: 0.42703895438428496 test_loss: 0.4575675194654321\n",
      "iteration 742train_loss: 0.4270379975578043 test_loss: 0.4575698606312103\n",
      "iteration 743train_loss: 0.4270370476665795 test_loss: 0.45757219836432655\n",
      "iteration 744train_loss: 0.4270361046563366 test_loss: 0.45757453266716475\n",
      "iteration 745train_loss: 0.42703516847325135 test_loss: 0.45757686354220434\n",
      "iteration 746train_loss: 0.42703423906394417 test_loss: 0.4575791909920197\n",
      "iteration 747train_loss: 0.42703331637547715 test_loss: 0.4575815150192776\n",
      "iteration 748train_loss: 0.4270324003553497 test_loss: 0.45758383562673627\n",
      "iteration 749train_loss: 0.427031490951495 test_loss: 0.4575861528172433\n",
      "iteration 750train_loss: 0.42703058811227573 test_loss: 0.4575884665937349\n",
      "iteration 751train_loss: 0.4270296917864808 test_loss: 0.45759077695923334\n",
      "iteration 752train_loss: 0.42702880192332127 test_loss: 0.4575930839168469\n",
      "iteration 753train_loss: 0.4270279184724269 test_loss: 0.4575953874697668\n",
      "iteration 754train_loss: 0.42702704138384223 test_loss: 0.4575976876212671\n",
      "iteration 755train_loss: 0.4270261706080231 test_loss: 0.4575999843747025\n",
      "iteration 756train_loss: 0.4270253060958331 test_loss: 0.45760227773350737\n",
      "iteration 757train_loss: 0.42702444779853976 test_loss: 0.45760456770119384\n",
      "iteration 758train_loss: 0.4270235956678112 test_loss: 0.45760685428135106\n",
      "iteration 759train_loss: 0.42702274965571274 test_loss: 0.45760913747764353\n",
      "iteration 760train_loss: 0.42702190971470316 test_loss: 0.45761141729380966\n",
      "iteration 761train_loss: 0.4270210757976317 test_loss: 0.45761369373366073\n",
      "iteration 762train_loss: 0.427020247857734 test_loss: 0.4576159668010793\n",
      "iteration 763train_loss: 0.42701942584862923 test_loss: 0.45761823650001826\n",
      "iteration 764train_loss: 0.4270186097243166 test_loss: 0.4576205028344992\n",
      "iteration 765train_loss: 0.4270177994391723 test_loss: 0.4576227658086116\n",
      "iteration 766train_loss: 0.42701699494794554 test_loss: 0.4576250254265112\n",
      "iteration 767train_loss: 0.4270161962057562 test_loss: 0.45762728169241895\n",
      "iteration 768train_loss: 0.42701540316809095 test_loss: 0.4576295346106197\n",
      "iteration 769train_loss: 0.4270146157908004 test_loss: 0.45763178418546147\n",
      "iteration 770train_loss: 0.42701383403009574 test_loss: 0.45763403042135353\n",
      "iteration 771train_loss: 0.4270130578425457 test_loss: 0.45763627332276574\n",
      "iteration 772train_loss: 0.4270122871850736 test_loss: 0.4576385128942277\n",
      "iteration 773train_loss: 0.42701152201495396 test_loss: 0.4576407491403266\n",
      "iteration 774train_loss: 0.42701076228980966 test_loss: 0.4576429820657073\n",
      "iteration 775train_loss: 0.42701000796760924 test_loss: 0.4576452116750703\n",
      "iteration 776train_loss: 0.427009259006663 test_loss: 0.4576474379731714\n",
      "iteration 777train_loss: 0.4270085153656211 test_loss: 0.4576496609648201\n",
      "iteration 778train_loss: 0.4270077770034701 test_loss: 0.45765188065487894\n",
      "iteration 779train_loss: 0.42700704387952987 test_loss: 0.45765409704826193\n",
      "iteration 780train_loss: 0.4270063159534512 test_loss: 0.45765631014993413\n",
      "iteration 781train_loss: 0.42700559318521253 test_loss: 0.4576585199649102\n",
      "iteration 782train_loss: 0.4270048755351177 test_loss: 0.45766072649825373\n",
      "iteration 783train_loss: 0.4270041629637921 test_loss: 0.457662929755076\n",
      "iteration 784train_loss: 0.42700345543218116 test_loss: 0.4576651297405353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 785train_loss: 0.4270027529015467 test_loss: 0.4576673264598354\n",
      "iteration 786train_loss: 0.42700205533346475 test_loss: 0.4576695199182253\n",
      "iteration 787train_loss: 0.42700136268982253 test_loss: 0.457671710120998\n",
      "iteration 788train_loss: 0.42700067493281557 test_loss: 0.4576738970734893\n",
      "iteration 789train_loss: 0.4269999920249457 test_loss: 0.4576760807810776\n",
      "iteration 790train_loss: 0.42699931392901824 test_loss: 0.45767826124918204\n",
      "iteration 791train_loss: 0.4269986406081388 test_loss: 0.4576804384832626\n",
      "iteration 792train_loss: 0.42699797202571155 test_loss: 0.45768261248881836\n",
      "iteration 793train_loss: 0.4269973081454361 test_loss: 0.4576847832713875\n",
      "iteration 794train_loss: 0.42699664893130523 test_loss: 0.4576869508365456\n",
      "iteration 795train_loss: 0.42699599434760227 test_loss: 0.45768911518990524\n",
      "iteration 796train_loss: 0.42699534435889874 test_loss: 0.4576912763371156\n",
      "iteration 797train_loss: 0.42699469893005176 test_loss: 0.4576934342838606\n",
      "iteration 798train_loss: 0.4269940580262017 test_loss: 0.45769558903585905\n",
      "iteration 799train_loss: 0.42699342161276954 test_loss: 0.4576977405988633\n",
      "iteration 800train_loss: 0.4269927896554549 test_loss: 0.4576998889786586\n",
      "iteration 801train_loss: 0.4269921621202336 test_loss: 0.45770203418106264\n",
      "iteration 802train_loss: 0.42699153897335484 test_loss: 0.4577041762119241\n",
      "iteration 803train_loss: 0.42699092018133916 test_loss: 0.4577063150771229\n",
      "iteration 804train_loss: 0.42699030571097646 test_loss: 0.4577084507825682\n",
      "iteration 805train_loss: 0.4269896955293232 test_loss: 0.457710583334199\n",
      "iteration 806train_loss: 0.42698908960370047 test_loss: 0.4577127127379821\n",
      "iteration 807train_loss: 0.4269884879016916 test_loss: 0.45771483899991267\n",
      "iteration 808train_loss: 0.4269878903911398 test_loss: 0.4577169621260125\n",
      "iteration 809train_loss: 0.42698729704014643 test_loss: 0.45771908212233003\n",
      "iteration 810train_loss: 0.4269867078170683 test_loss: 0.4577211989949391\n",
      "iteration 811train_loss: 0.4269861226905158 test_loss: 0.4577233127499389\n",
      "iteration 812train_loss: 0.42698554162935065 test_loss: 0.45772542339345257\n",
      "iteration 813train_loss: 0.42698496460268365 test_loss: 0.45772753093162716\n",
      "iteration 814train_loss: 0.426984391579873 test_loss: 0.4577296353706327\n",
      "iteration 815train_loss: 0.4269838225305219 test_loss: 0.45773173671666173\n",
      "iteration 816train_loss: 0.42698325742447624 test_loss: 0.45773383497592834\n",
      "iteration 817train_loss: 0.426982696231823 test_loss: 0.45773593015466796\n",
      "iteration 818train_loss: 0.42698213892288817 test_loss: 0.45773802225913646\n",
      "iteration 819train_loss: 0.4269815854682344 test_loss: 0.45774011129560976\n",
      "iteration 820train_loss: 0.42698103583865943 test_loss: 0.45774219727038307\n",
      "iteration 821train_loss: 0.4269804900051937 test_loss: 0.45774428018977015\n",
      "iteration 822train_loss: 0.4269799479390987 test_loss: 0.4577463600601034\n",
      "iteration 823train_loss: 0.42697940961186504 test_loss: 0.4577484368877323\n",
      "iteration 824train_loss: 0.42697887499521003 test_loss: 0.4577505106790238\n",
      "iteration 825train_loss: 0.42697834406107654 test_loss: 0.4577525814403612\n",
      "iteration 826train_loss: 0.42697781678163066 test_loss: 0.4577546491781436\n",
      "iteration 827train_loss: 0.42697729312925975 test_loss: 0.45775671389878564\n",
      "iteration 828train_loss: 0.42697677307657067 test_loss: 0.4577587756087169\n",
      "iteration 829train_loss: 0.4269762565963882 test_loss: 0.45776083431438136\n",
      "iteration 830train_loss: 0.42697574366175295 test_loss: 0.4577628900222364\n",
      "iteration 831train_loss: 0.4269752342459197 test_loss: 0.4577649427387534\n",
      "iteration 832train_loss: 0.42697472832235506 test_loss: 0.4577669924704161\n",
      "iteration 833train_loss: 0.42697422586473677 test_loss: 0.4577690392237206\n",
      "iteration 834train_loss: 0.42697372684695095 test_loss: 0.457771083005175\n",
      "iteration 835train_loss: 0.42697323124309083 test_loss: 0.4577731238212986\n",
      "iteration 836train_loss: 0.4269727390274551 test_loss: 0.4577751616786219\n",
      "iteration 837train_loss: 0.4269722501745456 test_loss: 0.4577771965836857\n",
      "iteration 838train_loss: 0.42697176465906655 test_loss: 0.45777922854304026\n",
      "iteration 839train_loss: 0.42697128245592186 test_loss: 0.45778125756324606\n",
      "iteration 840train_loss: 0.4269708035402144 test_loss: 0.45778328365087234\n",
      "iteration 841train_loss: 0.42697032788724376 test_loss: 0.45778530681249696\n",
      "iteration 842train_loss: 0.42696985547250466 test_loss: 0.4577873270547058\n",
      "iteration 843train_loss: 0.4269693862716854 test_loss: 0.4577893443840929\n",
      "iteration 844train_loss: 0.4269689202606665 test_loss: 0.45779135880725924\n",
      "iteration 845train_loss: 0.426968457415519 test_loss: 0.457793370330813\n",
      "iteration 846train_loss: 0.4269679977125022 test_loss: 0.4577953789613688\n",
      "iteration 847train_loss: 0.42696754112806307 test_loss: 0.45779738470554726\n",
      "iteration 848train_loss: 0.42696708763883423 test_loss: 0.45779938756997474\n",
      "iteration 849train_loss: 0.42696663722163253 test_loss: 0.4578013875612831\n",
      "iteration 850train_loss: 0.4269661898534572 test_loss: 0.4578033846861088\n",
      "iteration 851train_loss: 0.42696574551148897 test_loss: 0.45780537895109313\n",
      "iteration 852train_loss: 0.4269653041730877 test_loss: 0.45780737036288155\n",
      "iteration 853train_loss: 0.426964865815792 test_loss: 0.4578093589281231\n",
      "iteration 854train_loss: 0.42696443041731663 test_loss: 0.45781134465347034\n",
      "iteration 855train_loss: 0.42696399795555184 test_loss: 0.45781332754557924\n",
      "iteration 856train_loss: 0.42696356840856153 test_loss: 0.45781530761110817\n",
      "iteration 857train_loss: 0.4269631417545817 test_loss: 0.45781728485671774\n",
      "iteration 858train_loss: 0.42696271797201996 test_loss: 0.45781925928907097\n",
      "iteration 859train_loss: 0.4269622970394526 test_loss: 0.45782123091483246\n",
      "iteration 860train_loss: 0.42696187893562443 test_loss: 0.457823199740668\n",
      "iteration 861train_loss: 0.42696146363944687 test_loss: 0.4578251657732448\n",
      "iteration 862train_loss: 0.42696105112999666 test_loss: 0.4578271290192303\n",
      "iteration 863train_loss: 0.4269606413865147 test_loss: 0.4578290894852927\n",
      "iteration 864train_loss: 0.42696023438840397 test_loss: 0.4578310471781002\n",
      "iteration 865train_loss: 0.42695983011522953 test_loss: 0.45783300210432093\n",
      "iteration 866train_loss: 0.4269594285467158 test_loss: 0.45783495427062204\n",
      "iteration 867train_loss: 0.4269590296627461 test_loss: 0.45783690368367036\n",
      "iteration 868train_loss: 0.42695863344336105 test_loss: 0.45783885035013144\n",
      "iteration 869train_loss: 0.4269582398687572 test_loss: 0.45784079427666935\n",
      "iteration 870train_loss: 0.426957848919286 test_loss: 0.45784273546994647\n",
      "iteration 871train_loss: 0.42695746057545253 test_loss: 0.4578446739366234\n",
      "iteration 872train_loss: 0.4269570748179137 test_loss: 0.4578466096833583\n",
      "iteration 873train_loss: 0.4269566916274778 test_loss: 0.45784854271680675\n",
      "iteration 874train_loss: 0.42695631098510284 test_loss: 0.457850473043622\n",
      "iteration 875train_loss: 0.42695593287189504 test_loss: 0.4578524006704535\n",
      "iteration 876train_loss: 0.42695555726910844 test_loss: 0.4578543256039483\n",
      "iteration 877train_loss: 0.42695518415814276 test_loss: 0.4578562478507491\n",
      "iteration 878train_loss: 0.42695481352054293 test_loss: 0.45785816741749524\n",
      "iteration 879train_loss: 0.42695444533799737 test_loss: 0.4578600843108218\n",
      "iteration 880train_loss: 0.4269540795923372 test_loss: 0.4578619985373597\n",
      "iteration 881train_loss: 0.42695371626553497 test_loss: 0.4578639101037354\n",
      "iteration 882train_loss: 0.4269533553397036 test_loss: 0.45786581901657014\n",
      "iteration 883train_loss: 0.4269529967970945 test_loss: 0.45786772528248065\n",
      "iteration 884train_loss: 0.426952640620098 test_loss: 0.45786962890807836\n",
      "iteration 885train_loss: 0.42695228679124037 test_loss: 0.45787152989996877\n",
      "iteration 886train_loss: 0.42695193529318415 test_loss: 0.4578734282647524\n",
      "iteration 887train_loss: 0.42695158610872636 test_loss: 0.4578753240090233\n",
      "iteration 888train_loss: 0.4269512392207976 test_loss: 0.4578772171393699\n",
      "iteration 889train_loss: 0.4269508946124608 test_loss: 0.45787910766237394\n",
      "iteration 890train_loss: 0.42695055226691014 test_loss: 0.4578809955846107\n",
      "iteration 891train_loss: 0.42695021216747053 test_loss: 0.4578828809126489\n",
      "iteration 892train_loss: 0.42694987429759573 test_loss: 0.4578847636530502\n",
      "iteration 893train_loss: 0.426949538640868 test_loss: 0.45788664381236927\n",
      "iteration 894train_loss: 0.4269492051809964 test_loss: 0.4578885213971533\n",
      "iteration 895train_loss: 0.42694887390181624 test_loss: 0.45789039641394214\n",
      "iteration 896train_loss: 0.42694854478728816 test_loss: 0.45789226886926776\n",
      "iteration 897train_loss: 0.4269482178214966 test_loss: 0.4578941387696546\n",
      "iteration 898train_loss: 0.4269478929886492 test_loss: 0.4578960061216187\n",
      "iteration 899train_loss: 0.42694757027307567 test_loss: 0.45789787093166806\n",
      "iteration 900train_loss: 0.42694724965922687 test_loss: 0.4578997332063023\n",
      "iteration 901train_loss: 0.4269469311316736 test_loss: 0.4579015929520123\n",
      "iteration 902train_loss: 0.4269466146751061 test_loss: 0.45790345017528056\n",
      "iteration 903train_loss: 0.4269463002743325 test_loss: 0.4579053048825802\n",
      "iteration 904train_loss: 0.42694598791427835 test_loss: 0.45790715708037577\n",
      "iteration 905train_loss: 0.42694567757998547 test_loss: 0.45790900677512214\n",
      "iteration 906train_loss: 0.4269453692566107 test_loss: 0.4579108539732652\n",
      "iteration 907train_loss: 0.42694506292942586 test_loss: 0.4579126986812411\n",
      "iteration 908train_loss: 0.42694475858381586 test_loss: 0.45791454090547645\n",
      "iteration 909train_loss: 0.42694445620527827 test_loss: 0.45791638065238793\n",
      "iteration 910train_loss: 0.4269441557794223 test_loss: 0.4579182179283822\n",
      "iteration 911train_loss: 0.42694385729196815 test_loss: 0.457920052739856\n",
      "iteration 912train_loss: 0.42694356072874573 test_loss: 0.4579218850931956\n",
      "iteration 913train_loss: 0.4269432660756936 test_loss: 0.4579237149947772\n",
      "iteration 914train_loss: 0.42694297331885916 test_loss: 0.4579255424509661\n",
      "iteration 915train_loss: 0.42694268244439654 test_loss: 0.45792736746811724\n",
      "iteration 916train_loss: 0.42694239343856655 test_loss: 0.4579291900525747\n",
      "iteration 917train_loss: 0.4269421062877351 test_loss: 0.4579310102106716\n",
      "iteration 918train_loss: 0.42694182097837335 test_loss: 0.4579328279487301\n",
      "iteration 919train_loss: 0.4269415374970559 test_loss: 0.4579346432730611\n",
      "iteration 920train_loss: 0.42694125583046044 test_loss: 0.4579364561899644\n",
      "iteration 921train_loss: 0.4269409759653669 test_loss: 0.4579382667057283\n",
      "iteration 922train_loss: 0.4269406978886569 test_loss: 0.45794007482662963\n",
      "iteration 923train_loss: 0.42694042158731177 test_loss: 0.4579418805589336\n",
      "iteration 924train_loss: 0.42694014704841365 test_loss: 0.45794368390889356\n",
      "iteration 925train_loss: 0.4269398742591427 test_loss: 0.45794548488275144\n",
      "iteration 926train_loss: 0.4269396032067778 test_loss: 0.45794728348673674\n",
      "iteration 927train_loss: 0.4269393338786951 test_loss: 0.45794907972706733\n",
      "iteration 928train_loss: 0.4269390662623673 test_loss: 0.45795087360994857\n",
      "iteration 929train_loss: 0.42693880034536275 test_loss: 0.45795266514157384\n",
      "iteration 930train_loss: 0.42693853611534527 test_loss: 0.4579544543281241\n",
      "iteration 931train_loss: 0.4269382735600725 test_loss: 0.4579562411757679\n",
      "iteration 932train_loss: 0.426938012667396 test_loss: 0.4579580256906612\n",
      "iteration 933train_loss: 0.4269377534252601 test_loss: 0.4579598078789474\n",
      "iteration 934train_loss: 0.42693749582170104 test_loss: 0.45796158774675716\n",
      "iteration 935train_loss: 0.4269372398448464 test_loss: 0.45796336530020854\n",
      "iteration 936train_loss: 0.4269369854829143 test_loss: 0.45796514054540627\n",
      "iteration 937train_loss: 0.4269367327242132 test_loss: 0.45796691348844265\n",
      "iteration 938train_loss: 0.4269364815571401 test_loss: 0.4579686841353965\n",
      "iteration 939train_loss: 0.4269362319701808 test_loss: 0.45797045249233387\n",
      "iteration 940train_loss: 0.4269359839519088 test_loss: 0.45797221856530745\n",
      "iteration 941train_loss: 0.42693573749098457 test_loss: 0.4579739823603564\n",
      "iteration 942train_loss: 0.42693549257615515 test_loss: 0.4579757438835072\n",
      "iteration 943train_loss: 0.426935249196253 test_loss: 0.45797750314077207\n",
      "iteration 944train_loss: 0.4269350073401958 test_loss: 0.4579792601381505\n",
      "iteration 945train_loss: 0.4269347669969852 test_loss: 0.4579810148816279\n",
      "iteration 946train_loss: 0.42693452815570704 test_loss: 0.45798276737717636\n",
      "iteration 947train_loss: 0.42693429080552975 test_loss: 0.457984517630754\n",
      "iteration 948train_loss: 0.42693405493570397 test_loss: 0.45798626564830525\n",
      "iteration 949train_loss: 0.4269338205355626 test_loss: 0.45798801143576107\n",
      "iteration 950train_loss: 0.4269335875945189 test_loss: 0.45798975499903816\n",
      "iteration 951train_loss: 0.42693335610206706 test_loss: 0.45799149634403946\n",
      "iteration 952train_loss: 0.42693312604778066 test_loss: 0.45799323547665377\n",
      "iteration 953train_loss: 0.4269328974213125 test_loss: 0.4579949724027557\n",
      "iteration 954train_loss: 0.426932670212394 test_loss: 0.45799670712820645\n",
      "iteration 955train_loss: 0.4269324444108343 test_loss: 0.45799843965885223\n",
      "iteration 956train_loss: 0.4269322200065199 test_loss: 0.4580001700005255\n",
      "iteration 957train_loss: 0.42693199698941386 test_loss: 0.4580018981590443\n",
      "iteration 958train_loss: 0.42693177534955523 test_loss: 0.45800362414021245\n",
      "iteration 959train_loss: 0.4269315550770589 test_loss: 0.45800534794981934\n",
      "iteration 960train_loss: 0.426931336162114 test_loss: 0.4580070695936399\n",
      "iteration 961train_loss: 0.42693111859498434 test_loss: 0.4580087890774349\n",
      "iteration 962train_loss: 0.4269309023660072 test_loss: 0.4580105064069502\n",
      "iteration 963train_loss: 0.4269306874655932 test_loss: 0.45801222158791743\n",
      "iteration 964train_loss: 0.4269304738842253 test_loss: 0.4580139346260535\n",
      "iteration 965train_loss: 0.4269302616124583 test_loss: 0.45801564552706103\n",
      "iteration 966train_loss: 0.4269300506409184 test_loss: 0.4580173542966274\n",
      "iteration 967train_loss: 0.4269298409603029 test_loss: 0.4580190609404257\n",
      "iteration 968train_loss: 0.42692963256137917 test_loss: 0.45802076546411435\n",
      "iteration 969train_loss: 0.426929425434984 test_loss: 0.45802246787333695\n",
      "iteration 970train_loss: 0.42692921957202407 test_loss: 0.45802416817372205\n",
      "iteration 971train_loss: 0.4269290149634737 test_loss: 0.4580258663708836\n",
      "iteration 972train_loss: 0.42692881160037616 test_loss: 0.45802756247042087\n",
      "iteration 973train_loss: 0.4269286094738418 test_loss: 0.4580292564779179\n",
      "iteration 974train_loss: 0.4269284085750481 test_loss: 0.45803094839894415\n",
      "iteration 975train_loss: 0.42692820889523897 test_loss: 0.45803263823905377\n",
      "iteration 976train_loss: 0.4269280104257242 test_loss: 0.45803432600378635\n",
      "iteration 977train_loss: 0.4269278131578794 test_loss: 0.45803601169866615\n",
      "iteration 978train_loss: 0.42692761708314464 test_loss: 0.45803769532920274\n",
      "iteration 979train_loss: 0.42692742219302465 test_loss: 0.4580393769008905\n",
      "iteration 980train_loss: 0.42692722847908815 test_loss: 0.45804105641920856\n",
      "iteration 981train_loss: 0.42692703593296727 test_loss: 0.45804273388962136\n",
      "iteration 982train_loss: 0.42692684454635665 test_loss: 0.45804440931757806\n",
      "iteration 983train_loss: 0.42692665431101395 test_loss: 0.45804608270851244\n",
      "iteration 984train_loss: 0.4269264652187584 test_loss: 0.4580477540678437\n",
      "iteration 985train_loss: 0.4269262772614707 test_loss: 0.4580494234009756\n",
      "iteration 986train_loss: 0.42692609043109286 test_loss: 0.4580510907132966\n",
      "iteration 987train_loss: 0.4269259047196269 test_loss: 0.45805275601018003\n",
      "iteration 988train_loss: 0.42692572011913527 test_loss: 0.4580544192969845\n",
      "iteration 989train_loss: 0.4269255366217398 test_loss: 0.45805608057905284\n",
      "iteration 990train_loss: 0.4269253542196214 test_loss: 0.45805773986171283\n",
      "iteration 991train_loss: 0.4269251729050196 test_loss: 0.4580593971502772\n",
      "iteration 992train_loss: 0.42692499267023215 test_loss: 0.45806105245004325\n",
      "iteration 993train_loss: 0.4269248135076146 test_loss: 0.4580627057662931\n",
      "iteration 994train_loss: 0.4269246354095796 test_loss: 0.4580643571042935\n",
      "iteration 995train_loss: 0.4269244583685969 test_loss: 0.45806600646929635\n",
      "iteration 996train_loss: 0.4269242823771922 test_loss: 0.45806765386653775\n",
      "iteration 997train_loss: 0.42692410742794756 test_loss: 0.4580692993012389\n",
      "iteration 998train_loss: 0.4269239335135003 test_loss: 0.4580709427786055\n",
      "iteration 999train_loss: 0.426923760626543 test_loss: 0.45807258430382813\n",
      "iteration 1000train_loss: 0.4269235887598227 test_loss: 0.4580742238820819\n",
      "iteration 1001train_loss: 0.4269234179061408 test_loss: 0.4580758615185269\n",
      "iteration 1002train_loss: 0.42692324805835247 test_loss: 0.45807749721830776\n",
      "iteration 1003train_loss: 0.4269230792093661 test_loss: 0.45807913098655384\n",
      "iteration 1004train_loss: 0.4269229113521434 test_loss: 0.4580807628283791\n",
      "iteration 1005train_loss: 0.4269227444796982 test_loss: 0.45808239274888246\n",
      "iteration 1006train_loss: 0.42692257858509697 test_loss: 0.4580840207531474\n",
      "iteration 1007train_loss: 0.42692241366145756 test_loss: 0.4580856468462419\n",
      "iteration 1008train_loss: 0.4269222497019492 test_loss: 0.4580872710332193\n",
      "iteration 1009train_loss: 0.42692208669979226 test_loss: 0.45808889331911684\n",
      "iteration 1010train_loss: 0.42692192464825734 test_loss: 0.45809051370895715\n",
      "iteration 1011train_loss: 0.4269217635406655 test_loss: 0.4580921322077471\n",
      "iteration 1012train_loss: 0.4269216033703877 test_loss: 0.4580937488204786\n",
      "iteration 1013train_loss: 0.4269214441308437 test_loss: 0.4580953635521283\n",
      "iteration 1014train_loss: 0.42692128581550287 test_loss: 0.45809697640765745\n",
      "iteration 1015train_loss: 0.4269211284178827 test_loss: 0.45809858739201215\n",
      "iteration 1016train_loss: 0.42692097193154943 test_loss: 0.458100196510123\n",
      "iteration 1017train_loss: 0.42692081635011675 test_loss: 0.4581018037669058\n",
      "iteration 1018train_loss: 0.42692066166724607 test_loss: 0.4581034091672608\n",
      "iteration 1019train_loss: 0.4269205078766459 test_loss: 0.4581050127160733\n",
      "iteration 1020train_loss: 0.4269203549720716 test_loss: 0.458106614418213\n",
      "iteration 1021train_loss: 0.4269202029473248 test_loss: 0.45810821427853493\n",
      "iteration 1022train_loss: 0.4269200517962531 test_loss: 0.4581098123018784\n",
      "iteration 1023train_loss: 0.42691990151275006 test_loss: 0.458111408493068\n",
      "iteration 1024train_loss: 0.42691975209075456 test_loss: 0.45811300285691275\n",
      "iteration 1025train_loss: 0.4269196035242501 test_loss: 0.4581145953982069\n",
      "iteration 1026train_loss: 0.4269194558072655 test_loss: 0.45811618612172955\n",
      "iteration 1027train_loss: 0.4269193089338731 test_loss: 0.45811777503224427\n",
      "iteration 1028train_loss: 0.4269191628981899 test_loss: 0.45811936213449983\n",
      "iteration 1029train_loss: 0.42691901769437596 test_loss: 0.4581209474332301\n",
      "iteration 1030train_loss: 0.42691887331663503 test_loss: 0.45812253093315347\n",
      "iteration 1031train_loss: 0.42691872975921374 test_loss: 0.4581241126389736\n",
      "iteration 1032train_loss: 0.4269185870164013 test_loss: 0.4581256925553789\n",
      "iteration 1033train_loss: 0.4269184450825289 test_loss: 0.45812727068704273\n",
      "iteration 1034train_loss: 0.42691830395197033 test_loss: 0.4581288470386238\n",
      "iteration 1035train_loss: 0.4269181636191405 test_loss: 0.45813042161476536\n",
      "iteration 1036train_loss: 0.426918024078496 test_loss: 0.4581319944200959\n",
      "iteration 1037train_loss: 0.42691788532453406 test_loss: 0.45813356545922906\n",
      "iteration 1038train_loss: 0.4269177473517931 test_loss: 0.45813513473676354\n",
      "iteration 1039train_loss: 0.42691761015485147 test_loss: 0.45813670225728287\n",
      "iteration 1040train_loss: 0.42691747372832767 test_loss: 0.45813826802535584\n",
      "iteration 1041train_loss: 0.4269173380668803 test_loss: 0.45813983204553643\n",
      "iteration 1042train_loss: 0.42691720316520715 test_loss: 0.45814139432236384\n",
      "iteration 1043train_loss: 0.42691706901804494 test_loss: 0.45814295486036205\n",
      "iteration 1044train_loss: 0.4269169356201699 test_loss: 0.45814451366404085\n",
      "iteration 1045train_loss: 0.42691680296639617 test_loss: 0.4581460707378947\n",
      "iteration 1046train_loss: 0.4269166710515766 test_loss: 0.45814762608640325\n",
      "iteration 1047train_loss: 0.42691653987060163 test_loss: 0.4581491797140321\n",
      "iteration 1048train_loss: 0.4269164094183997 test_loss: 0.45815073162523134\n",
      "iteration 1049train_loss: 0.4269162796899365 test_loss: 0.45815228182443685\n",
      "iteration 1050train_loss: 0.426916150680215 test_loss: 0.4581538303160695\n",
      "iteration 1051train_loss: 0.4269160223842746 test_loss: 0.45815537710453597\n",
      "iteration 1052train_loss: 0.42691589479719166 test_loss: 0.45815692219422777\n",
      "iteration 1053train_loss: 0.42691576791407854 test_loss: 0.4581584655895222\n",
      "iteration 1054train_loss: 0.4269156417300838 test_loss: 0.4581600072947819\n",
      "iteration 1055train_loss: 0.4269155162403915 test_loss: 0.45816154731435477\n",
      "iteration 1056train_loss: 0.42691539144022117 test_loss: 0.4581630856525745\n",
      "iteration 1057train_loss: 0.4269152673248277 test_loss: 0.45816462231376\n",
      "iteration 1058train_loss: 0.4269151438895006 test_loss: 0.4581661573022158\n",
      "iteration 1059train_loss: 0.42691502112956414 test_loss: 0.4581676906222323\n",
      "iteration 1060train_loss: 0.42691489904037705 test_loss: 0.45816922227808476\n",
      "iteration 1061train_loss: 0.42691477761733215 test_loss: 0.45817075227403475\n",
      "iteration 1062train_loss: 0.42691465685585583 test_loss: 0.4581722806143291\n",
      "iteration 1063train_loss: 0.42691453675140845 test_loss: 0.45817380730320045\n",
      "iteration 1064train_loss: 0.4269144172994835 test_loss: 0.45817533234486707\n",
      "iteration 1065train_loss: 0.4269142984956076 test_loss: 0.4581768557435329\n",
      "iteration 1066train_loss: 0.42691418033534 test_loss: 0.4581783775033878\n",
      "iteration 1067train_loss: 0.42691406281427285 test_loss: 0.4581798976286073\n",
      "iteration 1068train_loss: 0.4269139459280304 test_loss: 0.4581814161233526\n",
      "iteration 1069train_loss: 0.42691382967226904 test_loss: 0.45818293299177104\n",
      "iteration 1070train_loss: 0.4269137140426771 test_loss: 0.4581844482379954\n",
      "iteration 1071train_loss: 0.4269135990349744 test_loss: 0.4581859618661449\n",
      "iteration 1072train_loss: 0.4269134846449121 test_loss: 0.45818747388032427\n",
      "iteration 1073train_loss: 0.4269133708682725 test_loss: 0.45818898428462423\n",
      "iteration 1074train_loss: 0.42691325770086885 test_loss: 0.45819049308312154\n",
      "iteration 1075train_loss: 0.4269131451385449 test_loss: 0.45819200027987905\n",
      "iteration 1076train_loss: 0.426913033177175 test_loss: 0.4581935058789458\n",
      "iteration 1077train_loss: 0.42691292181266355 test_loss: 0.45819500988435635\n",
      "iteration 1078train_loss: 0.42691281104094486 test_loss: 0.45819651230013203\n",
      "iteration 1079train_loss: 0.4269127008579831 test_loss: 0.4581980131302796\n",
      "iteration 1080train_loss: 0.426912591259772 test_loss: 0.4581995123787928\n",
      "iteration 1081train_loss: 0.42691248224233413 test_loss: 0.4582010100496509\n",
      "iteration 1082train_loss: 0.4269123738017219 test_loss: 0.45820250614681984\n",
      "iteration 1083train_loss: 0.4269122659340157 test_loss: 0.45820400067425143\n",
      "iteration 1084train_loss: 0.4269121586353251 test_loss: 0.4582054936358842\n",
      "iteration 1085train_loss: 0.426912051901788 test_loss: 0.4582069850356426\n",
      "iteration 1086train_loss: 0.42691194572957003 test_loss: 0.45820847487743777\n",
      "iteration 1087train_loss: 0.4269118401148656 test_loss: 0.45820996316516704\n",
      "iteration 1088train_loss: 0.42691173505389607 test_loss: 0.45821144990271423\n",
      "iteration 1089train_loss: 0.42691163054291087 test_loss: 0.45821293509394967\n",
      "iteration 1090train_loss: 0.4269115265781866 test_loss: 0.45821441874273017\n",
      "iteration 1091train_loss: 0.426911423156027 test_loss: 0.45821590085289887\n",
      "iteration 1092train_loss: 0.4269113202727626 test_loss: 0.4582173814282857\n",
      "iteration 1093train_loss: 0.42691121792475095 test_loss: 0.4582188604727073\n",
      "iteration 1094train_loss: 0.426911116108376 test_loss: 0.45822033798996653\n",
      "iteration 1095train_loss: 0.4269110148200479 test_loss: 0.45822181398385314\n",
      "iteration 1096train_loss: 0.4269109140562031 test_loss: 0.4582232884581437\n",
      "iteration 1097train_loss: 0.4269108138133038 test_loss: 0.4582247614166012\n",
      "iteration 1098train_loss: 0.42691071408783804 test_loss: 0.4582262328629756\n",
      "iteration 1099train_loss: 0.4269106148763195 test_loss: 0.4582277028010038\n",
      "iteration 1100train_loss: 0.4269105161752871 test_loss: 0.45822917123440915\n",
      "iteration 1101train_loss: 0.42691041798130464 test_loss: 0.4582306381669022\n",
      "iteration 1102train_loss: 0.4269103202909615 test_loss: 0.4582321036021799\n",
      "iteration 1103train_loss: 0.4269102231008713 test_loss: 0.45823356754392713\n",
      "iteration 1104train_loss: 0.42691012640767245 test_loss: 0.45823502999581467\n",
      "iteration 1105train_loss: 0.42691003020802787 test_loss: 0.4582364909615007\n",
      "iteration 1106train_loss: 0.4269099344986244 test_loss: 0.45823795044463067\n",
      "iteration 1107train_loss: 0.4269098392761733 test_loss: 0.45823940844883687\n",
      "iteration 1108train_loss: 0.42690974453740943 test_loss: 0.4582408649777387\n",
      "iteration 1109train_loss: 0.4269096502790912 test_loss: 0.45824232003494275\n",
      "iteration 1110train_loss: 0.42690955649800105 test_loss: 0.4582437736240428\n",
      "iteration 1111train_loss: 0.42690946319094414 test_loss: 0.4582452257486198\n",
      "iteration 1112train_loss: 0.42690937035474913 test_loss: 0.4582466764122419\n",
      "iteration 1113train_loss: 0.42690927798626765 test_loss: 0.45824812561846484\n",
      "iteration 1114train_loss: 0.42690918608237394 test_loss: 0.4582495733708313\n",
      "iteration 1115train_loss: 0.426909094639965 test_loss: 0.45825101967287146\n",
      "iteration 1116train_loss: 0.42690900365596024 test_loss: 0.458252464528103\n",
      "iteration 1117train_loss: 0.42690891312730145 test_loss: 0.45825390794003074\n",
      "iteration 1118train_loss: 0.42690882305095257 test_loss: 0.4582553499121474\n",
      "iteration 1119train_loss: 0.4269087334238993 test_loss: 0.4582567904479328\n",
      "iteration 1120train_loss: 0.4269086442431491 test_loss: 0.4582582295508545\n",
      "iteration 1121train_loss: 0.42690855550573154 test_loss: 0.4582596672243676\n",
      "iteration 1122train_loss: 0.426908467208697 test_loss: 0.45826110347191484\n",
      "iteration 1123train_loss: 0.42690837934911774 test_loss: 0.4582625382969264\n",
      "iteration 1124train_loss: 0.4269082919240868 test_loss: 0.45826397170282046\n",
      "iteration 1125train_loss: 0.4269082049307183 test_loss: 0.4582654036930027\n",
      "iteration 1126train_loss: 0.42690811836614734 test_loss: 0.4582668342708665\n",
      "iteration 1127train_loss: 0.42690803222752954 test_loss: 0.4582682634397934\n",
      "iteration 1128train_loss: 0.42690794651204095 test_loss: 0.45826969120315225\n",
      "iteration 1129train_loss: 0.42690786121687824 test_loss: 0.4582711175643\n",
      "iteration 1130train_loss: 0.4269077763392583 test_loss: 0.4582725425265818\n",
      "iteration 1131train_loss: 0.4269076918764176 test_loss: 0.45827396609333015\n",
      "iteration 1132train_loss: 0.42690760782561304 test_loss: 0.4582753882678659\n",
      "iteration 1133train_loss: 0.4269075241841212 test_loss: 0.458276809053498\n",
      "iteration 1134train_loss: 0.4269074409492382 test_loss: 0.458278228453523\n",
      "iteration 1135train_loss: 0.4269073581182796 test_loss: 0.45827964647122593\n",
      "iteration 1136train_loss: 0.4269072756885802 test_loss: 0.45828106310987987\n",
      "iteration 1137train_loss: 0.4269071936574943 test_loss: 0.4582824783727459\n",
      "iteration 1138train_loss: 0.4269071120223949 test_loss: 0.4582838922630733\n",
      "iteration 1139train_loss: 0.426907030780674 test_loss: 0.45828530478409985\n",
      "iteration 1140train_loss: 0.4269069499297424 test_loss: 0.45828671593905124\n",
      "iteration 1141train_loss: 0.4269068694670295 test_loss: 0.45828812573114175\n",
      "iteration 1142train_loss: 0.42690678938998317 test_loss: 0.45828953416357365\n",
      "iteration 1143train_loss: 0.42690670969606975 test_loss: 0.458290941239538\n",
      "iteration 1144train_loss: 0.42690663038277354 test_loss: 0.45829234696221405\n",
      "iteration 1145train_loss: 0.426906551447597 test_loss: 0.45829375133476946\n",
      "iteration 1146train_loss: 0.42690647288806055 test_loss: 0.4582951543603603\n",
      "iteration 1147train_loss: 0.42690639470170233 test_loss: 0.4582965560421315\n",
      "iteration 1148train_loss: 0.4269063168860782 test_loss: 0.4582979563832162\n",
      "iteration 1149train_loss: 0.42690623943876177 test_loss: 0.45829935538673616\n",
      "iteration 1150train_loss: 0.42690616235734363 test_loss: 0.4583007530558021\n",
      "iteration 1151train_loss: 0.42690608563943194 test_loss: 0.45830214939351305\n",
      "iteration 1152train_loss: 0.42690600928265204 test_loss: 0.4583035444029569\n",
      "iteration 1153train_loss: 0.42690593328464593 test_loss: 0.4583049380872102\n",
      "iteration 1154train_loss: 0.42690585764307276 test_loss: 0.4583063304493382\n",
      "iteration 1155train_loss: 0.42690578235560855 test_loss: 0.4583077214923953\n",
      "iteration 1156train_loss: 0.4269057074199458 test_loss: 0.45830911121942436\n",
      "iteration 1157train_loss: 0.42690563283379357 test_loss: 0.4583104996334574\n",
      "iteration 1158train_loss: 0.4269055585948773 test_loss: 0.45831188673751516\n",
      "iteration 1159train_loss: 0.42690548470093864 test_loss: 0.4583132725346075\n",
      "iteration 1160train_loss: 0.4269054111497354 test_loss: 0.4583146570277331\n",
      "iteration 1161train_loss: 0.4269053379390417 test_loss: 0.45831604021987987\n",
      "iteration 1162train_loss: 0.42690526506664705 test_loss: 0.45831742211402465\n",
      "iteration 1163train_loss: 0.4269051925303573 test_loss: 0.4583188027131333\n",
      "iteration 1164train_loss: 0.42690512032799366 test_loss: 0.45832018202016106\n",
      "iteration 1165train_loss: 0.42690504845739285 test_loss: 0.4583215600380521\n",
      "iteration 1166train_loss: 0.4269049769164074 test_loss: 0.4583229367697401\n",
      "iteration 1167train_loss: 0.4269049057029046 test_loss: 0.4583243122181476\n",
      "iteration 1168train_loss: 0.4269048348147676 test_loss: 0.4583256863861868\n",
      "iteration 1169train_loss: 0.42690476424989415 test_loss: 0.45832705927675893\n",
      "iteration 1170train_loss: 0.4269046940061971 test_loss: 0.45832843089275466\n",
      "iteration 1171train_loss: 0.42690462408160446 test_loss: 0.45832980123705425\n",
      "iteration 1172train_loss: 0.4269045544740588 test_loss: 0.458331170312527\n",
      "iteration 1173train_loss: 0.42690448518151713 test_loss: 0.45833253812203206\n",
      "iteration 1174train_loss: 0.4269044162019515 test_loss: 0.4583339046684179\n",
      "iteration 1175train_loss: 0.42690434753334805 test_loss: 0.45833526995452245\n",
      "iteration 1176train_loss: 0.42690427917370727 test_loss: 0.4583366339831733\n",
      "iteration 1177train_loss: 0.42690421112104415 test_loss: 0.45833799675718767\n",
      "iteration 1178train_loss: 0.42690414337338756 test_loss: 0.45833935827937244\n",
      "iteration 1179train_loss: 0.42690407592878027 test_loss: 0.45834071855252406\n",
      "iteration 1180train_loss: 0.42690400878527945 test_loss: 0.4583420775794288\n",
      "iteration 1181train_loss: 0.42690394194095566 test_loss: 0.4583434353628626\n",
      "iteration 1182train_loss: 0.4269038753938933 test_loss: 0.4583447919055913\n",
      "iteration 1183train_loss: 0.42690380914219034 test_loss: 0.4583461472103702\n",
      "iteration 1184train_loss: 0.42690374318395846 test_loss: 0.45834750127994517\n",
      "iteration 1185train_loss: 0.42690367751732256 test_loss: 0.4583488541170512\n",
      "iteration 1186train_loss: 0.4269036121404209 test_loss: 0.4583502057244137\n",
      "iteration 1187train_loss: 0.42690354705140476 test_loss: 0.4583515561047479\n",
      "iteration 1188train_loss: 0.42690348224843905 test_loss: 0.4583529052607589\n",
      "iteration 1189train_loss: 0.4269034177297012 test_loss: 0.458354253195142\n",
      "iteration 1190train_loss: 0.42690335349338177 test_loss: 0.4583555999105825\n",
      "iteration 1191train_loss: 0.42690328953768425 test_loss: 0.4583569454097559\n",
      "iteration 1192train_loss: 0.4269032258608246 test_loss: 0.45835828969532766\n",
      "iteration 1193train_loss: 0.42690316246103155 test_loss: 0.4583596327699535\n",
      "iteration 1194train_loss: 0.4269030993365466 test_loss: 0.4583609746362793\n",
      "iteration 1195train_loss: 0.42690303648562317 test_loss: 0.45836231529694155\n",
      "iteration 1196train_loss: 0.42690297390652776 test_loss: 0.4583636547545663\n",
      "iteration 1197train_loss: 0.42690291159753846 test_loss: 0.45836499301177064\n",
      "iteration 1198train_loss: 0.42690284955694596 test_loss: 0.4583663300711613\n",
      "iteration 1199train_loss: 0.42690278778305296 test_loss: 0.45836766593533607\n",
      "iteration 1200train_loss: 0.426902726274174 test_loss: 0.4583690006068828\n",
      "iteration 1201train_loss: 0.4269026650286357 test_loss: 0.45837033408837985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1202train_loss: 0.42690260404477676 test_loss: 0.458371666382396\n",
      "iteration 1203train_loss: 0.4269025433209468 test_loss: 0.45837299749149063\n",
      "iteration 1204train_loss: 0.4269024828555081 test_loss: 0.45837432741821377\n",
      "iteration 1205train_loss: 0.42690242264683387 test_loss: 0.45837565616510567\n",
      "iteration 1206train_loss: 0.4269023626933088 test_loss: 0.45837698373469765\n",
      "iteration 1207train_loss: 0.4269023029933293 test_loss: 0.4583783101295114\n",
      "iteration 1208train_loss: 0.42690224354530304 test_loss: 0.4583796353520595\n",
      "iteration 1209train_loss: 0.4269021843476488 test_loss: 0.45838095940484497\n",
      "iteration 1210train_loss: 0.42690212539879646 test_loss: 0.4583822822903619\n",
      "iteration 1211train_loss: 0.42690206669718694 test_loss: 0.45838360401109485\n",
      "iteration 1212train_loss: 0.42690200824127245 test_loss: 0.4583849245695196\n",
      "iteration 1213train_loss: 0.4269019500295159 test_loss: 0.4583862439681024\n",
      "iteration 1214train_loss: 0.42690189206039086 test_loss: 0.45838756220930077\n",
      "iteration 1215train_loss: 0.4269018343323821 test_loss: 0.45838887929556266\n",
      "iteration 1216train_loss: 0.4269017768439846 test_loss: 0.4583901952293274\n",
      "iteration 1217train_loss: 0.4269017195937041 test_loss: 0.45839151001302525\n",
      "iteration 1218train_loss: 0.426901662580057 test_loss: 0.4583928236490773\n",
      "iteration 1219train_loss: 0.42690160580156994 test_loss: 0.458394136139896\n",
      "iteration 1220train_loss: 0.4269015492567799 test_loss: 0.45839544748788447\n",
      "iteration 1221train_loss: 0.42690149294423435 test_loss: 0.4583967576954373\n",
      "iteration 1222train_loss: 0.4269014368624908 test_loss: 0.4583980667649402\n",
      "iteration 1223train_loss: 0.426901381010117 test_loss: 0.45839937469876996\n",
      "iteration 1224train_loss: 0.4269013253856905 test_loss: 0.4584006814992947\n",
      "iteration 1225train_loss: 0.4269012699877993 test_loss: 0.45840198716887365\n",
      "iteration 1226train_loss: 0.4269012148150409 test_loss: 0.4584032917098574\n",
      "iteration 1227train_loss: 0.4269011598660228 test_loss: 0.4584045951245879\n",
      "iteration 1228train_loss: 0.42690110513936225 test_loss: 0.4584058974153986\n",
      "iteration 1229train_loss: 0.42690105063368616 test_loss: 0.4584071985846139\n",
      "iteration 1230train_loss: 0.4269009963476312 test_loss: 0.45840849863455\n",
      "iteration 1231train_loss: 0.42690094227984343 test_loss: 0.4584097975675145\n",
      "iteration 1232train_loss: 0.42690088842897866 test_loss: 0.4584110953858063\n",
      "iteration 1233train_loss: 0.4269008347937017 test_loss: 0.458412392091716\n",
      "iteration 1234train_loss: 0.4269007813726871 test_loss: 0.45841368768752566\n",
      "iteration 1235train_loss: 0.42690072816461827 test_loss: 0.45841498217550913\n",
      "iteration 1236train_loss: 0.42690067516818836 test_loss: 0.4584162755579314\n",
      "iteration 1237train_loss: 0.4269006223820992 test_loss: 0.4584175678370495\n",
      "iteration 1238train_loss: 0.426900569805062 test_loss: 0.45841885901511215\n",
      "iteration 1239train_loss: 0.4269005174357969 test_loss: 0.4584201490943594\n",
      "iteration 1240train_loss: 0.4269004652730328 test_loss: 0.45842143807702346\n",
      "iteration 1241train_loss: 0.42690041331550777 test_loss: 0.4584227259653281\n",
      "iteration 1242train_loss: 0.4269003615619685 test_loss: 0.4584240127614889\n",
      "iteration 1243train_loss: 0.42690031001117046 test_loss: 0.45842529846771346\n",
      "iteration 1244train_loss: 0.4269002586618779 test_loss: 0.45842658308620077\n",
      "iteration 1245train_loss: 0.4269002075128638 test_loss: 0.4584278666191423\n",
      "iteration 1246train_loss: 0.4269001565629091 test_loss: 0.45842914906872106\n",
      "iteration 1247train_loss: 0.42690010581080395 test_loss: 0.45843043043711207\n",
      "iteration 1248train_loss: 0.4269000552553469 test_loss: 0.4584317107264824\n",
      "iteration 1249train_loss: 0.42690000489534413 test_loss: 0.45843298993899145\n",
      "iteration 1250train_loss: 0.426899954729611 test_loss: 0.45843426807679\n",
      "iteration 1251train_loss: 0.4268999047569707 test_loss: 0.45843554514202134\n",
      "iteration 1252train_loss: 0.42689985497625466 test_loss: 0.4584368211368209\n",
      "iteration 1253train_loss: 0.4268998053863024 test_loss: 0.458438096063316\n",
      "iteration 1254train_loss: 0.4268997559859617 test_loss: 0.45843936992362627\n",
      "iteration 1255train_loss: 0.4268997067740882 test_loss: 0.4584406427198638\n",
      "iteration 1256train_loss: 0.4268996577495455 test_loss: 0.4584419144541323\n",
      "iteration 1257train_loss: 0.42689960891120526 test_loss: 0.45844318512852844\n",
      "iteration 1258train_loss: 0.42689956025794656 test_loss: 0.45844445474514056\n",
      "iteration 1259train_loss: 0.4268995117886569 test_loss: 0.4584457233060497\n",
      "iteration 1260train_loss: 0.426899463502231 test_loss: 0.45844699081332924\n",
      "iteration 1261train_loss: 0.42689941539757165 test_loss: 0.4584482572690447\n",
      "iteration 1262train_loss: 0.42689936747358875 test_loss: 0.45844952267525435\n",
      "iteration 1263train_loss: 0.42689931972920026 test_loss: 0.4584507870340086\n",
      "iteration 1264train_loss: 0.42689927216333134 test_loss: 0.4584520503473503\n",
      "iteration 1265train_loss: 0.42689922477491477 test_loss: 0.4584533126173153\n",
      "iteration 1266train_loss: 0.4268991775628907 test_loss: 0.45845457384593136\n",
      "iteration 1267train_loss: 0.4268991305262065 test_loss: 0.4584558340352191\n",
      "iteration 1268train_loss: 0.4268990836638169 test_loss: 0.45845709318719174\n",
      "iteration 1269train_loss: 0.426899036974684 test_loss: 0.458458351303855\n",
      "iteration 1270train_loss: 0.426898990457777 test_loss: 0.45845960838720734\n",
      "iteration 1271train_loss: 0.42689894411207213 test_loss: 0.4584608644392399\n",
      "iteration 1272train_loss: 0.4268988979365527 test_loss: 0.4584621194619364\n",
      "iteration 1273train_loss: 0.4268988519302093 test_loss: 0.45846337345727345\n",
      "iteration 1274train_loss: 0.42689880609203934 test_loss: 0.45846462642722036\n",
      "iteration 1275train_loss: 0.4268987604210471 test_loss: 0.4584658783737391\n",
      "iteration 1276train_loss: 0.4268987149162438 test_loss: 0.4584671292987847\n",
      "iteration 1277train_loss: 0.42689866957664757 test_loss: 0.4584683792043047\n",
      "iteration 1278train_loss: 0.4268986244012831 test_loss: 0.4584696280922402\n",
      "iteration 1279train_loss: 0.4268985793891822 test_loss: 0.4584708759645241\n",
      "iteration 1280train_loss: 0.4268985345393829 test_loss: 0.4584721228230833\n",
      "iteration 1281train_loss: 0.4268984898509301 test_loss: 0.458473368669837\n",
      "iteration 1282train_loss: 0.4268984453228754 test_loss: 0.45847461350669766\n",
      "iteration 1283train_loss: 0.4268984009542768 test_loss: 0.4584758573355708\n",
      "iteration 1284train_loss: 0.42689835674419874 test_loss: 0.4584771001583547\n",
      "iteration 1285train_loss: 0.4268983126917121 test_loss: 0.45847834197694093\n",
      "iteration 1286train_loss: 0.4268982687958944 test_loss: 0.45847958279321405\n",
      "iteration 1287train_loss: 0.42689822505582925 test_loss: 0.458480822609052\n",
      "iteration 1288train_loss: 0.42689818147060665 test_loss: 0.4584820614263254\n",
      "iteration 1289train_loss: 0.42689813803932286 test_loss: 0.4584832992468986\n",
      "iteration 1290train_loss: 0.4268980947610804 test_loss: 0.4584845360726288\n",
      "iteration 1291train_loss: 0.42689805163498795 test_loss: 0.45848577190536643\n",
      "iteration 1292train_loss: 0.42689800866016026 test_loss: 0.45848700674695514\n",
      "iteration 1293train_loss: 0.4268979658357181 test_loss: 0.4584882405992324\n",
      "iteration 1294train_loss: 0.4268979231607886 test_loss: 0.45848947346402813\n",
      "iteration 1295train_loss: 0.4268978806345045 test_loss: 0.4584907053431664\n",
      "iteration 1296train_loss: 0.42689783825600475 test_loss: 0.45849193623846407\n",
      "iteration 1297train_loss: 0.42689779602443406 test_loss: 0.4584931661517317\n",
      "iteration 1298train_loss: 0.4268977539389429 test_loss: 0.45849439508477324\n",
      "iteration 1299train_loss: 0.426897711998688 test_loss: 0.45849562303938596\n",
      "iteration 1300train_loss: 0.4268976702028314 test_loss: 0.4584968500173606\n",
      "iteration 1301train_loss: 0.42689762855054114 test_loss: 0.45849807602048176\n",
      "iteration 1302train_loss: 0.4268975870409909 test_loss: 0.458499301050527\n",
      "iteration 1303train_loss: 0.4268975456733601 test_loss: 0.4585005251092677\n",
      "iteration 1304train_loss: 0.4268975044468335 test_loss: 0.4585017481984692\n",
      "iteration 1305train_loss: 0.4268974633606016 test_loss: 0.45850297031988957\n",
      "iteration 1306train_loss: 0.4268974224138607 test_loss: 0.45850419147528154\n",
      "iteration 1307train_loss: 0.42689738160581225 test_loss: 0.4585054116663905\n",
      "iteration 1308train_loss: 0.4268973409356632 test_loss: 0.45850663089495636\n",
      "iteration 1309train_loss: 0.426897300402626 test_loss: 0.45850784916271226\n",
      "iteration 1310train_loss: 0.42689726000591854 test_loss: 0.45850906647138506\n",
      "iteration 1311train_loss: 0.4268972197447639 test_loss: 0.4585102828226957\n",
      "iteration 1312train_loss: 0.42689717961839074 test_loss: 0.45851149821835857\n",
      "iteration 1313train_loss: 0.42689713962603243 test_loss: 0.458512712660082\n",
      "iteration 1314train_loss: 0.4268970997669281 test_loss: 0.45851392614956826\n",
      "iteration 1315train_loss: 0.42689706004032185 test_loss: 0.4585151386885132\n",
      "iteration 1316train_loss: 0.4268970204454631 test_loss: 0.4585163502786069\n",
      "iteration 1317train_loss: 0.42689698098160617 test_loss: 0.4585175609215332\n",
      "iteration 1318train_loss: 0.4268969416480104 test_loss: 0.4585187706189697\n",
      "iteration 1319train_loss: 0.42689690244394046 test_loss: 0.4585199793725881\n",
      "iteration 1320train_loss: 0.42689686336866584 test_loss: 0.4585211871840543\n",
      "iteration 1321train_loss: 0.4268968244214611 test_loss: 0.45852239405502776\n",
      "iteration 1322train_loss: 0.4268967856016056 test_loss: 0.4585235999871624\n",
      "iteration 1323train_loss: 0.4268967469083836 test_loss: 0.45852480498210585\n",
      "iteration 1324train_loss: 0.4268967083410844 test_loss: 0.4585260090415001\n",
      "iteration 1325train_loss: 0.42689666989900193 test_loss: 0.45852721216698095\n",
      "iteration 1326train_loss: 0.42689663158143515 test_loss: 0.4585284143601787\n",
      "iteration 1327train_loss: 0.42689659338768754 test_loss: 0.45852961562271743\n",
      "iteration 1328train_loss: 0.42689655531706744 test_loss: 0.4585308159562156\n",
      "iteration 1329train_loss: 0.42689651736888756 test_loss: 0.4585320153622859\n",
      "iteration 1330train_loss: 0.426896479542466 test_loss: 0.45853321384253515\n",
      "iteration 1331train_loss: 0.4268964418371247 test_loss: 0.45853441139856443\n",
      "iteration 1332train_loss: 0.4268964042521906 test_loss: 0.4585356080319691\n",
      "iteration 1333train_loss: 0.4268963667869953 test_loss: 0.4585368037443388\n",
      "iteration 1334train_loss: 0.42689632944087447 test_loss: 0.4585379985372575\n",
      "iteration 1335train_loss: 0.4268962922131687 test_loss: 0.4585391924123035\n",
      "iteration 1336train_loss: 0.4268962551032228 test_loss: 0.45854038537104963\n",
      "iteration 1337train_loss: 0.4268962181103863 test_loss: 0.45854157741506274\n",
      "iteration 1338train_loss: 0.4268961812340128 test_loss: 0.45854276854590453\n",
      "iteration 1339train_loss: 0.4268961444734603 test_loss: 0.4585439587651307\n",
      "iteration 1340train_loss: 0.4268961078280914 test_loss: 0.4585451480742917\n",
      "iteration 1341train_loss: 0.42689607129727264 test_loss: 0.45854633647493254\n",
      "iteration 1342train_loss: 0.4268960348803752 test_loss: 0.45854752396859233\n",
      "iteration 1343train_loss: 0.4268959985767743 test_loss: 0.458548710556805\n",
      "iteration 1344train_loss: 0.4268959623858493 test_loss: 0.4585498962410991\n",
      "iteration 1345train_loss: 0.42689592630698403 test_loss: 0.4585510810229974\n",
      "iteration 1346train_loss: 0.4268958903395662 test_loss: 0.4585522649040177\n",
      "iteration 1347train_loss: 0.4268958544829876 test_loss: 0.4585534478856722\n",
      "iteration 1348train_loss: 0.42689581873664434 test_loss: 0.4585546299694675\n",
      "iteration 1349train_loss: 0.42689578309993664 test_loss: 0.4585558111569052\n",
      "iteration 1350train_loss: 0.42689574757226845 test_loss: 0.45855699144948164\n",
      "iteration 1351train_loss: 0.42689571215304795 test_loss: 0.45855817084868755\n",
      "iteration 1352train_loss: 0.4268956768416872 test_loss: 0.45855934935600856\n",
      "iteration 1353train_loss: 0.42689564163760224 test_loss: 0.4585605269729249\n",
      "iteration 1354train_loss: 0.426895606540213 test_loss: 0.45856170370091176\n",
      "iteration 1355train_loss: 0.4268955715489436 test_loss: 0.45856287954143926\n",
      "iteration 1356train_loss: 0.42689553666322155 test_loss: 0.4585640544959718\n",
      "iteration 1357train_loss: 0.4268955018824785 test_loss: 0.45856522856596915\n",
      "iteration 1358train_loss: 0.42689546720614974 test_loss: 0.45856640175288554\n",
      "iteration 1359train_loss: 0.4268954326336744 test_loss: 0.45856757405817045\n",
      "iteration 1360train_loss: 0.42689539816449557 test_loss: 0.45856874548326787\n",
      "iteration 1361train_loss: 0.4268953637980598 test_loss: 0.458569916029617\n",
      "iteration 1362train_loss: 0.42689532953381754 test_loss: 0.45857108569865196\n",
      "iteration 1363train_loss: 0.4268952953712226 test_loss: 0.45857225449180145\n",
      "iteration 1364train_loss: 0.4268952613097329 test_loss: 0.4585734224104898\n",
      "iteration 1365train_loss: 0.42689522734880964 test_loss: 0.45857458945613583\n",
      "iteration 1366train_loss: 0.4268951934879178 test_loss: 0.45857575563015346\n",
      "iteration 1367train_loss: 0.42689515972652586 test_loss: 0.4585769209339519\n",
      "iteration 1368train_loss: 0.4268951260641057 test_loss: 0.45857808536893524\n",
      "iteration 1369train_loss: 0.42689509250013297 test_loss: 0.45857924893650254\n",
      "iteration 1370train_loss: 0.4268950590340868 test_loss: 0.4585804116380483\n",
      "iteration 1371train_loss: 0.4268950256654497 test_loss: 0.4585815734749617\n",
      "iteration 1372train_loss: 0.4268949923937075 test_loss: 0.4585827344486277\n",
      "iteration 1373train_loss: 0.4268949592183498 test_loss: 0.45858389456042564\n",
      "iteration 1374train_loss: 0.4268949261388692 test_loss: 0.4585850538117305\n",
      "iteration 1375train_loss: 0.426894893154762 test_loss: 0.45858621220391266\n",
      "iteration 1376train_loss: 0.4268948602655277 test_loss: 0.45858736973833736\n",
      "iteration 1377train_loss: 0.42689482747066904 test_loss: 0.4585885264163652\n",
      "iteration 1378train_loss: 0.4268947947696924 test_loss: 0.45858968223935215\n",
      "iteration 1379train_loss: 0.42689476216210714 test_loss: 0.4585908372086491\n",
      "iteration 1380train_loss: 0.4268947296474259 test_loss: 0.45859199132560285\n",
      "iteration 1381train_loss: 0.4268946972251647 test_loss: 0.45859314459155504\n",
      "iteration 1382train_loss: 0.4268946648948425 test_loss: 0.4585942970078427\n",
      "iteration 1383train_loss: 0.42689463265598193 test_loss: 0.4585954485757986\n",
      "iteration 1384train_loss: 0.4268946005081082 test_loss: 0.45859659929675034\n",
      "iteration 1385train_loss: 0.42689456845075024 test_loss: 0.45859774917202145\n",
      "iteration 1386train_loss: 0.4268945364834394 test_loss: 0.4585988982029305\n",
      "iteration 1387train_loss: 0.42689450460571104 test_loss: 0.45860004639079177\n",
      "iteration 1388train_loss: 0.4268944728171028 test_loss: 0.4586011937369148\n",
      "iteration 1389train_loss: 0.42689444111715574 test_loss: 0.4586023402426047\n",
      "iteration 1390train_loss: 0.4268944095054139 test_loss: 0.45860348590916195\n",
      "iteration 1391train_loss: 0.4268943779814243 test_loss: 0.4586046307378829\n",
      "iteration 1392train_loss: 0.42689434654473707 test_loss: 0.45860577473005903\n",
      "iteration 1393train_loss: 0.42689431519490517 test_loss: 0.4586069178869777\n",
      "iteration 1394train_loss: 0.42689428393148454 test_loss: 0.4586080602099215\n",
      "iteration 1395train_loss: 0.426894252754034 test_loss: 0.4586092017001689\n",
      "iteration 1396train_loss: 0.4268942216621154 test_loss: 0.4586103423589939\n",
      "iteration 1397train_loss: 0.4268941906552935 test_loss: 0.4586114821876661\n",
      "iteration 1398train_loss: 0.4268941597331359 test_loss: 0.4586126211874508\n",
      "iteration 1399train_loss: 0.4268941288952127 test_loss: 0.4586137593596088\n",
      "iteration 1400train_loss: 0.42689409814109736 test_loss: 0.4586148967053969\n",
      "iteration 1401train_loss: 0.42689406747036596 test_loss: 0.45861603322606725\n",
      "iteration 1402train_loss: 0.42689403688259714 test_loss: 0.4586171689228681\n",
      "iteration 1403train_loss: 0.42689400637737257 test_loss: 0.45861830379704294\n",
      "iteration 1404train_loss: 0.42689397595427664 test_loss: 0.4586194378498315\n",
      "iteration 1405train_loss: 0.42689394561289623 test_loss: 0.45862057108246906\n",
      "iteration 1406train_loss: 0.42689391535282123 test_loss: 0.45862170349618675\n",
      "iteration 1407train_loss: 0.4268938851736441 test_loss: 0.4586228350922114\n",
      "iteration 1408train_loss: 0.42689385507496 test_loss: 0.45862396587176574\n",
      "iteration 1409train_loss: 0.4268938250563666 test_loss: 0.45862509583606853\n",
      "iteration 1410train_loss: 0.42689379511746445 test_loss: 0.45862622498633404\n",
      "iteration 1411train_loss: 0.4268937652578564 test_loss: 0.4586273533237725\n",
      "iteration 1412train_loss: 0.42689373547714826 test_loss: 0.4586284808495904\n",
      "iteration 1413train_loss: 0.4268937057749481 test_loss: 0.4586296075649897\n",
      "iteration 1414train_loss: 0.4268936761508668 test_loss: 0.4586307334711686\n",
      "iteration 1415train_loss: 0.4268936466045175 test_loss: 0.458631858569321\n",
      "iteration 1416train_loss: 0.4268936171355164 test_loss: 0.458632982860637\n",
      "iteration 1417train_loss: 0.4268935877434813 test_loss: 0.4586341063463024\n",
      "iteration 1418train_loss: 0.4268935584280334 test_loss: 0.45863522902749937\n",
      "iteration 1419train_loss: 0.42689352918879586 test_loss: 0.4586363509054058\n",
      "iteration 1420train_loss: 0.4268935000253945 test_loss: 0.45863747198119564\n",
      "iteration 1421train_loss: 0.4268934709374575 test_loss: 0.4586385922560392\n",
      "iteration 1422train_loss: 0.42689344192461537 test_loss: 0.45863971173110235\n",
      "iteration 1423train_loss: 0.42689341298650113 test_loss: 0.4586408304075475\n",
      "iteration 1424train_loss: 0.42689338412275035 test_loss: 0.4586419482865329\n",
      "iteration 1425train_loss: 0.42689335533300055 test_loss: 0.458643065369213\n",
      "iteration 1426train_loss: 0.42689332661689217 test_loss: 0.4586441816567383\n",
      "iteration 1427train_loss: 0.4268932979740673 test_loss: 0.45864529715025576\n",
      "iteration 1428train_loss: 0.4268932694041709 test_loss: 0.45864641185090793\n",
      "iteration 1429train_loss: 0.42689324090685005 test_loss: 0.45864752575983414\n",
      "iteration 1430train_loss: 0.42689321248175394 test_loss: 0.45864863887816976\n",
      "iteration 1431train_loss: 0.42689318412853444 test_loss: 0.45864975120704604\n",
      "iteration 1432train_loss: 0.4268931558468452 test_loss: 0.45865086274759076\n",
      "iteration 1433train_loss: 0.42689312763634263 test_loss: 0.458651973500928\n",
      "iteration 1434train_loss: 0.42689309949668486 test_loss: 0.45865308346817774\n",
      "iteration 1435train_loss: 0.42689307142753247 test_loss: 0.45865419265045676\n",
      "iteration 1436train_loss: 0.4268930434285483 test_loss: 0.45865530104887775\n",
      "iteration 1437train_loss: 0.42689301549939723 test_loss: 0.4586564086645498\n",
      "iteration 1438train_loss: 0.4268929876397462 test_loss: 0.4586575154985783\n",
      "iteration 1439train_loss: 0.4268929598492647 test_loss: 0.4586586215520652\n",
      "iteration 1440train_loss: 0.4268929321276239 test_loss: 0.4586597268261085\n",
      "iteration 1441train_loss: 0.4268929044744974 test_loss: 0.4586608313218028\n",
      "iteration 1442train_loss: 0.4268928768895607 test_loss: 0.4586619350402389\n",
      "iteration 1443train_loss: 0.42689284937249145 test_loss: 0.45866303798250413\n",
      "iteration 1444train_loss: 0.4268928219229695 test_loss: 0.45866414014968215\n",
      "iteration 1445train_loss: 0.4268927945406766 test_loss: 0.45866524154285326\n",
      "iteration 1446train_loss: 0.42689276722529657 test_loss: 0.458666342163094\n",
      "iteration 1447train_loss: 0.42689273997651533 test_loss: 0.4586674420114773\n",
      "iteration 1448train_loss: 0.4268927127940207 test_loss: 0.45866854108907285\n",
      "iteration 1449train_loss: 0.4268926856775026 test_loss: 0.4586696393969465\n",
      "iteration 1450train_loss: 0.426892658626653 test_loss: 0.45867073693616106\n",
      "iteration 1451train_loss: 0.42689263164116553 test_loss: 0.4586718337077754\n",
      "iteration 1452train_loss: 0.4268926047207361 test_loss: 0.45867292971284523\n",
      "iteration 1453train_loss: 0.4268925778650627 test_loss: 0.4586740249524225\n",
      "iteration 1454train_loss: 0.4268925510738446 test_loss: 0.45867511942755623\n",
      "iteration 1455train_loss: 0.42689252434678376 test_loss: 0.45867621313929136\n",
      "iteration 1456train_loss: 0.4268924976835835 test_loss: 0.45867730608867013\n",
      "iteration 1457train_loss: 0.4268924710839493 test_loss: 0.4586783982767309\n",
      "iteration 1458train_loss: 0.42689244454758823 test_loss: 0.45867948970450884\n",
      "iteration 1459train_loss: 0.4268924180742097 test_loss: 0.45868058037303566\n",
      "iteration 1460train_loss: 0.4268923916635247 test_loss: 0.45868167028333984\n",
      "iteration 1461train_loss: 0.4268923653152459 test_loss: 0.45868275943644643\n",
      "iteration 1462train_loss: 0.42689233902908813 test_loss: 0.45868384783337723\n",
      "iteration 1463train_loss: 0.4268923128047677 test_loss: 0.4586849354751507\n",
      "iteration 1464train_loss: 0.42689228664200307 test_loss: 0.45868602236278205\n",
      "iteration 1465train_loss: 0.42689226054051416 test_loss: 0.45868710849728306\n",
      "iteration 1466train_loss: 0.4268922345000228 test_loss: 0.4586881938796626\n",
      "iteration 1467train_loss: 0.42689220852025284 test_loss: 0.4586892785109259\n",
      "iteration 1468train_loss: 0.42689218260092937 test_loss: 0.4586903623920751\n",
      "iteration 1469train_loss: 0.42689215674177966 test_loss: 0.4586914455241093\n",
      "iteration 1470train_loss: 0.4268921309425325 test_loss: 0.45869252790802406\n",
      "iteration 1471train_loss: 0.4268921052029183 test_loss: 0.458693609544812\n",
      "iteration 1472train_loss: 0.42689207952266944 test_loss: 0.4586946904354625\n",
      "iteration 1473train_loss: 0.4268920539015199 test_loss: 0.4586957705809617\n",
      "iteration 1474train_loss: 0.42689202833920525 test_loss: 0.4586968499822927\n",
      "iteration 1475train_loss: 0.42689200283546286 test_loss: 0.45869792864043535\n",
      "iteration 1476train_loss: 0.4268919773900314 test_loss: 0.4586990065563664\n",
      "iteration 1477train_loss: 0.4268919520026518 test_loss: 0.45870008373105964\n",
      "iteration 1478train_loss: 0.4268919266730659 test_loss: 0.4587011601654855\n",
      "iteration 1479train_loss: 0.426891901401018 test_loss: 0.4587022358606115\n",
      "iteration 1480train_loss: 0.42689187618625324 test_loss: 0.45870331081740207\n",
      "iteration 1481train_loss: 0.4268918510285188 test_loss: 0.4587043850368185\n",
      "iteration 1482train_loss: 0.42689182592756336 test_loss: 0.4587054585198191\n",
      "iteration 1483train_loss: 0.42689180088313694 test_loss: 0.4587065312673591\n",
      "iteration 1484train_loss: 0.42689177589499167 test_loss: 0.45870760328039084\n",
      "iteration 1485train_loss: 0.42689175096288046 test_loss: 0.4587086745598635\n",
      "iteration 1486train_loss: 0.4268917260865587 test_loss: 0.45870974510672324\n",
      "iteration 1487train_loss: 0.42689170126578235 test_loss: 0.45871081492191346\n",
      "iteration 1488train_loss: 0.4268916765003096 test_loss: 0.4587118840063743\n",
      "iteration 1489train_loss: 0.42689165178989996 test_loss: 0.4587129523610432\n",
      "iteration 1490train_loss: 0.4268916271343143 test_loss: 0.45871401998685446\n",
      "iteration 1491train_loss: 0.426891602533315 test_loss: 0.4587150868847395\n",
      "iteration 1492train_loss: 0.4268915779866663 test_loss: 0.45871615305562696\n",
      "iteration 1493train_loss: 0.4268915534941333 test_loss: 0.45871721850044234\n",
      "iteration 1494train_loss: 0.4268915290554831 test_loss: 0.4587182832201085\n",
      "iteration 1495train_loss: 0.4268915046704838 test_loss: 0.458719347215545\n",
      "iteration 1496train_loss: 0.4268914803389053 test_loss: 0.4587204104876692\n",
      "iteration 1497train_loss: 0.42689145606051887 test_loss: 0.45872147303739497\n",
      "iteration 1498train_loss: 0.426891431835097 test_loss: 0.45872253486563364\n",
      "iteration 1499train_loss: 0.4268914076624138 test_loss: 0.4587235959732936\n",
      "iteration 1500train_loss: 0.42689138354224465 test_loss: 0.4587246563612805\n",
      "iteration 1501train_loss: 0.42689135947436646 test_loss: 0.45872571603049705\n",
      "iteration 1502train_loss: 0.42689133545855734 test_loss: 0.45872677498184333\n",
      "iteration 1503train_loss: 0.4268913114945969 test_loss: 0.4587278332162165\n",
      "iteration 1504train_loss: 0.4268912875822662 test_loss: 0.4587288907345111\n",
      "iteration 1505train_loss: 0.4268912637213474 test_loss: 0.4587299475376188\n",
      "iteration 1506train_loss: 0.42689123991162425 test_loss: 0.4587310036264284\n",
      "iteration 1507train_loss: 0.42689121615288167 test_loss: 0.45873205900182606\n",
      "iteration 1508train_loss: 0.42689119244490603 test_loss: 0.4587331136646955\n",
      "iteration 1509train_loss: 0.42689116878748484 test_loss: 0.45873416761591707\n",
      "iteration 1510train_loss: 0.42689114518040716 test_loss: 0.4587352208563692\n",
      "iteration 1511train_loss: 0.42689112162346315 test_loss: 0.45873627338692685\n",
      "iteration 1512train_loss: 0.4268910981164444 test_loss: 0.4587373252084631\n",
      "iteration 1513train_loss: 0.42689107465914367 test_loss: 0.4587383763218477\n",
      "iteration 1514train_loss: 0.42689105125135507 test_loss: 0.45873942672794793\n",
      "iteration 1515train_loss: 0.42689102789287386 test_loss: 0.45874047642762855\n",
      "iteration 1516train_loss: 0.42689100458349666 test_loss: 0.45874152542175173\n",
      "iteration 1517train_loss: 0.4268909813230215 test_loss: 0.4587425737111766\n",
      "iteration 1518train_loss: 0.42689095811124717 test_loss: 0.45874362129676033\n",
      "iteration 1519train_loss: 0.42689093494797425 test_loss: 0.45874466817935683\n",
      "iteration 1520train_loss: 0.4268909118330041 test_loss: 0.45874571435981787\n",
      "iteration 1521train_loss: 0.4268908887661395 test_loss: 0.4587467598389924\n",
      "iteration 1522train_loss: 0.42689086574718443 test_loss: 0.45874780461772696\n",
      "iteration 1523train_loss: 0.4268908427759441 test_loss: 0.4587488486968654\n",
      "iteration 1524train_loss: 0.42689081985222466 test_loss: 0.458749892077249\n",
      "iteration 1525train_loss: 0.4268907969758337 test_loss: 0.4587509347597168\n",
      "iteration 1526train_loss: 0.42689077414658 test_loss: 0.458751976745105\n",
      "iteration 1527train_loss: 0.42689075136427335 test_loss: 0.45875301803424723\n",
      "iteration 1528train_loss: 0.4268907286287247 test_loss: 0.4587540586279749\n",
      "iteration 1529train_loss: 0.4268907059397463 test_loss: 0.4587550985271168\n",
      "iteration 1530train_loss: 0.42689068329715146 test_loss: 0.45875613773249924\n",
      "iteration 1531train_loss: 0.4268906607007544 test_loss: 0.4587571762449459\n",
      "iteration 1532train_loss: 0.42689063815037087 test_loss: 0.4587582140652782\n",
      "iteration 1533train_loss: 0.42689061564581743 test_loss: 0.45875925119431515\n",
      "iteration 1534train_loss: 0.4268905931869119 test_loss: 0.4587602876328732\n",
      "iteration 1535train_loss: 0.42689057077347314 test_loss: 0.45876132338176623\n",
      "iteration 1536train_loss: 0.4268905484053211 test_loss: 0.4587623584418059\n",
      "iteration 1537train_loss: 0.42689052608227684 test_loss: 0.4587633928138016\n",
      "iteration 1538train_loss: 0.42689050380416255 test_loss: 0.4587644264985598\n",
      "iteration 1539train_loss: 0.4268904815708013 test_loss: 0.4587654594968852\n",
      "iteration 1540train_loss: 0.42689045938201753 test_loss: 0.4587664918095797\n",
      "iteration 1541train_loss: 0.42689043723763637 test_loss: 0.4587675234374429\n",
      "iteration 1542train_loss: 0.42689041513748427 test_loss: 0.4587685543812723\n",
      "iteration 1543train_loss: 0.4268903930813888 test_loss: 0.4587695846418626\n",
      "iteration 1544train_loss: 0.42689037106917815 test_loss: 0.4587706142200066\n",
      "iteration 1545train_loss: 0.426890349100682 test_loss: 0.45877164311649443\n",
      "iteration 1546train_loss: 0.42689032717573083 test_loss: 0.45877267133211425\n",
      "iteration 1547train_loss: 0.426890305294156 test_loss: 0.4587736988676515\n",
      "iteration 1548train_loss: 0.4268902834557902 test_loss: 0.4587747257238896\n",
      "iteration 1549train_loss: 0.42689026166046684 test_loss: 0.4587757519016097\n",
      "iteration 1550train_loss: 0.42689023990802055 test_loss: 0.45877677740159034\n",
      "iteration 1551train_loss: 0.4268902181982866 test_loss: 0.4587778022246081\n",
      "iteration 1552train_loss: 0.42689019653110183 test_loss: 0.45877882637143746\n",
      "iteration 1553train_loss: 0.4268901749063032 test_loss: 0.45877984984285014\n",
      "iteration 1554train_loss: 0.4268901533237297 test_loss: 0.4587808726396159\n",
      "iteration 1555train_loss: 0.42689013178322033 test_loss: 0.4587818947625024\n",
      "iteration 1556train_loss: 0.4268901102846155 test_loss: 0.4587829162122747\n",
      "iteration 1557train_loss: 0.42689008882775653 test_loss: 0.45878393698969616\n",
      "iteration 1558train_loss: 0.4268900674124857 test_loss: 0.4587849570955274\n",
      "iteration 1559train_loss: 0.4268900460386461 test_loss: 0.45878597653052733\n",
      "iteration 1560train_loss: 0.4268900247060818 test_loss: 0.4587869952954522\n",
      "iteration 1561train_loss: 0.42689000341463773 test_loss: 0.4587880133910564\n",
      "iteration 1562train_loss: 0.4268899821641599 test_loss: 0.4587890308180919\n",
      "iteration 1563train_loss: 0.4268899609544953 test_loss: 0.45879004757730907\n",
      "iteration 1564train_loss: 0.4268899397854914 test_loss: 0.45879106366945555\n",
      "iteration 1565train_loss: 0.4268899186569969 test_loss: 0.45879207909527686\n",
      "iteration 1566train_loss: 0.4268898975688614 test_loss: 0.4587930938555168\n",
      "iteration 1567train_loss: 0.4268898765209352 test_loss: 0.45879410795091663\n",
      "iteration 1568train_loss: 0.42688985551306974 test_loss: 0.4587951213822156\n",
      "iteration 1569train_loss: 0.42688983454511703 test_loss: 0.45879613415015125\n",
      "iteration 1570train_loss: 0.42688981361693007 test_loss: 0.4587971462554584\n",
      "iteration 1571train_loss: 0.42688979272836286 test_loss: 0.4587981576988702\n",
      "iteration 1572train_loss: 0.42688977187927013 test_loss: 0.4587991684811177\n",
      "iteration 1573train_loss: 0.42688975106950733 test_loss: 0.4588001786029296\n",
      "iteration 1574train_loss: 0.426889730298931 test_loss: 0.4588011880650328\n",
      "iteration 1575train_loss: 0.42688970956739847 test_loss: 0.458802196868152\n",
      "iteration 1576train_loss: 0.42688968887476775 test_loss: 0.4588032050130102\n",
      "iteration 1577train_loss: 0.4268896682208977 test_loss: 0.4588042125003277\n",
      "iteration 1578train_loss: 0.4268896476056483 test_loss: 0.45880521933082347\n",
      "iteration 1579train_loss: 0.4268896270288797 test_loss: 0.45880622550521405\n",
      "iteration 1580train_loss: 0.42688960649045377 test_loss: 0.458807231024214\n",
      "iteration 1581train_loss: 0.4268895859902324 test_loss: 0.4588082358885361\n",
      "iteration 1582train_loss: 0.42688956552807855 test_loss: 0.45880924009889107\n",
      "iteration 1583train_loss: 0.42688954510385607 test_loss: 0.45881024365598716\n",
      "iteration 1584train_loss: 0.4268895247174296 test_loss: 0.4588112465605315\n",
      "iteration 1585train_loss: 0.4268895043686643 test_loss: 0.4588122488132287\n",
      "iteration 1586train_loss: 0.4268894840574263 test_loss: 0.45881325041478144\n",
      "iteration 1587train_loss: 0.42688946378358267 test_loss: 0.4588142513658906\n",
      "iteration 1588train_loss: 0.4268894435470008 test_loss: 0.45881525166725495\n",
      "iteration 1589train_loss: 0.4268894233475493 test_loss: 0.45881625131957154\n",
      "iteration 1590train_loss: 0.4268894031850974 test_loss: 0.4588172503235353\n",
      "iteration 1591train_loss: 0.42688938305951496 test_loss: 0.4588182486798395\n",
      "iteration 1592train_loss: 0.42688936297067254 test_loss: 0.4588192463891751\n",
      "iteration 1593train_loss: 0.42688934291844155 test_loss: 0.4588202434522313\n",
      "iteration 1594train_loss: 0.4268893229026943 test_loss: 0.4588212398696958\n",
      "iteration 1595train_loss: 0.4268893029233037 test_loss: 0.458822235642254\n",
      "iteration 1596train_loss: 0.42688928298014306 test_loss: 0.45882323077058945\n",
      "iteration 1597train_loss: 0.426889263073087 test_loss: 0.4588242252553839\n",
      "iteration 1598train_loss: 0.42688924320201044 test_loss: 0.4588252190973173\n",
      "iteration 1599train_loss: 0.42688922336678914 test_loss: 0.4588262122970676\n",
      "iteration 1600train_loss: 0.4268892035672997 test_loss: 0.45882720485531114\n",
      "iteration 1601train_loss: 0.42688918380341906 test_loss: 0.4588281967727221\n",
      "iteration 1602train_loss: 0.4268891640750252 test_loss: 0.45882918804997314\n",
      "iteration 1603train_loss: 0.42688914438199677 test_loss: 0.45883017868773485\n",
      "iteration 1604train_loss: 0.4268891247242129 test_loss: 0.45883116868667617\n",
      "iteration 1605train_loss: 0.42688910510155353 test_loss: 0.4588321580474642\n",
      "iteration 1606train_loss: 0.4268890855138993 test_loss: 0.45883314677076414\n",
      "iteration 1607train_loss: 0.4268890659611314 test_loss: 0.45883413485723956\n",
      "iteration 1608train_loss: 0.426889046443132 test_loss: 0.4588351223075519\n",
      "iteration 1609train_loss: 0.42688902695978365 test_loss: 0.4588361091223616\n",
      "iteration 1610train_loss: 0.4268890075109695 test_loss: 0.4588370953023263\n",
      "iteration 1611train_loss: 0.42688898809657355 test_loss: 0.45883808084810257\n",
      "iteration 1612train_loss: 0.42688896871648024 test_loss: 0.45883906576034506\n",
      "iteration 1613train_loss: 0.4268889493705752 test_loss: 0.45884005003970674\n",
      "iteration 1614train_loss: 0.426888930058744 test_loss: 0.45884103368683843\n",
      "iteration 1615train_loss: 0.4268889107808732 test_loss: 0.45884201670238983\n",
      "iteration 1616train_loss: 0.42688889153685 test_loss: 0.4588429990870086\n",
      "iteration 1617train_loss: 0.4268888723265622 test_loss: 0.45884398084134065\n",
      "iteration 1618train_loss: 0.42688885314989816 test_loss: 0.45884496196603014\n",
      "iteration 1619train_loss: 0.42688883400674704 test_loss: 0.4588459424617196\n",
      "iteration 1620train_loss: 0.4268888148969982 test_loss: 0.4588469223290501\n",
      "iteration 1621train_loss: 0.42688879582054234 test_loss: 0.4588479015686606\n",
      "iteration 1622train_loss: 0.42688877677727005 test_loss: 0.45884888018118863\n",
      "iteration 1623train_loss: 0.4268887577670728 test_loss: 0.45884985816727003\n",
      "iteration 1624train_loss: 0.42688873878984285 test_loss: 0.45885083552753875\n",
      "iteration 1625train_loss: 0.4268887198454727 test_loss: 0.4588518122626275\n",
      "iteration 1626train_loss: 0.4268887009338558 test_loss: 0.4588527883731669\n",
      "iteration 1627train_loss: 0.42688868205488584 test_loss: 0.4588537638597862\n",
      "iteration 1628train_loss: 0.4268886632084572 test_loss: 0.4588547387231129\n",
      "iteration 1629train_loss: 0.42688864439446533 test_loss: 0.4588557129637728\n",
      "iteration 1630train_loss: 0.4268886256128055 test_loss: 0.45885668658239026\n",
      "iteration 1631train_loss: 0.42688860686337393 test_loss: 0.4588576595795879\n",
      "iteration 1632train_loss: 0.4268885881460674 test_loss: 0.4588586319559867\n",
      "iteration 1633train_loss: 0.42688856946078335 test_loss: 0.45885960371220613\n",
      "iteration 1634train_loss: 0.42688855080741955 test_loss: 0.45886057484886394\n",
      "iteration 1635train_loss: 0.42688853218587447 test_loss: 0.4588615453665764\n",
      "iteration 1636train_loss: 0.4268885135960471 test_loss: 0.458862515265958\n",
      "iteration 1637train_loss: 0.426888495037837 test_loss: 0.458863484547622\n",
      "iteration 1638train_loss: 0.42688847651114425 test_loss: 0.45886445321217967\n",
      "iteration 1639train_loss: 0.4268884580158696 test_loss: 0.458865421260241\n",
      "iteration 1640train_loss: 0.4268884395519142 test_loss: 0.45886638869241436\n",
      "iteration 1641train_loss: 0.4268884211191797 test_loss: 0.4588673555093065\n",
      "iteration 1642train_loss: 0.4268884027175683 test_loss: 0.45886832171152264\n",
      "iteration 1643train_loss: 0.42688838434698295 test_loss: 0.45886928729966664\n",
      "iteration 1644train_loss: 0.4268883660073269 test_loss: 0.45887025227434036\n",
      "iteration 1645train_loss: 0.426888347698504 test_loss: 0.45887121663614455\n",
      "iteration 1646train_loss: 0.42688832942041854 test_loss: 0.4588721803856785\n",
      "iteration 1647train_loss: 0.42688831117297543 test_loss: 0.45887314352353964\n",
      "iteration 1648train_loss: 0.4268882929560801 test_loss: 0.458874106050324\n",
      "iteration 1649train_loss: 0.4268882747696385 test_loss: 0.4588750679666263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1650train_loss: 0.4268882566135569 test_loss: 0.45887602927303955\n",
      "iteration 1651train_loss: 0.42688823848774216 test_loss: 0.45887698997015525\n",
      "iteration 1652train_loss: 0.42688822039210184 test_loss: 0.45887795005856374\n",
      "iteration 1653train_loss: 0.4268882023265438 test_loss: 0.45887890953885324\n",
      "iteration 1654train_loss: 0.4268881842909764 test_loss: 0.4588798684116113\n",
      "iteration 1655train_loss: 0.4268881662853085 test_loss: 0.4588808266774235\n",
      "iteration 1656train_loss: 0.4268881483094495 test_loss: 0.458881784336874\n",
      "iteration 1657train_loss: 0.42688813036330925 test_loss: 0.4588827413905456\n",
      "iteration 1658train_loss: 0.42688811244679814 test_loss: 0.45888369783901956\n",
      "iteration 1659train_loss: 0.42688809455982685 test_loss: 0.4588846536828758\n",
      "iteration 1660train_loss: 0.4268880767023068 test_loss: 0.45888560892269264\n",
      "iteration 1661train_loss: 0.4268880588741496 test_loss: 0.4588865635590473\n",
      "iteration 1662train_loss: 0.4268880410752676 test_loss: 0.45888751759251517\n",
      "iteration 1663train_loss: 0.42688802330557346 test_loss: 0.45888847102367053\n",
      "iteration 1664train_loss: 0.42688800556498024 test_loss: 0.4588894238530862\n",
      "iteration 1665train_loss: 0.42688798785340165 test_loss: 0.4588903760813333\n",
      "iteration 1666train_loss: 0.42688797017075164 test_loss: 0.4588913277089819\n",
      "iteration 1667train_loss: 0.4268879525169449 test_loss: 0.45889227873660055\n",
      "iteration 1668train_loss: 0.42688793489189614 test_loss: 0.4588932291647564\n",
      "iteration 1669train_loss: 0.42688791729552106 test_loss: 0.4588941789940152\n",
      "iteration 1670train_loss: 0.4268878997277353 test_loss: 0.45889512822494133\n",
      "iteration 1671train_loss: 0.426887882188455 test_loss: 0.4588960768580978\n",
      "iteration 1672train_loss: 0.4268878646775973 test_loss: 0.4588970248940463\n",
      "iteration 1673train_loss: 0.42688784719507905 test_loss: 0.45889797233334706\n",
      "iteration 1674train_loss: 0.4268878297408181 test_loss: 0.45889891917655906\n",
      "iteration 1675train_loss: 0.4268878123147321 test_loss: 0.45889986542423983\n",
      "iteration 1676train_loss: 0.4268877949167399 test_loss: 0.45890081107694564\n",
      "iteration 1677train_loss: 0.42688777754676016 test_loss: 0.45890175613523143\n",
      "iteration 1678train_loss: 0.4268877602047122 test_loss: 0.45890270059965077\n",
      "iteration 1679train_loss: 0.4268877428905158 test_loss: 0.4589036444707558\n",
      "iteration 1680train_loss: 0.42688772560409094 test_loss: 0.45890458774909737\n",
      "iteration 1681train_loss: 0.42688770834535833 test_loss: 0.4589055304352253\n",
      "iteration 1682train_loss: 0.4268876911142388 test_loss: 0.4589064725296877\n",
      "iteration 1683train_loss: 0.4268876739106538 test_loss: 0.4589074140330316\n",
      "iteration 1684train_loss: 0.426887656734525 test_loss: 0.4589083549458027\n",
      "iteration 1685train_loss: 0.42688763958577464 test_loss: 0.4589092952685452\n",
      "iteration 1686train_loss: 0.4268876224643252 test_loss: 0.4589102350018024\n",
      "iteration 1687train_loss: 0.42688760537009984 test_loss: 0.458911174146116\n",
      "iteration 1688train_loss: 0.42688758830302165 test_loss: 0.4589121127020266\n",
      "iteration 1689train_loss: 0.4268875712630146 test_loss: 0.4589130506700733\n",
      "iteration 1690train_loss: 0.4268875542500025 test_loss: 0.45891398805079425\n",
      "iteration 1691train_loss: 0.42688753726391027 test_loss: 0.458914924844726\n",
      "iteration 1692train_loss: 0.4268875203046626 test_loss: 0.45891586105240406\n",
      "iteration 1693train_loss: 0.42688750337218473 test_loss: 0.45891679667436264\n",
      "iteration 1694train_loss: 0.42688748646640245 test_loss: 0.4589177317111347\n",
      "iteration 1695train_loss: 0.4268874695872417 test_loss: 0.45891866616325183\n",
      "iteration 1696train_loss: 0.4268874527346291 test_loss: 0.4589196000312447\n",
      "iteration 1697train_loss: 0.42688743590849104 test_loss: 0.45892053331564236\n",
      "iteration 1698train_loss: 0.4268874191087551 test_loss: 0.45892146601697287\n",
      "iteration 1699train_loss: 0.42688740233534855 test_loss: 0.458922398135763\n",
      "iteration 1700train_loss: 0.4268873855881994 test_loss: 0.45892332967253824\n",
      "iteration 1701train_loss: 0.42688736886723583 test_loss: 0.45892426062782304\n",
      "iteration 1702train_loss: 0.42688735217238644 test_loss: 0.4589251910021406\n",
      "iteration 1703train_loss: 0.42688733550358027 test_loss: 0.4589261207960127\n",
      "iteration 1704train_loss: 0.4268873188607466 test_loss: 0.4589270500099601\n",
      "iteration 1705train_loss: 0.42688730224381505 test_loss: 0.45892797864450235\n",
      "iteration 1706train_loss: 0.4268872856527157 test_loss: 0.45892890670015773\n",
      "iteration 1707train_loss: 0.42688726908737895 test_loss: 0.4589298341774436\n",
      "iteration 1708train_loss: 0.4268872525477355 test_loss: 0.45893076107687564\n",
      "iteration 1709train_loss: 0.42688723603371637 test_loss: 0.4589316873989689\n",
      "iteration 1710train_loss: 0.426887219545253 test_loss: 0.45893261314423694\n",
      "iteration 1711train_loss: 0.42688720308227707 test_loss: 0.4589335383131923\n",
      "iteration 1712train_loss: 0.42688718664472075 test_loss: 0.4589344629063461\n",
      "iteration 1713train_loss: 0.42688717023251643 test_loss: 0.45893538692420865\n",
      "iteration 1714train_loss: 0.426887153845597 test_loss: 0.45893631036728905\n",
      "iteration 1715train_loss: 0.4268871374838952 test_loss: 0.45893723323609487\n",
      "iteration 1716train_loss: 0.4268871211473447 test_loss: 0.4589381555311331\n",
      "iteration 1717train_loss: 0.4268871048358791 test_loss: 0.45893907725290906\n",
      "iteration 1718train_loss: 0.4268870885494326 test_loss: 0.4589399984019274\n",
      "iteration 1719train_loss: 0.4268870722879394 test_loss: 0.45894091897869127\n",
      "iteration 1720train_loss: 0.42688705605133437 test_loss: 0.4589418389837031\n",
      "iteration 1721train_loss: 0.4268870398395524 test_loss: 0.4589427584174636\n",
      "iteration 1722train_loss: 0.4268870236525289 test_loss: 0.45894367728047314\n",
      "iteration 1723train_loss: 0.42688700749019953 test_loss: 0.4589445955732302\n",
      "iteration 1724train_loss: 0.4268869913525002 test_loss: 0.4589455132962328\n",
      "iteration 1725train_loss: 0.42688697523936714 test_loss: 0.4589464304499774\n",
      "iteration 1726train_loss: 0.4268869591507371 test_loss: 0.4589473470349596\n",
      "iteration 1727train_loss: 0.42688694308654684 test_loss: 0.4589482630516739\n",
      "iteration 1728train_loss: 0.4268869270467336 test_loss: 0.4589491785006135\n",
      "iteration 1729train_loss: 0.4268869110312348 test_loss: 0.4589500933822709\n",
      "iteration 1730train_loss: 0.4268868950399882 test_loss: 0.4589510076971371\n",
      "iteration 1731train_loss: 0.426886879072932 test_loss: 0.4589519214457023\n",
      "iteration 1732train_loss: 0.42688686313000457 test_loss: 0.45895283462845565\n",
      "iteration 1733train_loss: 0.4268868472111445 test_loss: 0.4589537472458849\n",
      "iteration 1734train_loss: 0.42688683131629096 test_loss: 0.45895465929847723\n",
      "iteration 1735train_loss: 0.42688681544538304 test_loss: 0.45895557078671834\n",
      "iteration 1736train_loss: 0.4268867995983604 test_loss: 0.4589564817110931\n",
      "iteration 1737train_loss: 0.42688678377516265 test_loss: 0.45895739207208525\n",
      "iteration 1738train_loss: 0.42688676797573016 test_loss: 0.4589583018701777\n",
      "iteration 1739train_loss: 0.4268867522000033 test_loss: 0.4589592111058518\n",
      "iteration 1740train_loss: 0.4268867364479227 test_loss: 0.45896011977958834\n",
      "iteration 1741train_loss: 0.42688672071942946 test_loss: 0.45896102789186694\n",
      "iteration 1742train_loss: 0.42688670501446463 test_loss: 0.4589619354431662\n",
      "iteration 1743train_loss: 0.42688668933296986 test_loss: 0.4589628424339636\n",
      "iteration 1744train_loss: 0.42688667367488686 test_loss: 0.45896374886473573\n",
      "iteration 1745train_loss: 0.4268866580401579 test_loss: 0.4589646547359581\n",
      "iteration 1746train_loss: 0.42688664242872515 test_loss: 0.4589655600481052\n",
      "iteration 1747train_loss: 0.4268866268405312 test_loss: 0.4589664648016505\n",
      "iteration 1748train_loss: 0.42688661127551897 test_loss: 0.45896736899706664\n",
      "iteration 1749train_loss: 0.42688659573363175 test_loss: 0.4589682726348249\n",
      "iteration 1750train_loss: 0.4268865802148128 test_loss: 0.4589691757153959\n",
      "iteration 1751train_loss: 0.42688656471900577 test_loss: 0.4589700782392491\n",
      "iteration 1752train_loss: 0.42688654924615466 test_loss: 0.45897098020685306\n",
      "iteration 1753train_loss: 0.42688653379620367 test_loss: 0.4589718816186753\n",
      "iteration 1754train_loss: 0.42688651836909736 test_loss: 0.4589727824751823\n",
      "iteration 1755train_loss: 0.4268865029647802 test_loss: 0.45897368277683975\n",
      "iteration 1756train_loss: 0.4268864875831973 test_loss: 0.4589745825241122\n",
      "iteration 1757train_loss: 0.4268864722242938 test_loss: 0.4589754817174633\n",
      "iteration 1758train_loss: 0.42688645688801535 test_loss: 0.45897638035735555\n",
      "iteration 1759train_loss: 0.42688644157430744 test_loss: 0.4589772784442509\n",
      "iteration 1760train_loss: 0.42688642628311607 test_loss: 0.45897817597861\n",
      "iteration 1761train_loss: 0.4268864110143876 test_loss: 0.4589790729608927\n",
      "iteration 1762train_loss: 0.42688639576806847 test_loss: 0.45897996939155766\n",
      "iteration 1763train_loss: 0.42688638054410527 test_loss: 0.45898086527106297\n",
      "iteration 1764train_loss: 0.42688636534244495 test_loss: 0.45898176059986556\n",
      "iteration 1765train_loss: 0.42688635016303467 test_loss: 0.45898265537842137\n",
      "iteration 1766train_loss: 0.4268863350058219 test_loss: 0.45898354960718557\n",
      "iteration 1767train_loss: 0.4268863198707543 test_loss: 0.45898444328661214\n",
      "iteration 1768train_loss: 0.42688630475777994 test_loss: 0.45898533641715467\n",
      "iteration 1769train_loss: 0.42688628966684655 test_loss: 0.4589862289992651\n",
      "iteration 1770train_loss: 0.42688627459790285 test_loss: 0.4589871210333949\n",
      "iteration 1771train_loss: 0.4268862595508973 test_loss: 0.4589880125199947\n",
      "iteration 1772train_loss: 0.4268862445257787 test_loss: 0.4589889034595138\n",
      "iteration 1773train_loss: 0.42688622952249616 test_loss: 0.45898979385240113\n",
      "iteration 1774train_loss: 0.4268862145409989 test_loss: 0.45899068369910406\n",
      "iteration 1775train_loss: 0.4268861995812365 test_loss: 0.45899157300006976\n",
      "iteration 1776train_loss: 0.42688618464315864 test_loss: 0.45899246175574404\n",
      "iteration 1777train_loss: 0.4268861697267153 test_loss: 0.4589933499665719\n",
      "iteration 1778train_loss: 0.42688615483185655 test_loss: 0.45899423763299757\n",
      "iteration 1779train_loss: 0.426886139958533 test_loss: 0.4589951247554642\n",
      "iteration 1780train_loss: 0.42688612510669505 test_loss: 0.4589960113344143\n",
      "iteration 1781train_loss: 0.4268861102762937 test_loss: 0.4589968973702893\n",
      "iteration 1782train_loss: 0.4268860954672798 test_loss: 0.4589977828635297\n",
      "iteration 1783train_loss: 0.4268860806796048 test_loss: 0.45899866781457543\n",
      "iteration 1784train_loss: 0.4268860659132201 test_loss: 0.45899955222386535\n",
      "iteration 1785train_loss: 0.4268860511680775 test_loss: 0.4590004360918373\n",
      "iteration 1786train_loss: 0.4268860364441287 test_loss: 0.45900131941892863\n",
      "iteration 1787train_loss: 0.42688602174132595 test_loss: 0.45900220220557547\n",
      "iteration 1788train_loss: 0.4268860070596214 test_loss: 0.4590030844522132\n",
      "iteration 1789train_loss: 0.4268859923989679 test_loss: 0.4590039661592766\n",
      "iteration 1790train_loss: 0.4268859777593178 test_loss: 0.45900484732719915\n",
      "iteration 1791train_loss: 0.4268859631406243 test_loss: 0.4590057279564139\n",
      "iteration 1792train_loss: 0.4268859485428405 test_loss: 0.45900660804735277\n",
      "iteration 1793train_loss: 0.4268859339659196 test_loss: 0.4590074876004469\n",
      "iteration 1794train_loss: 0.42688591940981535 test_loss: 0.4590083666161269\n",
      "iteration 1795train_loss: 0.4268859048744813 test_loss: 0.45900924509482194\n",
      "iteration 1796train_loss: 0.4268858903598715 test_loss: 0.45901012303696087\n",
      "iteration 1797train_loss: 0.42688587586594007 test_loss: 0.4590110004429716\n",
      "iteration 1798train_loss: 0.4268858613926413 test_loss: 0.459011877313281\n",
      "iteration 1799train_loss: 0.4268858469399297 test_loss: 0.45901275364831556\n",
      "iteration 1800train_loss: 0.4268858325077601 test_loss: 0.4590136294485002\n",
      "iteration 1801train_loss: 0.4268858180960872 test_loss: 0.4590145047142599\n",
      "iteration 1802train_loss: 0.42688580370486645 test_loss: 0.4590153794460183\n",
      "iteration 1803train_loss: 0.4268857893340528 test_loss: 0.4590162536441983\n",
      "iteration 1804train_loss: 0.426885774983602 test_loss: 0.4590171273092221\n",
      "iteration 1805train_loss: 0.4268857606534695 test_loss: 0.459018000441511\n",
      "iteration 1806train_loss: 0.42688574634361115 test_loss: 0.45901887304148575\n",
      "iteration 1807train_loss: 0.4268857320539832 test_loss: 0.4590197451095658\n",
      "iteration 1808train_loss: 0.42688571778454176 test_loss: 0.4590206166461702\n",
      "iteration 1809train_loss: 0.42688570353524324 test_loss: 0.4590214876517172\n",
      "iteration 1810train_loss: 0.4268856893060442 test_loss: 0.45902235812662406\n",
      "iteration 1811train_loss: 0.42688567509690145 test_loss: 0.4590232280713076\n",
      "iteration 1812train_loss: 0.42688566090777197 test_loss: 0.45902409748618334\n",
      "iteration 1813train_loss: 0.42688564673861285 test_loss: 0.45902496637166645\n",
      "iteration 1814train_loss: 0.42688563258938134 test_loss: 0.4590258347281712\n",
      "iteration 1815train_loss: 0.426885618460035 test_loss: 0.45902670255611105\n",
      "iteration 1816train_loss: 0.4268856043505314 test_loss: 0.4590275698558987\n",
      "iteration 1817train_loss: 0.4268855902608286 test_loss: 0.4590284366279461\n",
      "iteration 1818train_loss: 0.42688557619088435 test_loss: 0.4590293028726646\n",
      "iteration 1819train_loss: 0.4268855621406569 test_loss: 0.4590301685904644\n",
      "iteration 1820train_loss: 0.4268855481101048 test_loss: 0.45903103378175525\n",
      "iteration 1821train_loss: 0.42688553409918617 test_loss: 0.45903189844694614\n",
      "iteration 1822train_loss: 0.42688552010786013 test_loss: 0.4590327625864451\n",
      "iteration 1823train_loss: 0.4268855061360853 test_loss: 0.4590336262006597\n",
      "iteration 1824train_loss: 0.42688549218382066 test_loss: 0.4590344892899964\n",
      "iteration 1825train_loss: 0.42688547825102563 test_loss: 0.4590353518548614\n",
      "iteration 1826train_loss: 0.4268854643376592 test_loss: 0.45903621389565963\n",
      "iteration 1827train_loss: 0.4268854504436813 test_loss: 0.45903707541279554\n",
      "iteration 1828train_loss: 0.4268854365690514 test_loss: 0.45903793640667306\n",
      "iteration 1829train_loss: 0.4268854227137292 test_loss: 0.459038796877695\n",
      "iteration 1830train_loss: 0.42688540887767507 test_loss: 0.45903965682626363\n",
      "iteration 1831train_loss: 0.4268853950608489 test_loss: 0.45904051625278053\n",
      "iteration 1832train_loss: 0.4268853812632112 test_loss: 0.4590413751576464\n",
      "iteration 1833train_loss: 0.42688536748472217 test_loss: 0.4590422335412615\n",
      "iteration 1834train_loss: 0.4268853537253427 test_loss: 0.45904309140402505\n",
      "iteration 1835train_loss: 0.42688533998503364 test_loss: 0.45904394874633564\n",
      "iteration 1836train_loss: 0.4268853262637557 test_loss: 0.4590448055685913\n",
      "iteration 1837train_loss: 0.42688531256147016 test_loss: 0.4590456618711892\n",
      "iteration 1838train_loss: 0.4268852988781382 test_loss: 0.45904651765452603\n",
      "iteration 1839train_loss: 0.42688528521372127 test_loss: 0.4590473729189973\n",
      "iteration 1840train_loss: 0.426885271568181 test_loss: 0.4590482276649982\n",
      "iteration 1841train_loss: 0.4268852579414789 test_loss: 0.45904908189292337\n",
      "iteration 1842train_loss: 0.4268852443335771 test_loss: 0.4590499356031664\n",
      "iteration 1843train_loss: 0.42688523074443757 test_loss: 0.4590507887961202\n",
      "iteration 1844train_loss: 0.4268852171740222 test_loss: 0.4590516414721772\n",
      "iteration 1845train_loss: 0.4268852036222937 test_loss: 0.4590524936317289\n",
      "iteration 1846train_loss: 0.4268851900892143 test_loss: 0.4590533452751664\n",
      "iteration 1847train_loss: 0.42688517657474645 test_loss: 0.4590541964028799\n",
      "iteration 1848train_loss: 0.4268851630788533 test_loss: 0.45905504701525907\n",
      "iteration 1849train_loss: 0.42688514960149737 test_loss: 0.45905589711269273\n",
      "iteration 1850train_loss: 0.42688513614264195 test_loss: 0.459056746695569\n",
      "iteration 1851train_loss: 0.4268851227022501 test_loss: 0.4590575957642756\n",
      "iteration 1852train_loss: 0.4268851092802851 test_loss: 0.4590584443191994\n",
      "iteration 1853train_loss: 0.4268850958767106 test_loss: 0.4590592923607266\n",
      "iteration 1854train_loss: 0.42688508249149004 test_loss: 0.4590601398892428\n",
      "iteration 1855train_loss: 0.42688506912458707 test_loss: 0.45906098690513264\n",
      "iteration 1856train_loss: 0.42688505577596575 test_loss: 0.45906183340878065\n",
      "iteration 1857train_loss: 0.42688504244559006 test_loss: 0.4590626794005703\n",
      "iteration 1858train_loss: 0.426885029133424 test_loss: 0.4590635248808844\n",
      "iteration 1859train_loss: 0.4268850158394321 test_loss: 0.4590643698501053\n",
      "iteration 1860train_loss: 0.42688500256357864 test_loss: 0.4590652143086145\n",
      "iteration 1861train_loss: 0.4268849893058282 test_loss: 0.45906605825679314\n",
      "iteration 1862train_loss: 0.42688497606614534 test_loss: 0.45906690169502135\n",
      "iteration 1863train_loss: 0.426884962844495 test_loss: 0.45906774462367894\n",
      "iteration 1864train_loss: 0.42688494964084217 test_loss: 0.4590685870431449\n",
      "iteration 1865train_loss: 0.426884936455152 test_loss: 0.4590694289537974\n",
      "iteration 1866train_loss: 0.4268849232873895 test_loss: 0.4590702703560146\n",
      "iteration 1867train_loss: 0.4268849101375201 test_loss: 0.4590711112501732\n",
      "iteration 1868train_loss: 0.4268848970055093 test_loss: 0.4590719516366499\n",
      "iteration 1869train_loss: 0.42688488389132273 test_loss: 0.4590727915158205\n",
      "iteration 1870train_loss: 0.42688487079492593 test_loss: 0.4590736308880601\n",
      "iteration 1871train_loss: 0.4268848577162849 test_loss: 0.4590744697537435\n",
      "iteration 1872train_loss: 0.42688484465536575 test_loss: 0.45907530811324465\n",
      "iteration 1873train_loss: 0.4268848316121344 test_loss: 0.4590761459669367\n",
      "iteration 1874train_loss: 0.426884818586557 test_loss: 0.4590769833151926\n",
      "iteration 1875train_loss: 0.42688480557860015 test_loss: 0.4590778201583844\n",
      "iteration 1876train_loss: 0.4268847925882301 test_loss: 0.4590786564968835\n",
      "iteration 1877train_loss: 0.42688477961541355 test_loss: 0.459079492331061\n",
      "iteration 1878train_loss: 0.4268847666601172 test_loss: 0.459080327661287\n",
      "iteration 1879train_loss: 0.42688475372230794 test_loss: 0.45908116248793107\n",
      "iteration 1880train_loss: 0.4268847408019527 test_loss: 0.4590819968113627\n",
      "iteration 1881train_loss: 0.4268847278990185 test_loss: 0.4590828306319499\n",
      "iteration 1882train_loss: 0.4268847150134727 test_loss: 0.45908366395006095\n",
      "iteration 1883train_loss: 0.42688470214528235 test_loss: 0.4590844967660626\n",
      "iteration 1884train_loss: 0.426884689294415 test_loss: 0.459085329080322\n",
      "iteration 1885train_loss: 0.42688467646083833 test_loss: 0.459086160893205\n",
      "iteration 1886train_loss: 0.4268846636445198 test_loss: 0.4590869922050771\n",
      "iteration 1887train_loss: 0.42688465084542737 test_loss: 0.45908782301630324\n",
      "iteration 1888train_loss: 0.4268846380635289 test_loss: 0.4590886533272477\n",
      "iteration 1889train_loss: 0.4268846252987923 test_loss: 0.4590894831382742\n",
      "iteration 1890train_loss: 0.4268846125511857 test_loss: 0.45909031244974585\n",
      "iteration 1891train_loss: 0.42688459982067745 test_loss: 0.45909114126202527\n",
      "iteration 1892train_loss: 0.42688458710723576 test_loss: 0.4590919695754743\n",
      "iteration 1893train_loss: 0.4268845744108291 test_loss: 0.45909279739045467\n",
      "iteration 1894train_loss: 0.42688456173142625 test_loss: 0.4590936247073269\n",
      "iteration 1895train_loss: 0.4268845490689957 test_loss: 0.45909445152645123\n",
      "iteration 1896train_loss: 0.42688453642350627 test_loss: 0.4590952778481875\n",
      "iteration 1897train_loss: 0.4268845237949267 test_loss: 0.45909610367289466\n",
      "iteration 1898train_loss: 0.42688451118322623 test_loss: 0.4590969290009314\n",
      "iteration 1899train_loss: 0.426884498588374 test_loss: 0.4590977538326557\n",
      "iteration 1900train_loss: 0.426884486010339 test_loss: 0.4590985781684248\n",
      "iteration 1901train_loss: 0.4268844734490908 test_loss: 0.4590994020085958\n",
      "iteration 1902train_loss: 0.42688446090459864 test_loss: 0.45910022535352485\n",
      "iteration 1903train_loss: 0.4268844483768321 test_loss: 0.4591010482035677\n",
      "iteration 1904train_loss: 0.426884435865761 test_loss: 0.4591018705590795\n",
      "iteration 1905train_loss: 0.42688442337135485 test_loss: 0.45910269242041507\n",
      "iteration 1906train_loss: 0.42688441089358364 test_loss: 0.45910351378792824\n",
      "iteration 1907train_loss: 0.42688439843241716 test_loss: 0.4591043346619727\n",
      "iteration 1908train_loss: 0.4268843859878257 test_loss: 0.4591051550429015\n",
      "iteration 1909train_loss: 0.42688437355977926 test_loss: 0.4591059749310668\n",
      "iteration 1910train_loss: 0.4268843611482481 test_loss: 0.45910679432682083\n",
      "iteration 1911train_loss: 0.4268843487532027 test_loss: 0.4591076132305148\n",
      "iteration 1912train_loss: 0.4268843363746134 test_loss: 0.4591084316424994\n",
      "iteration 1913train_loss: 0.4268843240124508 test_loss: 0.45910924956312515\n",
      "iteration 1914train_loss: 0.42688431166668556 test_loss: 0.45911006699274176\n",
      "iteration 1915train_loss: 0.42688429933728844 test_loss: 0.45911088393169824\n",
      "iteration 1916train_loss: 0.42688428702423026 test_loss: 0.4591117003803435\n",
      "iteration 1917train_loss: 0.42688427472748197 test_loss: 0.45911251633902556\n",
      "iteration 1918train_loss: 0.4268842624470147 test_loss: 0.45911333180809216\n",
      "iteration 1919train_loss: 0.4268842501827996 test_loss: 0.4591141467878902\n",
      "iteration 1920train_loss: 0.4268842379348077 test_loss: 0.45911496127876644\n",
      "iteration 1921train_loss: 0.4268842257030105 test_loss: 0.45911577528106695\n",
      "iteration 1922train_loss: 0.4268842134873795 test_loss: 0.4591165887951371\n",
      "iteration 1923train_loss: 0.4268842012878861 test_loss: 0.45911740182132205\n",
      "iteration 1924train_loss: 0.426884189104502 test_loss: 0.45911821435996625\n",
      "iteration 1925train_loss: 0.4268841769371989 test_loss: 0.4591190264114136\n",
      "iteration 1926train_loss: 0.4268841647859484 test_loss: 0.45911983797600764\n",
      "iteration 1927train_loss: 0.4268841526507227 test_loss: 0.4591206490540914\n",
      "iteration 1928train_loss: 0.42688414053149365 test_loss: 0.45912145964600726\n",
      "iteration 1929train_loss: 0.4268841284282333 test_loss: 0.45912226975209713\n",
      "iteration 1930train_loss: 0.4268841163409139 test_loss: 0.45912307937270247\n",
      "iteration 1931train_loss: 0.4268841042695076 test_loss: 0.4591238885081643\n",
      "iteration 1932train_loss: 0.4268840922139869 test_loss: 0.45912469715882287\n",
      "iteration 1933train_loss: 0.42688408017432405 test_loss: 0.45912550532501833\n",
      "iteration 1934train_loss: 0.4268840681504918 test_loss: 0.45912631300708984\n",
      "iteration 1935train_loss: 0.4268840561424626 test_loss: 0.45912712020537655\n",
      "iteration 1936train_loss: 0.4268840441502091 test_loss: 0.45912792692021687\n",
      "iteration 1937train_loss: 0.4268840321737042 test_loss: 0.45912873315194863\n",
      "iteration 1938train_loss: 0.4268840202129208 test_loss: 0.45912953890090935\n",
      "iteration 1939train_loss: 0.42688400826783185 test_loss: 0.4591303441674361\n",
      "iteration 1940train_loss: 0.4268839963384104 test_loss: 0.45913114895186513\n",
      "iteration 1941train_loss: 0.42688398442462944 test_loss: 0.4591319532545326\n",
      "iteration 1942train_loss: 0.42688397252646243 test_loss: 0.459132757075774\n",
      "iteration 1943train_loss: 0.4268839606438825 test_loss: 0.4591335604159243\n",
      "iteration 1944train_loss: 0.42688394877686303 test_loss: 0.459134363275318\n",
      "iteration 1945train_loss: 0.42688393692537757 test_loss: 0.4591351656542892\n",
      "iteration 1946train_loss: 0.42688392508939965 test_loss: 0.45913596755317154\n",
      "iteration 1947train_loss: 0.42688391326890285 test_loss: 0.45913676897229794\n",
      "iteration 1948train_loss: 0.42688390146386107 test_loss: 0.4591375699120012\n",
      "iteration 1949train_loss: 0.42688388967424784 test_loss: 0.45913837037261335\n",
      "iteration 1950train_loss: 0.4268838779000373 test_loss: 0.4591391703544662\n",
      "iteration 1951train_loss: 0.42688386614120327 test_loss: 0.4591399698578909\n",
      "iteration 1952train_loss: 0.42688385439771975 test_loss: 0.45914076888321814\n",
      "iteration 1953train_loss: 0.42688384266956103 test_loss: 0.45914156743077833\n",
      "iteration 1954train_loss: 0.4268838309567011 test_loss: 0.459142365500901\n",
      "iteration 1955train_loss: 0.4268838192591145 test_loss: 0.4591431630939158\n",
      "iteration 1956train_loss: 0.4268838075767753 test_loss: 0.45914396021015147\n",
      "iteration 1957train_loss: 0.4268837959096581 test_loss: 0.45914475684993644\n",
      "iteration 1958train_loss: 0.42688378425773743 test_loss: 0.4591455530135987\n",
      "iteration 1959train_loss: 0.42688377262098787 test_loss: 0.45914634870146565\n",
      "iteration 1960train_loss: 0.4268837609993841 test_loss: 0.4591471439138646\n",
      "iteration 1961train_loss: 0.42688374939290075 test_loss: 0.4591479386511218\n",
      "iteration 1962train_loss: 0.4268837378015128 test_loss: 0.4591487329135636\n",
      "iteration 1963train_loss: 0.42688372622519494 test_loss: 0.45914952670151565\n",
      "iteration 1964train_loss: 0.42688371466392255 test_loss: 0.4591503200153032\n",
      "iteration 1965train_loss: 0.42688370311767027 test_loss: 0.45915111285525095\n",
      "iteration 1966train_loss: 0.4268836915864133 test_loss: 0.45915190522168337\n",
      "iteration 1967train_loss: 0.426883680070127 test_loss: 0.4591526971149243\n",
      "iteration 1968train_loss: 0.42688366856878657 test_loss: 0.4591534885352972\n",
      "iteration 1969train_loss: 0.42688365708236725 test_loss: 0.4591542794831251\n",
      "iteration 1970train_loss: 0.42688364561084474 test_loss: 0.45915506995873046\n",
      "iteration 1971train_loss: 0.42688363415419417 test_loss: 0.45915585996243563\n",
      "iteration 1972train_loss: 0.42688362271239133 test_loss: 0.4591566494945621\n",
      "iteration 1973train_loss: 0.42688361128541186 test_loss: 0.4591574385554312\n",
      "iteration 1974train_loss: 0.4268835998732314 test_loss: 0.4591582271453637\n",
      "iteration 1975train_loss: 0.4268835884758256 test_loss: 0.4591590152646801\n",
      "iteration 1976train_loss: 0.4268835770931706 test_loss: 0.4591598029137004\n",
      "iteration 1977train_loss: 0.4268835657252422 test_loss: 0.45916059009274396\n",
      "iteration 1978train_loss: 0.42688355437201625 test_loss: 0.4591613768021298\n",
      "iteration 1979train_loss: 0.426883543033469 test_loss: 0.4591621630421769\n",
      "iteration 1980train_loss: 0.4268835317095765 test_loss: 0.45916294881320324\n",
      "iteration 1981train_loss: 0.4268835204003149 test_loss: 0.45916373411552674\n",
      "iteration 1982train_loss: 0.4268835091056606 test_loss: 0.45916451894946475\n",
      "iteration 1983train_loss: 0.42688349782558976 test_loss: 0.45916530331533423\n",
      "iteration 1984train_loss: 0.426883486560079 test_loss: 0.45916608721345187\n",
      "iteration 1985train_loss: 0.42688347530910464 test_loss: 0.4591668706441336\n",
      "iteration 1986train_loss: 0.42688346407264327 test_loss: 0.4591676536076953\n",
      "iteration 1987train_loss: 0.4268834528506715 test_loss: 0.45916843610445224\n",
      "iteration 1988train_loss: 0.426883441643166 test_loss: 0.45916921813471917\n",
      "iteration 1989train_loss: 0.4268834304501033 test_loss: 0.4591699996988106\n",
      "iteration 1990train_loss: 0.4268834192714607 test_loss: 0.4591707807970408\n",
      "iteration 1991train_loss: 0.42688340810721453 test_loss: 0.4591715614297231\n",
      "iteration 1992train_loss: 0.42688339695734207 test_loss: 0.45917234159717096\n",
      "iteration 1993train_loss: 0.4268833858218203 test_loss: 0.4591731212996971\n",
      "iteration 1994train_loss: 0.4268833747006261 test_loss: 0.45917390053761387\n",
      "iteration 1995train_loss: 0.4268833635937368 test_loss: 0.45917467931123346\n",
      "iteration 1996train_loss: 0.4268833525011294 test_loss: 0.45917545762086737\n",
      "iteration 1997train_loss: 0.42688334142278134 test_loss: 0.45917623546682684\n",
      "iteration 1998train_loss: 0.42688333035866977 test_loss: 0.4591770128494226\n",
      "iteration 1999train_loss: 0.42688331930877227 test_loss: 0.45917778976896517\n",
      "iteration 2000train_loss: 0.4268833082730661 test_loss: 0.4591785662257644\n",
      "iteration 2001train_loss: 0.42688329725152885 test_loss: 0.45917934222013007\n",
      "iteration 2002train_loss: 0.4268832862441382 test_loss: 0.4591801177523712\n",
      "iteration 2003train_loss: 0.4268832752508715 test_loss: 0.4591808928227967\n",
      "iteration 2004train_loss: 0.42688326427170675 test_loss: 0.45918166743171507\n",
      "iteration 2005train_loss: 0.4268832533066216 test_loss: 0.4591824415794342\n",
      "iteration 2006train_loss: 0.4268832423555938 test_loss: 0.4591832152662618\n",
      "iteration 2007train_loss: 0.4268832314186012 test_loss: 0.45918398849250514\n",
      "iteration 2008train_loss: 0.42688322049562194 test_loss: 0.459184761258471\n",
      "iteration 2009train_loss: 0.4268832095866338 test_loss: 0.45918553356446584\n",
      "iteration 2010train_loss: 0.426883198691615 test_loss: 0.4591863054107958\n",
      "iteration 2011train_loss: 0.42688318781054346 test_loss: 0.4591870767977665\n",
      "iteration 2012train_loss: 0.4268831769433976 test_loss: 0.4591878477256834\n",
      "iteration 2013train_loss: 0.4268831660901554 test_loss: 0.4591886181948512\n",
      "iteration 2014train_loss: 0.4268831552507955 test_loss: 0.45918938820557464\n",
      "iteration 2015train_loss: 0.42688314442529574 test_loss: 0.45919015775815775\n",
      "iteration 2016train_loss: 0.42688313361363495 test_loss: 0.45919092685290447\n",
      "iteration 2017train_loss: 0.4268831228157916 test_loss: 0.4591916954901181\n",
      "iteration 2018train_loss: 0.42688311203174395 test_loss: 0.4591924636701017\n",
      "iteration 2019train_loss: 0.4268831012614707 test_loss: 0.45919323139315793\n",
      "iteration 2020train_loss: 0.4268830905049505 test_loss: 0.459193998659589\n",
      "iteration 2021train_loss: 0.4268830797621622 test_loss: 0.45919476546969695\n",
      "iteration 2022train_loss: 0.42688306903308426 test_loss: 0.4591955318237832\n",
      "iteration 2023train_loss: 0.4268830583176957 test_loss: 0.4591962977221489\n",
      "iteration 2024train_loss: 0.4268830476159753 test_loss: 0.459197063165095\n",
      "iteration 2025train_loss: 0.42688303692790186 test_loss: 0.45919782815292187\n",
      "iteration 2026train_loss: 0.4268830262534546 test_loss: 0.45919859268592944\n",
      "iteration 2027train_loss: 0.42688301559261244 test_loss: 0.4591993567644176\n",
      "iteration 2028train_loss: 0.42688300494535436 test_loss: 0.4592001203886856\n",
      "iteration 2029train_loss: 0.4268829943116596 test_loss: 0.4592008835590322\n",
      "iteration 2030train_loss: 0.4268829836915074 test_loss: 0.4592016462757564\n",
      "iteration 2031train_loss: 0.4268829730848768 test_loss: 0.45920240853915617\n",
      "iteration 2032train_loss: 0.42688296249174734 test_loss: 0.45920317034952934\n",
      "iteration 2033train_loss: 0.4268829519120984 test_loss: 0.45920393170717366\n",
      "iteration 2034train_loss: 0.42688294134590904 test_loss: 0.4592046926123861\n",
      "iteration 2035train_loss: 0.42688293079315914 test_loss: 0.4592054530654635\n",
      "iteration 2036train_loss: 0.4268829202538278 test_loss: 0.4592062130667025\n",
      "iteration 2037train_loss: 0.4268829097278949 test_loss: 0.4592069726163989\n",
      "iteration 2038train_loss: 0.42688289921533995 test_loss: 0.45920773171484863\n",
      "iteration 2039train_loss: 0.4268828887161425 test_loss: 0.4592084903623471\n",
      "iteration 2040train_loss: 0.4268828782302826 test_loss: 0.45920924855918915\n",
      "iteration 2041train_loss: 0.4268828677577395 test_loss: 0.4592100063056697\n",
      "iteration 2042train_loss: 0.4268828572984935 test_loss: 0.45921076360208296\n",
      "iteration 2043train_loss: 0.4268828468525243 test_loss: 0.4592115204487229\n",
      "iteration 2044train_loss: 0.4268828364198117 test_loss: 0.4592122768458832\n",
      "iteration 2045train_loss: 0.4268828260003359 test_loss: 0.4592130327938572\n",
      "iteration 2046train_loss: 0.4268828155940767 test_loss: 0.45921378829293785\n",
      "iteration 2047train_loss: 0.4268828052010144 test_loss: 0.45921454334341755\n",
      "iteration 2048train_loss: 0.42688279482112895 test_loss: 0.45921529794558885\n",
      "iteration 2049train_loss: 0.4268827844544006 test_loss: 0.4592160520997435\n",
      "iteration 2050train_loss: 0.4268827741008095 test_loss: 0.4592168058061732\n",
      "iteration 2051train_loss: 0.42688276376033596 test_loss: 0.4592175590651691\n",
      "iteration 2052train_loss: 0.4268827534329603 test_loss: 0.45921831187702195\n",
      "iteration 2053train_loss: 0.4268827431186629 test_loss: 0.4592190642420225\n",
      "iteration 2054train_loss: 0.42688273281742417 test_loss: 0.4592198161604611\n",
      "iteration 2055train_loss: 0.4268827225292247 test_loss: 0.4592205676326275\n",
      "iteration 2056train_loss: 0.4268827122540447 test_loss: 0.45922131865881116\n",
      "iteration 2057train_loss: 0.4268827019918649 test_loss: 0.45922206923930137\n",
      "iteration 2058train_loss: 0.42688269174266597 test_loss: 0.45922281937438714\n",
      "iteration 2059train_loss: 0.42688268150642855 test_loss: 0.4592235690643568\n",
      "iteration 2060train_loss: 0.4268826712831333 test_loss: 0.4592243183094988\n",
      "iteration 2061train_loss: 0.42688266107276074 test_loss: 0.45922506711010086\n",
      "iteration 2062train_loss: 0.4268826508752921 test_loss: 0.4592258154664507\n",
      "iteration 2063train_loss: 0.426882640690708 test_loss: 0.45922656337883544\n",
      "iteration 2064train_loss: 0.42688263051898934 test_loss: 0.4592273108475421\n",
      "iteration 2065train_loss: 0.4268826203601169 test_loss: 0.4592280578728572\n",
      "iteration 2066train_loss: 0.42688261021407187 test_loss: 0.45922880445506703\n",
      "iteration 2067train_loss: 0.4268826000808353 test_loss: 0.4592295505944574\n",
      "iteration 2068train_loss: 0.4268825899603881 test_loss: 0.4592302962913141\n",
      "iteration 2069train_loss: 0.4268825798527115 test_loss: 0.4592310415459223\n",
      "iteration 2070train_loss: 0.42688256975778666 test_loss: 0.45923178635856715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2071train_loss: 0.42688255967559463 test_loss: 0.45923253072953313\n",
      "iteration 2072train_loss: 0.42688254960611677 test_loss: 0.4592332746591048\n",
      "iteration 2073train_loss: 0.42688253954933436 test_loss: 0.45923401814756576\n",
      "iteration 2074train_loss: 0.4268825295052289 test_loss: 0.45923476119520007\n",
      "iteration 2075train_loss: 0.42688251947378153 test_loss: 0.45923550380229095\n",
      "iteration 2076train_loss: 0.42688250945497386 test_loss: 0.45923624596912155\n",
      "iteration 2077train_loss: 0.42688249944878703 test_loss: 0.4592369876959747\n",
      "iteration 2078train_loss: 0.4268824894552029 test_loss: 0.4592377289831326\n",
      "iteration 2079train_loss: 0.42688247947420294 test_loss: 0.4592384698308775\n",
      "iteration 2080train_loss: 0.4268824695057687 test_loss: 0.4592392102394913\n",
      "iteration 2081train_loss: 0.42688245954988185 test_loss: 0.4592399502092554\n",
      "iteration 2082train_loss: 0.4268824496065241 test_loss: 0.4592406897404511\n",
      "iteration 2083train_loss: 0.4268824396756771 test_loss: 0.4592414288333591\n",
      "iteration 2084train_loss: 0.42688242975732266 test_loss: 0.4592421674882601\n",
      "iteration 2085train_loss: 0.42688241985144254 test_loss: 0.4592429057054344\n",
      "iteration 2086train_loss: 0.42688240995801874 test_loss: 0.459243643485162\n",
      "iteration 2087train_loss: 0.426882400077033 test_loss: 0.4592443808277224\n",
      "iteration 2088train_loss: 0.4268823902084673 test_loss: 0.459245117733395\n",
      "iteration 2089train_loss: 0.4268823803523036 test_loss: 0.459245854202459\n",
      "iteration 2090train_loss: 0.426882370508524 test_loss: 0.459246590235193\n",
      "iteration 2091train_loss: 0.42688236067711044 test_loss: 0.4592473258318754\n",
      "iteration 2092train_loss: 0.42688235085804516 test_loss: 0.4592480609927846\n",
      "iteration 2093train_loss: 0.4268823410513101 test_loss: 0.45924879571819816\n",
      "iteration 2094train_loss: 0.4268823312568877 test_loss: 0.4592495300083938\n",
      "iteration 2095train_loss: 0.42688232147475996 test_loss: 0.45925026386364864\n",
      "iteration 2096train_loss: 0.4268823117049093 test_loss: 0.4592509972842398\n",
      "iteration 2097train_loss: 0.42688230194731774 test_loss: 0.45925173027044386\n",
      "iteration 2098train_loss: 0.42688229220196805 test_loss: 0.4592524628225372\n",
      "iteration 2099train_loss: 0.42688228246884224 test_loss: 0.4592531949407957\n",
      "iteration 2100train_loss: 0.4268822727479229 test_loss: 0.45925392662549536\n",
      "iteration 2101train_loss: 0.4268822630391925 test_loss: 0.45925465787691155\n",
      "iteration 2102train_loss: 0.42688225334263347 test_loss: 0.45925538869531957\n",
      "iteration 2103train_loss: 0.4268822436582284 test_loss: 0.4592561190809942\n",
      "iteration 2104train_loss: 0.4268822339859598 test_loss: 0.45925684903421\n",
      "iteration 2105train_loss: 0.42688222432581036 test_loss: 0.4592575785552414\n",
      "iteration 2106train_loss: 0.4268822146777628 test_loss: 0.4592583076443624\n",
      "iteration 2107train_loss: 0.4268822050417995 test_loss: 0.4592590363018467\n",
      "iteration 2108train_loss: 0.4268821954179034 test_loss: 0.45925976452796774\n",
      "iteration 2109train_loss: 0.42688218580605736 test_loss: 0.45926049232299865\n",
      "iteration 2110train_loss: 0.4268821762062441 test_loss: 0.45926121968721245\n",
      "iteration 2111train_loss: 0.42688216661844636 test_loss: 0.45926194662088154\n",
      "iteration 2112train_loss: 0.4268821570426471 test_loss: 0.45926267312427843\n",
      "iteration 2113train_loss: 0.4268821474788293 test_loss: 0.45926339919767495\n",
      "iteration 2114train_loss: 0.42688213792697577 test_loss: 0.45926412484134294\n",
      "iteration 2115train_loss: 0.4268821283870697 test_loss: 0.45926485005555384\n",
      "iteration 2116train_loss: 0.42688211885909383 test_loss: 0.45926557484057884\n",
      "iteration 2117train_loss: 0.42688210934303156 test_loss: 0.4592662991966889\n",
      "iteration 2118train_loss: 0.42688209983886577 test_loss: 0.4592670231241544\n",
      "iteration 2119train_loss: 0.4268820903465796 test_loss: 0.45926774662324593\n",
      "iteration 2120train_loss: 0.4268820808661563 test_loss: 0.45926846969423346\n",
      "iteration 2121train_loss: 0.42688207139757905 test_loss: 0.4592691923373868\n",
      "iteration 2122train_loss: 0.4268820619408311 test_loss: 0.45926991455297544\n",
      "iteration 2123train_loss: 0.42688205249589584 test_loss: 0.4592706363412685\n",
      "iteration 2124train_loss: 0.4268820430627563 test_loss: 0.4592713577025352\n",
      "iteration 2125train_loss: 0.4268820336413961 test_loss: 0.4592720786370439\n",
      "iteration 2126train_loss: 0.4268820242317986 test_loss: 0.4592727991450633\n",
      "iteration 2127train_loss: 0.42688201483394694 test_loss: 0.4592735192268614\n",
      "iteration 2128train_loss: 0.42688200544782495 test_loss: 0.459274238882706\n",
      "iteration 2129train_loss: 0.426881996073416 test_loss: 0.45927495811286473\n",
      "iteration 2130train_loss: 0.42688198671070343 test_loss: 0.45927567691760496\n",
      "iteration 2131train_loss: 0.42688197735967104 test_loss: 0.4592763952971939\n",
      "iteration 2132train_loss: 0.42688196802030237 test_loss: 0.4592771132518979\n",
      "iteration 2133train_loss: 0.426881958692581 test_loss: 0.459277830781984\n",
      "iteration 2134train_loss: 0.42688194937649065 test_loss: 0.45927854788771794\n",
      "iteration 2135train_loss: 0.4268819400720149 test_loss: 0.4592792645693661\n",
      "iteration 2136train_loss: 0.42688193077913766 test_loss: 0.45927998082719407\n",
      "iteration 2137train_loss: 0.4268819214978426 test_loss: 0.45928069666146726\n",
      "iteration 2138train_loss: 0.4268819122281135 test_loss: 0.4592814120724508\n",
      "iteration 2139train_loss: 0.42688190296993417 test_loss: 0.4592821270604098\n",
      "iteration 2140train_loss: 0.42688189372328855 test_loss: 0.45928284162560873\n",
      "iteration 2141train_loss: 0.42688188448816056 test_loss: 0.45928355576831204\n",
      "iteration 2142train_loss: 0.426881875264534 test_loss: 0.45928426948878387\n",
      "iteration 2143train_loss: 0.42688186605239303 test_loss: 0.4592849827872882\n",
      "iteration 2144train_loss: 0.42688185685172153 test_loss: 0.4592856956640885\n",
      "iteration 2145train_loss: 0.4268818476625035 test_loss: 0.4592864081194481\n",
      "iteration 2146train_loss: 0.4268818384847231 test_loss: 0.45928712015363027\n",
      "iteration 2147train_loss: 0.42688182931836444 test_loss: 0.4592878317668977\n",
      "iteration 2148train_loss: 0.4268818201634115 test_loss: 0.459288542959513\n",
      "iteration 2149train_loss: 0.4268818110198485 test_loss: 0.4592892537317385\n",
      "iteration 2150train_loss: 0.4268818018876597 test_loss: 0.45928996408383627\n",
      "iteration 2151train_loss: 0.42688179276682936 test_loss: 0.4592906740160682\n",
      "iteration 2152train_loss: 0.4268817836573416 test_loss: 0.4592913835286957\n",
      "iteration 2153train_loss: 0.42688177455918075 test_loss: 0.45929209262198006\n",
      "iteration 2154train_loss: 0.42688176547233136 test_loss: 0.4592928012961825\n",
      "iteration 2155train_loss: 0.4268817563967774 test_loss: 0.4592935095515637\n",
      "iteration 2156train_loss: 0.4268817473325035 test_loss: 0.4592942173883842\n",
      "iteration 2157train_loss: 0.42688173827949405 test_loss: 0.45929492480690426\n",
      "iteration 2158train_loss: 0.4268817292377334 test_loss: 0.4592956318073841\n",
      "iteration 2159train_loss: 0.42688172020720605 test_loss: 0.4592963383900832\n",
      "iteration 2160train_loss: 0.42688171118789664 test_loss: 0.4592970445552613\n",
      "iteration 2161train_loss: 0.42688170217978966 test_loss: 0.4592977503031777\n",
      "iteration 2162train_loss: 0.4268816931828695 test_loss: 0.4592984556340915\n",
      "iteration 2163train_loss: 0.42688168419712097 test_loss: 0.45929916054826125\n",
      "iteration 2164train_loss: 0.4268816752225286 test_loss: 0.4592998650459458\n",
      "iteration 2165train_loss: 0.42688166625907725 test_loss: 0.4593005691274031\n",
      "iteration 2166train_loss: 0.42688165730675137 test_loss: 0.4593012727928915\n",
      "iteration 2167train_loss: 0.4268816483655356 test_loss: 0.4593019760426687\n",
      "iteration 2168train_loss: 0.4268816394354151 test_loss: 0.4593026788769923\n",
      "iteration 2169train_loss: 0.4268816305163742 test_loss: 0.45930338129611964\n",
      "iteration 2170train_loss: 0.426881621608398 test_loss: 0.45930408330030764\n",
      "iteration 2171train_loss: 0.4268816127114713 test_loss: 0.45930478488981324\n",
      "iteration 2172train_loss: 0.42688160382557877 test_loss: 0.45930548606489313\n",
      "iteration 2173train_loss: 0.42688159495070555 test_loss: 0.4593061868258036\n",
      "iteration 2174train_loss: 0.4268815860868365 test_loss: 0.4593068871728007\n",
      "iteration 2175train_loss: 0.4268815772339566 test_loss: 0.45930758710614034\n",
      "iteration 2176train_loss: 0.42688156839205077 test_loss: 0.45930828662607814\n",
      "iteration 2177train_loss: 0.426881559561104 test_loss: 0.45930898573286955\n",
      "iteration 2178train_loss: 0.4268815507411015 test_loss: 0.4593096844267696\n",
      "iteration 2179train_loss: 0.42688154193202826 test_loss: 0.4593103827080334\n",
      "iteration 2180train_loss: 0.4268815331338693 test_loss: 0.4593110805769154\n",
      "iteration 2181train_loss: 0.42688152434660975 test_loss: 0.4593117780336701\n",
      "iteration 2182train_loss: 0.426881515570235 test_loss: 0.45931247507855194\n",
      "iteration 2183train_loss: 0.42688150680472997 test_loss: 0.4593131717118146\n",
      "iteration 2184train_loss: 0.42688149805007997 test_loss: 0.45931386793371193\n",
      "iteration 2185train_loss: 0.42688148930627035 test_loss: 0.4593145637444975\n",
      "iteration 2186train_loss: 0.4268814805732863 test_loss: 0.4593152591444245\n",
      "iteration 2187train_loss: 0.426881471851113 test_loss: 0.4593159541337459\n",
      "iteration 2188train_loss: 0.4268814631397359 test_loss: 0.4593166487127146\n",
      "iteration 2189train_loss: 0.42688145443914033 test_loss: 0.45931734288158327\n",
      "iteration 2190train_loss: 0.4268814457493118 test_loss: 0.459318036640604\n",
      "iteration 2191train_loss: 0.4268814370702355 test_loss: 0.45931872999002915\n",
      "iteration 2192train_loss: 0.4268814284018971 test_loss: 0.45931942293011047\n",
      "iteration 2193train_loss: 0.4268814197442819 test_loss: 0.4593201154610996\n",
      "iteration 2194train_loss: 0.42688141109737543 test_loss: 0.4593208075832479\n",
      "iteration 2195train_loss: 0.4268814024611633 test_loss: 0.4593214992968067\n",
      "iteration 2196train_loss: 0.42688139383563084 test_loss: 0.4593221906020269\n",
      "iteration 2197train_loss: 0.4268813852207638 test_loss: 0.45932288149915923\n",
      "iteration 2198train_loss: 0.42688137661654796 test_loss: 0.4593235719884543\n",
      "iteration 2199train_loss: 0.42688136802296855 test_loss: 0.45932426207016225\n",
      "iteration 2200train_loss: 0.4268813594400115 test_loss: 0.4593249517445332\n",
      "iteration 2201train_loss: 0.42688135086766243 test_loss: 0.459325641011817\n",
      "iteration 2202train_loss: 0.426881342305907 test_loss: 0.4593263298722632\n",
      "iteration 2203train_loss: 0.4268813337547309 test_loss: 0.4593270183261213\n",
      "iteration 2204train_loss: 0.4268813252141201 test_loss: 0.4593277063736403\n",
      "iteration 2205train_loss: 0.42688131668406015 test_loss: 0.45932839401506925\n",
      "iteration 2206train_loss: 0.42688130816453707 test_loss: 0.4593290812506568\n",
      "iteration 2207train_loss: 0.42688129965553656 test_loss: 0.45932976808065157\n",
      "iteration 2208train_loss: 0.42688129115704454 test_loss: 0.4593304545053017\n",
      "iteration 2209train_loss: 0.4268812826690469 test_loss: 0.4593311405248552\n",
      "iteration 2210train_loss: 0.4268812741915295 test_loss: 0.4593318261395601\n",
      "iteration 2211train_loss: 0.4268812657244783 test_loss: 0.45933251134966374\n",
      "iteration 2212train_loss: 0.42688125726787945 test_loss: 0.45933319615541385\n",
      "iteration 2213train_loss: 0.42688124882171874 test_loss: 0.4593338805570573\n",
      "iteration 2214train_loss: 0.4268812403859822 test_loss: 0.45933456455484123\n",
      "iteration 2215train_loss: 0.42688123196065597 test_loss: 0.4593352481490123\n",
      "iteration 2216train_loss: 0.4268812235457261 test_loss: 0.4593359313398171\n",
      "iteration 2217train_loss: 0.42688121514117855 test_loss: 0.4593366141275018\n",
      "iteration 2218train_loss: 0.4268812067469997 test_loss: 0.45933729651231253\n",
      "iteration 2219train_loss: 0.4268811983631755 test_loss: 0.4593379784944952\n",
      "iteration 2220train_loss: 0.4268811899896921 test_loss: 0.4593386600742955\n",
      "iteration 2221train_loss: 0.42688118162653593 test_loss: 0.4593393412519588\n",
      "iteration 2222train_loss: 0.4268811732736928 test_loss: 0.4593400220277304\n",
      "iteration 2223train_loss: 0.42688116493114947 test_loss: 0.4593407024018552\n",
      "iteration 2224train_loss: 0.42688115659889175 test_loss: 0.45934138237457817\n",
      "iteration 2225train_loss: 0.42688114827690626 test_loss: 0.45934206194614374\n",
      "iteration 2226train_loss: 0.4268811399651791 test_loss: 0.45934274111679635\n",
      "iteration 2227train_loss: 0.4268811316636968 test_loss: 0.4593434198867802\n",
      "iteration 2228train_loss: 0.42688112337244555 test_loss: 0.45934409825633915\n",
      "iteration 2229train_loss: 0.4268811150914118 test_loss: 0.45934477622571707\n",
      "iteration 2230train_loss: 0.4268811068205821 test_loss: 0.45934545379515734\n",
      "iteration 2231train_loss: 0.42688109855994266 test_loss: 0.4593461309649034\n",
      "iteration 2232train_loss: 0.42688109030948024 test_loss: 0.45934680773519826\n",
      "iteration 2233train_loss: 0.42688108206918096 test_loss: 0.4593474841062849\n",
      "iteration 2234train_loss: 0.4268810738390317 test_loss: 0.459348160078406\n",
      "iteration 2235train_loss: 0.4268810656190187 test_loss: 0.45934883565180407\n",
      "iteration 2236train_loss: 0.4268810574091286 test_loss: 0.4593495108267214\n",
      "iteration 2237train_loss: 0.4268810492093481 test_loss: 0.45935018560339996\n",
      "iteration 2238train_loss: 0.4268810410196638 test_loss: 0.4593508599820818\n",
      "iteration 2239train_loss: 0.42688103284006207 test_loss: 0.4593515339630083\n",
      "iteration 2240train_loss: 0.4268810246705299 test_loss: 0.45935220754642125\n",
      "iteration 2241train_loss: 0.42688101651105387 test_loss: 0.45935288073256175\n",
      "iteration 2242train_loss: 0.42688100836162046 test_loss: 0.4593535535216708\n",
      "iteration 2243train_loss: 0.4268810002222167 test_loss: 0.4593542259139891\n",
      "iteration 2244train_loss: 0.42688099209282915 test_loss: 0.4593548979097577\n",
      "iteration 2245train_loss: 0.4268809839734446 test_loss: 0.4593555695092167\n",
      "iteration 2246train_loss: 0.4268809758640499 test_loss: 0.4593562407126064\n",
      "iteration 2247train_loss: 0.42688096776463186 test_loss: 0.45935691152016694\n",
      "iteration 2248train_loss: 0.4268809596751772 test_loss: 0.4593575819321381\n",
      "iteration 2249train_loss: 0.426880951595673 test_loss: 0.4593582519487593\n",
      "iteration 2250train_loss: 0.426880943526106 test_loss: 0.4593589215702703\n",
      "iteration 2251train_loss: 0.42688093546646305 test_loss: 0.4593595907969102\n",
      "iteration 2252train_loss: 0.42688092741673117 test_loss: 0.4593602596289179\n",
      "iteration 2253train_loss: 0.42688091937689726 test_loss: 0.4593609280665324\n",
      "iteration 2254train_loss: 0.4268809113469484 test_loss: 0.4593615961099923\n",
      "iteration 2255train_loss: 0.42688090332687145 test_loss: 0.45936226375953576\n",
      "iteration 2256train_loss: 0.42688089531665346 test_loss: 0.4593629310154015\n",
      "iteration 2257train_loss: 0.42688088731628154 test_loss: 0.4593635978778271\n",
      "iteration 2258train_loss: 0.42688087932574276 test_loss: 0.4593642643470506\n",
      "iteration 2259train_loss: 0.42688087134502406 test_loss: 0.4593649304233097\n",
      "iteration 2260train_loss: 0.42688086337411263 test_loss: 0.4593655961068418\n",
      "iteration 2261train_loss: 0.4268808554129957 test_loss: 0.45936626139788395\n",
      "iteration 2262train_loss: 0.42688084746166016 test_loss: 0.4593669262966735\n",
      "iteration 2263train_loss: 0.4268808395200935 test_loss: 0.4593675908034471\n",
      "iteration 2264train_loss: 0.42688083158828266 test_loss: 0.45936825491844147\n",
      "iteration 2265train_loss: 0.42688082366621494 test_loss: 0.4593689186418931\n",
      "iteration 2266train_loss: 0.42688081575387754 test_loss: 0.45936958197403815\n",
      "iteration 2267train_loss: 0.4268808078512579 test_loss: 0.45937024491511286\n",
      "iteration 2268train_loss: 0.4268807999583431 test_loss: 0.4593709074653531\n",
      "iteration 2269train_loss: 0.4268807920751205 test_loss: 0.45937156962499437\n",
      "iteration 2270train_loss: 0.4268807842015773 test_loss: 0.4593722313942723\n",
      "iteration 2271train_loss: 0.4268807763377011 test_loss: 0.4593728927734223\n",
      "iteration 2272train_loss: 0.4268807684834791 test_loss: 0.45937355376267935\n",
      "iteration 2273train_loss: 0.42688076063889874 test_loss: 0.4593742143622783\n",
      "iteration 2274train_loss: 0.4268807528039473 test_loss: 0.4593748745724542\n",
      "iteration 2275train_loss: 0.42688074497861245 test_loss: 0.4593755343934412\n",
      "iteration 2276train_loss: 0.42688073716288133 test_loss: 0.45937619382547396\n",
      "iteration 2277train_loss: 0.4268807293567417 test_loss: 0.45937685286878654\n",
      "iteration 2278train_loss: 0.42688072156018075 test_loss: 0.45937751152361284\n",
      "iteration 2279train_loss: 0.4268807137731862 test_loss: 0.45937816979018664\n",
      "iteration 2280train_loss: 0.4268807059957456 test_loss: 0.45937882766874166\n",
      "iteration 2281train_loss: 0.42688069822784647 test_loss: 0.4593794851595114\n",
      "iteration 2282train_loss: 0.42688069046947624 test_loss: 0.4593801422627288\n",
      "iteration 2283train_loss: 0.42688068272062263 test_loss: 0.459380798978627\n",
      "iteration 2284train_loss: 0.4268806749812733 test_loss: 0.459381455307439\n",
      "iteration 2285train_loss: 0.42688066725141566 test_loss: 0.45938211124939726\n",
      "iteration 2286train_loss: 0.4268806595310377 test_loss: 0.4593827668047344\n",
      "iteration 2287train_loss: 0.42688065182012686 test_loss: 0.45938342197368265\n",
      "iteration 2288train_loss: 0.4268806441186709 test_loss: 0.45938407675647414\n",
      "iteration 2289train_loss: 0.42688063642665736 test_loss: 0.4593847311533407\n",
      "iteration 2290train_loss: 0.42688062874407434 test_loss: 0.4593853851645142\n",
      "iteration 2291train_loss: 0.42688062107090924 test_loss: 0.4593860387902261\n",
      "iteration 2292train_loss: 0.42688061340715006 test_loss: 0.4593866920307078\n",
      "iteration 2293train_loss: 0.4268806057527844 test_loss: 0.4593873448861905\n",
      "iteration 2294train_loss: 0.4268805981078003 test_loss: 0.45938799735690533\n",
      "iteration 2295train_loss: 0.42688059047218546 test_loss: 0.4593886494430828\n",
      "iteration 2296train_loss: 0.42688058284592767 test_loss: 0.4593893011449538\n",
      "iteration 2297train_loss: 0.426880575229015 test_loss: 0.4593899524627486\n",
      "iteration 2298train_loss: 0.4268805676214351 test_loss: 0.4593906033966976\n",
      "iteration 2299train_loss: 0.4268805600231761 test_loss: 0.45939125394703095\n",
      "iteration 2300train_loss: 0.42688055243422585 test_loss: 0.45939190411397857\n",
      "iteration 2301train_loss: 0.4268805448545722 test_loss: 0.45939255389777006\n",
      "iteration 2302train_loss: 0.4268805372842033 test_loss: 0.459393203298635\n",
      "iteration 2303train_loss: 0.4268805297231069 test_loss: 0.45939385231680274\n",
      "iteration 2304train_loss: 0.42688052217127126 test_loss: 0.45939450095250256\n",
      "iteration 2305train_loss: 0.4268805146286843 test_loss: 0.4593951492059634\n",
      "iteration 2306train_loss: 0.42688050709533415 test_loss: 0.45939579707741424\n",
      "iteration 2307train_loss: 0.42688049957120866 test_loss: 0.45939644456708356\n",
      "iteration 2308train_loss: 0.4268804920562961 test_loss: 0.4593970916751998\n",
      "iteration 2309train_loss: 0.42688048455058447 test_loss: 0.45939773840199155\n",
      "iteration 2310train_loss: 0.4268804770540619 test_loss: 0.4593983847476867\n",
      "iteration 2311train_loss: 0.42688046956671677 test_loss: 0.45939903071251326\n",
      "iteration 2312train_loss: 0.4268804620885369 test_loss: 0.4593996762966991\n",
      "iteration 2313train_loss: 0.4268804546195107 test_loss: 0.4594003215004716\n",
      "iteration 2314train_loss: 0.42688044715962625 test_loss: 0.45940096632405836\n",
      "iteration 2315train_loss: 0.42688043970887185 test_loss: 0.45940161076768654\n",
      "iteration 2316train_loss: 0.42688043226723565 test_loss: 0.45940225483158326\n",
      "iteration 2317train_loss: 0.426880424834706 test_loss: 0.4594028985159753\n",
      "iteration 2318train_loss: 0.426880417411271 test_loss: 0.45940354182108956\n",
      "iteration 2319train_loss: 0.4268804099969191 test_loss: 0.45940418474715256\n",
      "iteration 2320train_loss: 0.4268804025916387 test_loss: 0.45940482729439047\n",
      "iteration 2321train_loss: 0.4268803951954178 test_loss: 0.4594054694630298\n",
      "iteration 2322train_loss: 0.426880387808245 test_loss: 0.4594061112532963\n",
      "iteration 2323train_loss: 0.4268803804301087 test_loss: 0.45940675266541603\n",
      "iteration 2324train_loss: 0.4268803730609972 test_loss: 0.45940739369961453\n",
      "iteration 2325train_loss: 0.4268803657008988 test_loss: 0.45940803435611727\n",
      "iteration 2326train_loss: 0.4268803583498019 test_loss: 0.45940867463514984\n",
      "iteration 2327train_loss: 0.4268803510076953 test_loss: 0.4594093145369371\n",
      "iteration 2328train_loss: 0.4268803436745672 test_loss: 0.4594099540617043\n",
      "iteration 2329train_loss: 0.4268803363504058 test_loss: 0.4594105932096761\n",
      "iteration 2330train_loss: 0.4268803290352001 test_loss: 0.45941123198107725\n",
      "iteration 2331train_loss: 0.4268803217289383 test_loss: 0.4594118703761322\n",
      "iteration 2332train_loss: 0.4268803144316091 test_loss: 0.4594125083950652\n",
      "iteration 2333train_loss: 0.42688030714320085 test_loss: 0.4594131460381004\n",
      "iteration 2334train_loss: 0.42688029986370224 test_loss: 0.45941378330546206\n",
      "iteration 2335train_loss: 0.4268802925931019 test_loss: 0.4594144201973736\n",
      "iteration 2336train_loss: 0.4268802853313883 test_loss: 0.45941505671405886\n",
      "iteration 2337train_loss: 0.4268802780785501 test_loss: 0.45941569285574124\n",
      "iteration 2338train_loss: 0.4268802708345759 test_loss: 0.4594163286226441\n",
      "iteration 2339train_loss: 0.4268802635994545 test_loss: 0.45941696401499055\n",
      "iteration 2340train_loss: 0.4268802563731744 test_loss: 0.45941759903300355\n",
      "iteration 2341train_loss: 0.4268802491557244 test_loss: 0.459418233676906\n",
      "iteration 2342train_loss: 0.4268802419470932 test_loss: 0.45941886794692033\n",
      "iteration 2343train_loss: 0.4268802347472693 test_loss: 0.4594195018432692\n",
      "iteration 2344train_loss: 0.4268802275562417 test_loss: 0.45942013536617476\n",
      "iteration 2345train_loss: 0.4268802203739992 test_loss: 0.4594207685158593\n",
      "iteration 2346train_loss: 0.42688021320053027 test_loss: 0.4594214012925446\n",
      "iteration 2347train_loss: 0.4268802060358239 test_loss: 0.4594220336964527\n",
      "iteration 2348train_loss: 0.426880198879869 test_loss: 0.4594226657278052\n",
      "iteration 2349train_loss: 0.4268801917326542 test_loss: 0.45942329738682325\n",
      "iteration 2350train_loss: 0.4268801845941683 test_loss: 0.4594239286737286\n",
      "iteration 2351train_loss: 0.4268801774644004 test_loss: 0.4594245595887422\n",
      "iteration 2352train_loss: 0.4268801703433391 test_loss: 0.459425190132085\n",
      "iteration 2353train_loss: 0.42688016323097355 test_loss: 0.459425820303978\n",
      "iteration 2354train_loss: 0.42688015612729246 test_loss: 0.45942645010464156\n",
      "iteration 2355train_loss: 0.4268801490322849 test_loss: 0.45942707953429646\n",
      "iteration 2356train_loss: 0.42688014194593976 test_loss: 0.4594277085931628\n",
      "iteration 2357train_loss: 0.42688013486824583 test_loss: 0.45942833728146104\n",
      "iteration 2358train_loss: 0.4268801277991924 test_loss: 0.4594289655994109\n",
      "iteration 2359train_loss: 0.4268801207387682 test_loss: 0.45942959354723245\n",
      "iteration 2360train_loss: 0.42688011368696255 test_loss: 0.45943022112514526\n",
      "iteration 2361train_loss: 0.426880106643764 test_loss: 0.45943084833336884\n",
      "iteration 2362train_loss: 0.4268800996091621 test_loss: 0.45943147517212257\n",
      "iteration 2363train_loss: 0.42688009258314563 test_loss: 0.4594321016416258\n",
      "iteration 2364train_loss: 0.42688008556570367 test_loss: 0.45943272774209737\n",
      "iteration 2365train_loss: 0.4268800785568253 test_loss: 0.4594333534737563\n",
      "iteration 2366train_loss: 0.4268800715564999 test_loss: 0.4594339788368212\n",
      "iteration 2367train_loss: 0.4268800645647161 test_loss: 0.4594346038315107\n",
      "iteration 2368train_loss: 0.42688005758146347 test_loss: 0.45943522845804324\n",
      "iteration 2369train_loss: 0.426880050606731 test_loss: 0.4594358527166371\n",
      "iteration 2370train_loss: 0.4268800436405078 test_loss: 0.4594364766075103\n",
      "iteration 2371train_loss: 0.4268800366827832 test_loss: 0.45943710013088074\n",
      "iteration 2372train_loss: 0.4268800297335464 test_loss: 0.4594377232869663\n",
      "iteration 2373train_loss: 0.4268800227927865 test_loss: 0.4594383460759845\n",
      "iteration 2374train_loss: 0.42688001586049285 test_loss: 0.4594389684981529\n",
      "iteration 2375train_loss: 0.4268800089366546 test_loss: 0.4594395905536886\n",
      "iteration 2376train_loss: 0.4268800020212611 test_loss: 0.459440212242809\n",
      "iteration 2377train_loss: 0.42687999511430175 test_loss: 0.459440833565731\n",
      "iteration 2378train_loss: 0.42687998821576556 test_loss: 0.4594414545226714\n",
      "iteration 2379train_loss: 0.426879981325642 test_loss: 0.4594420751138469\n",
      "iteration 2380train_loss: 0.42687997444392056 test_loss: 0.459442695339474\n",
      "iteration 2381train_loss: 0.42687996757059027 test_loss: 0.4594433151997691\n",
      "iteration 2382train_loss: 0.42687996070564077 test_loss: 0.4594439346949484\n",
      "iteration 2383train_loss: 0.4268799538490614 test_loss: 0.45944455382522786\n",
      "iteration 2384train_loss: 0.4268799470008413 test_loss: 0.45944517259082346\n",
      "iteration 2385train_loss: 0.4268799401609702 test_loss: 0.4594457909919511\n",
      "iteration 2386train_loss: 0.4268799333294374 test_loss: 0.45944640902882605\n",
      "iteration 2387train_loss: 0.4268799265062324 test_loss: 0.45944702670166415\n",
      "iteration 2388train_loss: 0.42687991969134453 test_loss: 0.4594476440106803\n",
      "iteration 2389train_loss: 0.4268799128847634 test_loss: 0.4594482609560899\n",
      "iteration 2390train_loss: 0.42687990608647847 test_loss: 0.45944887753810776\n",
      "iteration 2391train_loss: 0.4268798992964791 test_loss: 0.45944949375694877\n",
      "iteration 2392train_loss: 0.42687989251475505 test_loss: 0.4594501096128276\n",
      "iteration 2393train_loss: 0.42687988574129576 test_loss: 0.45945072510595886\n",
      "iteration 2394train_loss: 0.4268798789760907 test_loss: 0.45945134023655676\n",
      "iteration 2395train_loss: 0.42687987221912943 test_loss: 0.4594519550048357\n",
      "iteration 2396train_loss: 0.42687986547040174 test_loss: 0.45945256941100954\n",
      "iteration 2397train_loss: 0.426879858729897 test_loss: 0.45945318345529235\n",
      "iteration 2398train_loss: 0.4268798519976049 test_loss: 0.45945379713789786\n",
      "iteration 2399train_loss: 0.42687984527351513 test_loss: 0.45945441045903973\n",
      "iteration 2400train_loss: 0.4268798385576174 test_loss: 0.4594550234189312\n",
      "iteration 2401train_loss: 0.4268798318499011 test_loss: 0.45945563601778583\n",
      "iteration 2402train_loss: 0.426879825150356 test_loss: 0.45945624825581666\n",
      "iteration 2403train_loss: 0.426879818458972 test_loss: 0.4594568601332367\n",
      "iteration 2404train_loss: 0.4268798117757386 test_loss: 0.459457471650259\n",
      "iteration 2405train_loss: 0.42687980510064544 test_loss: 0.4594580828070961\n",
      "iteration 2406train_loss: 0.4268797984336826 test_loss: 0.4594586936039605\n",
      "iteration 2407train_loss: 0.4268797917748395 test_loss: 0.4594593040410647\n",
      "iteration 2408train_loss: 0.426879785124106 test_loss: 0.45945991411862114\n",
      "iteration 2409train_loss: 0.42687977848147185 test_loss: 0.45946052383684166\n",
      "iteration 2410train_loss: 0.426879771846927 test_loss: 0.45946113319593834\n",
      "iteration 2411train_loss: 0.4268797652204611 test_loss: 0.45946174219612296\n",
      "iteration 2412train_loss: 0.42687975860206395 test_loss: 0.4594623508376073\n",
      "iteration 2413train_loss: 0.4268797519917255 test_loss: 0.45946295912060275\n",
      "iteration 2414train_loss: 0.4268797453894356 test_loss: 0.4594635670453209\n",
      "iteration 2415train_loss: 0.426879738795184 test_loss: 0.45946417461197275\n",
      "iteration 2416train_loss: 0.42687973220896064 test_loss: 0.4594647818207695\n",
      "iteration 2417train_loss: 0.42687972563075555 test_loss: 0.45946538867192216\n",
      "iteration 2418train_loss: 0.4268797190605583 test_loss: 0.4594659951656413\n",
      "iteration 2419train_loss: 0.42687971249835915 test_loss: 0.45946660130213784\n",
      "iteration 2420train_loss: 0.42687970594414804 test_loss: 0.4594672070816222\n",
      "iteration 2421train_loss: 0.42687969939791465 test_loss: 0.4594678125043045\n",
      "iteration 2422train_loss: 0.4268796928596491 test_loss: 0.4594684175703953\n",
      "iteration 2423train_loss: 0.4268796863293415 test_loss: 0.45946902228010444\n",
      "iteration 2424train_loss: 0.4268796798069816 test_loss: 0.45946962663364194\n",
      "iteration 2425train_loss: 0.4268796732925595 test_loss: 0.4594702306312175\n",
      "iteration 2426train_loss: 0.4268796667860653 test_loss: 0.45947083427304086\n",
      "iteration 2427train_loss: 0.42687966028748897 test_loss: 0.45947143755932135\n",
      "iteration 2428train_loss: 0.42687965379682075 test_loss: 0.4594720404902686\n",
      "iteration 2429train_loss: 0.42687964731405037 test_loss: 0.4594726430660914\n",
      "iteration 2430train_loss: 0.42687964083916813 test_loss: 0.4594732452869992\n",
      "iteration 2431train_loss: 0.4268796343721641 test_loss: 0.4594738471532007\n",
      "iteration 2432train_loss: 0.42687962791302836 test_loss: 0.45947444866490456\n",
      "iteration 2433train_loss: 0.4268796214617511 test_loss: 0.45947504982231985\n",
      "iteration 2434train_loss: 0.42687961501832233 test_loss: 0.45947565062565454\n",
      "iteration 2435train_loss: 0.42687960858273233 test_loss: 0.4594762510751173\n",
      "iteration 2436train_loss: 0.4268796021549712 test_loss: 0.45947685117091613\n",
      "iteration 2437train_loss: 0.42687959573502915 test_loss: 0.45947745091325926\n",
      "iteration 2438train_loss: 0.4268795893228964 test_loss: 0.4594780503023545\n",
      "iteration 2439train_loss: 0.42687958291856304 test_loss: 0.4594786493384097\n",
      "iteration 2440train_loss: 0.42687957652201947 test_loss: 0.4594792480216325\n",
      "iteration 2441train_loss: 0.42687957013325567 test_loss: 0.45947984635223027\n",
      "iteration 2442train_loss: 0.4268795637522622 test_loss: 0.4594804443304104\n",
      "iteration 2443train_loss: 0.4268795573790291 test_loss: 0.45948104195638023\n",
      "iteration 2444train_loss: 0.42687955101354674 test_loss: 0.4594816392303466\n",
      "iteration 2445train_loss: 0.42687954465580535 test_loss: 0.45948223615251665\n",
      "iteration 2446train_loss: 0.4268795383057953 test_loss: 0.4594828327230972\n",
      "iteration 2447train_loss: 0.42687953196350686 test_loss: 0.45948342894229466\n",
      "iteration 2448train_loss: 0.4268795256289304 test_loss: 0.45948402481031575\n",
      "iteration 2449train_loss: 0.4268795193020563 test_loss: 0.4594846203273667\n",
      "iteration 2450train_loss: 0.4268795129828747 test_loss: 0.45948521549365384\n",
      "iteration 2451train_loss: 0.4268795066713762 test_loss: 0.45948581030938324\n",
      "iteration 2452train_loss: 0.4268795003675512 test_loss: 0.45948640477476077\n",
      "iteration 2453train_loss: 0.42687949407139 test_loss: 0.4594869988899924\n",
      "iteration 2454train_loss: 0.426879487782883 test_loss: 0.4594875926552837\n",
      "iteration 2455train_loss: 0.4268794815020208 test_loss: 0.4594881860708403\n",
      "iteration 2456train_loss: 0.4268794752287935 test_loss: 0.4594887791368675\n",
      "iteration 2457train_loss: 0.4268794689631918 test_loss: 0.4594893718535705\n",
      "iteration 2458train_loss: 0.4268794627052063 test_loss: 0.4594899642211547\n",
      "iteration 2459train_loss: 0.42687945645482717 test_loss: 0.4594905562398247\n",
      "iteration 2460train_loss: 0.42687945021204493 test_loss: 0.4594911479097856\n",
      "iteration 2461train_loss: 0.42687944397685035 test_loss: 0.45949173923124215\n",
      "iteration 2462train_loss: 0.42687943774923376 test_loss: 0.4594923302043988\n",
      "iteration 2463train_loss: 0.42687943152918567 test_loss: 0.4594929208294601\n",
      "iteration 2464train_loss: 0.4268794253166968 test_loss: 0.4594935111066302\n",
      "iteration 2465train_loss: 0.42687941911175753 test_loss: 0.45949410103611343\n",
      "iteration 2466train_loss: 0.42687941291435844 test_loss: 0.45949469061811365\n",
      "iteration 2467train_loss: 0.42687940672449026 test_loss: 0.459495279852835\n",
      "iteration 2468train_loss: 0.42687940054214346 test_loss: 0.45949586874048093\n",
      "iteration 2469train_loss: 0.42687939436730865 test_loss: 0.45949645728125527\n",
      "iteration 2470train_loss: 0.42687938819997645 test_loss: 0.45949704547536147\n",
      "iteration 2471train_loss: 0.4268793820401376 test_loss: 0.45949763332300286\n",
      "iteration 2472train_loss: 0.4268793758877828 test_loss: 0.4594982208243827\n",
      "iteration 2473train_loss: 0.42687936974290247 test_loss: 0.459498807979704\n",
      "iteration 2474train_loss: 0.4268793636054875 test_loss: 0.4594993947891698\n",
      "iteration 2475train_loss: 0.4268793574755284 test_loss: 0.4594999812529828\n",
      "iteration 2476train_loss: 0.42687935135301613 test_loss: 0.4595005673713458\n",
      "iteration 2477train_loss: 0.4268793452379412 test_loss: 0.4595011531444613\n",
      "iteration 2478train_loss: 0.4268793391302942 test_loss: 0.4595017385725316\n",
      "iteration 2479train_loss: 0.4268793330300663 test_loss: 0.45950232365575927\n",
      "iteration 2480train_loss: 0.4268793269372479 test_loss: 0.45950290839434604\n",
      "iteration 2481train_loss: 0.4268793208518299 test_loss: 0.4595034927884944\n",
      "iteration 2482train_loss: 0.4268793147738031 test_loss: 0.4595040768384059\n",
      "iteration 2483train_loss: 0.42687930870315827 test_loss: 0.45950466054428224\n",
      "iteration 2484train_loss: 0.4268793026398861 test_loss: 0.45950524390632524\n",
      "iteration 2485train_loss: 0.4268792965839775 test_loss: 0.45950582692473635\n",
      "iteration 2486train_loss: 0.4268792905354233 test_loss: 0.4595064095997168\n",
      "iteration 2487train_loss: 0.4268792844942144 test_loss: 0.4595069919314678\n",
      "iteration 2488train_loss: 0.4268792784603416 test_loss: 0.45950757392019076\n",
      "iteration 2489train_loss: 0.42687927243379586 test_loss: 0.4595081555660861\n",
      "iteration 2490train_loss: 0.42687926641456775 test_loss: 0.4595087368693551\n",
      "iteration 2491train_loss: 0.4268792604026485 test_loss: 0.4595093178301982\n",
      "iteration 2492train_loss: 0.4268792543980289 test_loss: 0.45950989844881607\n",
      "iteration 2493train_loss: 0.42687924840069985 test_loss: 0.4595104787254091\n",
      "iteration 2494train_loss: 0.4268792424106523 test_loss: 0.45951105866017755\n",
      "iteration 2495train_loss: 0.42687923642787706 test_loss: 0.4595116382533217\n",
      "iteration 2496train_loss: 0.4268792304523654 test_loss: 0.45951221750504145\n",
      "iteration 2497train_loss: 0.426879224484108 test_loss: 0.4595127964155367\n",
      "iteration 2498train_loss: 0.42687921852309607 test_loss: 0.45951337498500733\n",
      "iteration 2499train_loss: 0.42687921256932027 test_loss: 0.45951395321365307\n",
      "iteration 2500train_loss: 0.426879206622772 test_loss: 0.4595145311016732\n",
      "iteration 2501train_loss: 0.4268792006834419 test_loss: 0.4595151086492672\n",
      "iteration 2502train_loss: 0.42687919475132124 test_loss: 0.45951568585663427\n",
      "iteration 2503train_loss: 0.42687918882640097 test_loss: 0.4595162627239738\n",
      "iteration 2504train_loss: 0.4268791829086722 test_loss: 0.4595168392514844\n",
      "iteration 2505train_loss: 0.426879176998126 test_loss: 0.4595174154393652\n",
      "iteration 2506train_loss: 0.4268791710947534 test_loss: 0.45951799128781484\n",
      "iteration 2507train_loss: 0.42687916519854546 test_loss: 0.45951856679703196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2508train_loss: 0.4268791593094934 test_loss: 0.4595191419672151\n",
      "iteration 2509train_loss: 0.42687915342758825 test_loss: 0.45951971679856246\n",
      "iteration 2510train_loss: 0.4268791475528211 test_loss: 0.4595202912912725\n",
      "iteration 2511train_loss: 0.4268791416851831 test_loss: 0.45952086544554294\n",
      "iteration 2512train_loss: 0.4268791358246655 test_loss: 0.4595214392615722\n",
      "iteration 2513train_loss: 0.42687912997125926 test_loss: 0.45952201273955784\n",
      "iteration 2514train_loss: 0.4268791241249558 test_loss: 0.4595225858796975\n",
      "iteration 2515train_loss: 0.4268791182857464 test_loss: 0.459523158682189\n",
      "iteration 2516train_loss: 0.4268791124536217 test_loss: 0.4595237311472298\n",
      "iteration 2517train_loss: 0.42687910662857337 test_loss: 0.45952430327501703\n",
      "iteration 2518train_loss: 0.4268791008105926 test_loss: 0.459524875065748\n",
      "iteration 2519train_loss: 0.42687909499967047 test_loss: 0.4595254465196199\n",
      "iteration 2520train_loss: 0.4268790891957983 test_loss: 0.45952601763682954\n",
      "iteration 2521train_loss: 0.42687908339896735 test_loss: 0.4595265884175737\n",
      "iteration 2522train_loss: 0.426879077609169 test_loss: 0.4595271588620492\n",
      "iteration 2523train_loss: 0.4268790718263944 test_loss: 0.45952772897045263\n",
      "iteration 2524train_loss: 0.4268790660506347 test_loss: 0.45952829874298046\n",
      "iteration 2525train_loss: 0.42687906028188144 test_loss: 0.45952886817982874\n",
      "iteration 2526train_loss: 0.4268790545201259 test_loss: 0.459529437281194\n",
      "iteration 2527train_loss: 0.4268790487653594 test_loss: 0.45953000604727207\n",
      "iteration 2528train_loss: 0.42687904301757323 test_loss: 0.4595305744782591\n",
      "iteration 2529train_loss: 0.42687903727675874 test_loss: 0.45953114257435074\n",
      "iteration 2530train_loss: 0.42687903154290724 test_loss: 0.45953171033574286\n",
      "iteration 2531train_loss: 0.42687902581601017 test_loss: 0.45953227776263084\n",
      "iteration 2532train_loss: 0.42687902009605905 test_loss: 0.45953284485521034\n",
      "iteration 2533train_loss: 0.42687901438304504 test_loss: 0.45953341161367645\n",
      "iteration 2534train_loss: 0.4268790086769597 test_loss: 0.4595339780382245\n",
      "iteration 2535train_loss: 0.42687900297779435 test_loss: 0.45953454412904954\n",
      "iteration 2536train_loss: 0.4268789972855406 test_loss: 0.45953510988634655\n",
      "iteration 2537train_loss: 0.4268789916001897 test_loss: 0.4595356753103103\n",
      "iteration 2538train_loss: 0.4268789859217332 test_loss: 0.45953624040113544\n",
      "iteration 2539train_loss: 0.4268789802501626 test_loss: 0.45953680515901674\n",
      "iteration 2540train_loss: 0.42687897458546914 test_loss: 0.4595373695841485\n",
      "iteration 2541train_loss: 0.42687896892764465 test_loss: 0.45953793367672513\n",
      "iteration 2542train_loss: 0.42687896327668035 test_loss: 0.45953849743694075\n",
      "iteration 2543train_loss: 0.42687895763256795 test_loss: 0.45953906086498947\n",
      "iteration 2544train_loss: 0.42687895199529896 test_loss: 0.4595396239610654\n",
      "iteration 2545train_loss: 0.42687894636486473 test_loss: 0.4595401867253622\n",
      "iteration 2546train_loss: 0.426878940741257 test_loss: 0.4595407491580737\n",
      "iteration 2547train_loss: 0.42687893512446723 test_loss: 0.4595413112593934\n",
      "iteration 2548train_loss: 0.426878929514487 test_loss: 0.45954187302951477\n",
      "iteration 2549train_loss: 0.4268789239113079 test_loss: 0.45954243446863124\n",
      "iteration 2550train_loss: 0.4268789183149215 test_loss: 0.45954299557693606\n",
      "iteration 2551train_loss: 0.42687891272531947 test_loss: 0.4595435563546222\n",
      "iteration 2552train_loss: 0.4268789071424934 test_loss: 0.45954411680188273\n",
      "iteration 2553train_loss: 0.4268789015664347 test_loss: 0.4595446769189106\n",
      "iteration 2554train_loss: 0.4268788959971355 test_loss: 0.4595452367058985\n",
      "iteration 2555train_loss: 0.42687889043458693 test_loss: 0.45954579616303887\n",
      "iteration 2556train_loss: 0.42687888487878095 test_loss: 0.45954635529052446\n",
      "iteration 2557train_loss: 0.4268788793297091 test_loss: 0.45954691408854753\n",
      "iteration 2558train_loss: 0.42687887378736317 test_loss: 0.45954747255730033\n",
      "iteration 2559train_loss: 0.4268788682517347 test_loss: 0.45954803069697503\n",
      "iteration 2560train_loss: 0.4268788627228155 test_loss: 0.4595485885077636\n",
      "iteration 2561train_loss: 0.4268788572005972 test_loss: 0.4595491459898581\n",
      "iteration 2562train_loss: 0.4268788516850716 test_loss: 0.4595497031434501\n",
      "iteration 2563train_loss: 0.42687884617623045 test_loss: 0.4595502599687314\n",
      "iteration 2564train_loss: 0.42687884067406556 test_loss: 0.45955081646589335\n",
      "iteration 2565train_loss: 0.42687883517856845 test_loss: 0.4595513726351276\n",
      "iteration 2566train_loss: 0.42687882968973107 test_loss: 0.45955192847662535\n",
      "iteration 2567train_loss: 0.4268788242075451 test_loss: 0.45955248399057774\n",
      "iteration 2568train_loss: 0.4268788187320024 test_loss: 0.4595530391771758\n",
      "iteration 2569train_loss: 0.4268788132630948 test_loss: 0.45955359403661067\n",
      "iteration 2570train_loss: 0.42687880780081405 test_loss: 0.45955414856907295\n",
      "iteration 2571train_loss: 0.42687880234515196 test_loss: 0.45955470277475347\n",
      "iteration 2572train_loss: 0.4268787968961005 test_loss: 0.45955525665384267\n",
      "iteration 2573train_loss: 0.42687879145365115 test_loss: 0.45955581020653113\n",
      "iteration 2574train_loss: 0.4268787860177962 test_loss: 0.4595563634330092\n",
      "iteration 2575train_loss: 0.42687878058852735 test_loss: 0.459556916333467\n",
      "iteration 2576train_loss: 0.42687877516583644 test_loss: 0.45955746890809485\n",
      "iteration 2577train_loss: 0.4268787697497154 test_loss: 0.4595580211570826\n",
      "iteration 2578train_loss: 0.426878764340156 test_loss: 0.45955857308062004\n",
      "iteration 2579train_loss: 0.4268787589371503 test_loss: 0.45955912467889704\n",
      "iteration 2580train_loss: 0.42687875354069016 test_loss: 0.4595596759521032\n",
      "iteration 2581train_loss: 0.42687874815076754 test_loss: 0.459560226900428\n",
      "iteration 2582train_loss: 0.4268787427673744 test_loss: 0.459560777524061\n",
      "iteration 2583train_loss: 0.4268787373905026 test_loss: 0.4595613278231913\n",
      "iteration 2584train_loss: 0.42687873202014415 test_loss: 0.45956187779800806\n",
      "iteration 2585train_loss: 0.42687872665629106 test_loss: 0.4595624274487005\n",
      "iteration 2586train_loss: 0.42687872129893517 test_loss: 0.4595629767754575\n",
      "iteration 2587train_loss: 0.42687871594806864 test_loss: 0.45956352577846776\n",
      "iteration 2588train_loss: 0.42687871060368343 test_loss: 0.45956407445791997\n",
      "iteration 2589train_loss: 0.4268787052657715 test_loss: 0.45956462281400284\n",
      "iteration 2590train_loss: 0.426878699934325 test_loss: 0.45956517084690496\n",
      "iteration 2591train_loss: 0.42687869460933575 test_loss: 0.4595657185568143\n",
      "iteration 2592train_loss: 0.426878689290796 test_loss: 0.45956626594391936\n",
      "iteration 2593train_loss: 0.4268786839786977 test_loss: 0.4595668130084083\n",
      "iteration 2594train_loss: 0.4268786786730329 test_loss: 0.4595673597504688\n",
      "iteration 2595train_loss: 0.4268786733737937 test_loss: 0.45956790617028925\n",
      "iteration 2596train_loss: 0.4268786680809723 test_loss: 0.4595684522680568\n",
      "iteration 2597train_loss: 0.42687866279456066 test_loss: 0.4595689980439597\n",
      "iteration 2598train_loss: 0.42687865751455095 test_loss: 0.45956954349818513\n",
      "iteration 2599train_loss: 0.42687865224093513 test_loss: 0.4595700886309206\n",
      "iteration 2600train_loss: 0.4268786469737056 test_loss: 0.4595706334423533\n",
      "iteration 2601train_loss: 0.4268786417128544 test_loss: 0.45957117793267066\n",
      "iteration 2602train_loss: 0.4268786364583735 test_loss: 0.45957172210205977\n",
      "iteration 2603train_loss: 0.4268786312102552 test_loss: 0.45957226595070727\n",
      "iteration 2604train_loss: 0.4268786259684918 test_loss: 0.4595728094788003\n",
      "iteration 2605train_loss: 0.42687862073307514 test_loss: 0.45957335268652544\n",
      "iteration 2606train_loss: 0.42687861550399786 test_loss: 0.45957389557406947\n",
      "iteration 2607train_loss: 0.4268786102812518 test_loss: 0.4595744381416187\n",
      "iteration 2608train_loss: 0.4268786050648293 test_loss: 0.45957498038935957\n",
      "iteration 2609train_loss: 0.4268785998547226 test_loss: 0.45957552231747845\n",
      "iteration 2610train_loss: 0.42687859465092376 test_loss: 0.45957606392616157\n",
      "iteration 2611train_loss: 0.42687858945342533 test_loss: 0.4595766052155946\n",
      "iteration 2612train_loss: 0.42687858426221936 test_loss: 0.4595771461859639\n",
      "iteration 2613train_loss: 0.426878579077298 test_loss: 0.45957768683745515\n",
      "iteration 2614train_loss: 0.4268785738986538 test_loss: 0.45957822717025393\n",
      "iteration 2615train_loss: 0.42687856872627883 test_loss: 0.45957876718454604\n",
      "iteration 2616train_loss: 0.42687856356016546 test_loss: 0.4595793068805167\n",
      "iteration 2617train_loss: 0.4268785584003059 test_loss: 0.4595798462583515\n",
      "iteration 2618train_loss: 0.4268785532466927 test_loss: 0.45958038531823564\n",
      "iteration 2619train_loss: 0.4268785480993179 test_loss: 0.4595809240603542\n",
      "iteration 2620train_loss: 0.42687854295817407 test_loss: 0.45958146248489234\n",
      "iteration 2621train_loss: 0.42687853782325347 test_loss: 0.45958200059203486\n",
      "iteration 2622train_loss: 0.42687853269454823 test_loss: 0.45958253838196655\n",
      "iteration 2623train_loss: 0.42687852757205097 test_loss: 0.4595830758548722\n",
      "iteration 2624train_loss: 0.426878522455754 test_loss: 0.4595836130109363\n",
      "iteration 2625train_loss: 0.42687851734564974 test_loss: 0.4595841498503434\n",
      "iteration 2626train_loss: 0.42687851224173046 test_loss: 0.45958468637327776\n",
      "iteration 2627train_loss: 0.4268785071439887 test_loss: 0.45958522257992357\n",
      "iteration 2628train_loss: 0.4268785020524166 test_loss: 0.4595857584704652\n",
      "iteration 2629train_loss: 0.426878496967007 test_loss: 0.4595862940450864\n",
      "iteration 2630train_loss: 0.426878491887752 test_loss: 0.4595868293039713\n",
      "iteration 2631train_loss: 0.4268784868146442 test_loss: 0.45958736424730356\n",
      "iteration 2632train_loss: 0.42687848174767595 test_loss: 0.4595878988752669\n",
      "iteration 2633train_loss: 0.42687847668683976 test_loss: 0.4595884331880449\n",
      "iteration 2634train_loss: 0.42687847163212805 test_loss: 0.459588967185821\n",
      "iteration 2635train_loss: 0.42687846658353346 test_loss: 0.4595895008687786\n",
      "iteration 2636train_loss: 0.4268784615410483 test_loss: 0.4595900342371007\n",
      "iteration 2637train_loss: 0.4268784565046651 test_loss: 0.45959056729097086\n",
      "iteration 2638train_loss: 0.4268784514743765 test_loss: 0.45959110003057185\n",
      "iteration 2639train_loss: 0.4268784464501748 test_loss: 0.4595916324560866\n",
      "iteration 2640train_loss: 0.42687844143205267 test_loss: 0.45959216456769775\n",
      "iteration 2641train_loss: 0.4268784364200026 test_loss: 0.4595926963655882\n",
      "iteration 2642train_loss: 0.42687843141401716 test_loss: 0.4595932278499405\n",
      "iteration 2643train_loss: 0.426878426414089 test_loss: 0.45959375902093713\n",
      "iteration 2644train_loss: 0.42687842142021043 test_loss: 0.4595942898787602\n",
      "iteration 2645train_loss: 0.42687841643237423 test_loss: 0.4595948204235924\n",
      "iteration 2646train_loss: 0.42687841145057304 test_loss: 0.45959535065561546\n",
      "iteration 2647train_loss: 0.42687840647479924 test_loss: 0.4595958805750116\n",
      "iteration 2648train_loss: 0.4268784015050456 test_loss: 0.45959641018196257\n",
      "iteration 2649train_loss: 0.4268783965413046 test_loss: 0.45959693947665053\n",
      "iteration 2650train_loss: 0.42687839158356894 test_loss: 0.4595974684592568\n",
      "iteration 2651train_loss: 0.42687838663183136 test_loss: 0.4595979971299631\n",
      "iteration 2652train_loss: 0.4268783816860844 test_loss: 0.45959852548895097\n",
      "iteration 2653train_loss: 0.42687837674632056 test_loss: 0.4595990535364016\n",
      "iteration 2654train_loss: 0.4268783718125328 test_loss: 0.45959958127249656\n",
      "iteration 2655train_loss: 0.4268783668847136 test_loss: 0.4596001086974168\n",
      "iteration 2656train_loss: 0.4268783619628556 test_loss: 0.45960063581134325\n",
      "iteration 2657train_loss: 0.42687835704695165 test_loss: 0.4596011626144571\n",
      "iteration 2658train_loss: 0.4268783521369943 test_loss: 0.45960168910693905\n",
      "iteration 2659train_loss: 0.42687834723297635 test_loss: 0.4596022152889697\n",
      "iteration 2660train_loss: 0.4268783423348905 test_loss: 0.4596027411607299\n",
      "iteration 2661train_loss: 0.4268783374427293 test_loss: 0.4596032667223999\n",
      "iteration 2662train_loss: 0.4268783325564859 test_loss: 0.45960379197416035\n",
      "iteration 2663train_loss: 0.42687832767615264 test_loss: 0.4596043169161913\n",
      "iteration 2664train_loss: 0.42687832280172266 test_loss: 0.4596048415486731\n",
      "iteration 2665train_loss: 0.4268783179331882 test_loss: 0.45960536587178574\n",
      "iteration 2666train_loss: 0.42687831307054247 test_loss: 0.4596058898857091\n",
      "iteration 2667train_loss: 0.42687830821377815 test_loss: 0.45960641359062315\n",
      "iteration 2668train_loss: 0.4268783033628879 test_loss: 0.45960693698670757\n",
      "iteration 2669train_loss: 0.42687829851786463 test_loss: 0.4596074600741421\n",
      "iteration 2670train_loss: 0.42687829367870117 test_loss: 0.4596079828531061\n",
      "iteration 2671train_loss: 0.4268782888453903 test_loss: 0.45960850532377917\n",
      "iteration 2672train_loss: 0.4268782840179248 test_loss: 0.45960902748634047\n",
      "iteration 2673train_loss: 0.4268782791962977 test_loss: 0.4596095493409694\n",
      "iteration 2674train_loss: 0.4268782743805015 test_loss: 0.4596100708878448\n",
      "iteration 2675train_loss: 0.42687826957052943 test_loss: 0.4596105921271459\n",
      "iteration 2676train_loss: 0.4268782647663741 test_loss: 0.4596111130590514\n",
      "iteration 2677train_loss: 0.42687825996802853 test_loss: 0.45961163368374025\n",
      "iteration 2678train_loss: 0.4268782551754855 test_loss: 0.4596121540013909\n",
      "iteration 2679train_loss: 0.42687825038873806 test_loss: 0.4596126740121822\n",
      "iteration 2680train_loss: 0.4268782456077789 test_loss: 0.45961319371629245\n",
      "iteration 2681train_loss: 0.42687824083260095 test_loss: 0.4596137131139001\n",
      "iteration 2682train_loss: 0.4268782360631974 test_loss: 0.4596142322051832\n",
      "iteration 2683train_loss: 0.4268782312995609 test_loss: 0.45961475099032006\n",
      "iteration 2684train_loss: 0.42687822654168445 test_loss: 0.45961526946948866\n",
      "iteration 2685train_loss: 0.4268782217895611 test_loss: 0.459615787642867\n",
      "iteration 2686train_loss: 0.4268782170431836 test_loss: 0.45961630551063276\n",
      "iteration 2687train_loss: 0.42687821230254525 test_loss: 0.4596168230729638\n",
      "iteration 2688train_loss: 0.4268782075676386 test_loss: 0.4596173403300377\n",
      "iteration 2689train_loss: 0.42687820283845695 test_loss: 0.4596178572820319\n",
      "iteration 2690train_loss: 0.4268781981149932 test_loss: 0.4596183739291239\n",
      "iteration 2691train_loss: 0.42687819339724037 test_loss: 0.459618890271491\n",
      "iteration 2692train_loss: 0.42687818868519145 test_loss: 0.45961940630931025\n",
      "iteration 2693train_loss: 0.42687818397883953 test_loss: 0.4596199220427588\n",
      "iteration 2694train_loss: 0.4268781792781774 test_loss: 0.45962043747201364\n",
      "iteration 2695train_loss: 0.42687817458319827 test_loss: 0.4596209525972518\n",
      "iteration 2696train_loss: 0.42687816989389527 test_loss: 0.45962146741864984\n",
      "iteration 2697train_loss: 0.42687816521026134 test_loss: 0.45962198193638454\n",
      "iteration 2698train_loss: 0.42687816053228944 test_loss: 0.4596224961506324\n",
      "iteration 2699train_loss: 0.4268781558599729 test_loss: 0.4596230100615699\n",
      "iteration 2700train_loss: 0.42687815119330463 test_loss: 0.45962352366937337\n",
      "iteration 2701train_loss: 0.42687814653227774 test_loss: 0.45962403697421916\n",
      "iteration 2702train_loss: 0.42687814187688544 test_loss: 0.4596245499762833\n",
      "iteration 2703train_loss: 0.42687813722712065 test_loss: 0.45962506267574194\n",
      "iteration 2704train_loss: 0.4268781325829766 test_loss: 0.4596255750727708\n",
      "iteration 2705train_loss: 0.4268781279444463 test_loss: 0.45962608716754594\n",
      "iteration 2706train_loss: 0.4268781233115231 test_loss: 0.45962659896024305\n",
      "iteration 2707train_loss: 0.4268781186841999 test_loss: 0.4596271104510377\n",
      "iteration 2708train_loss: 0.42687811406247 test_loss: 0.4596276216401054\n",
      "iteration 2709train_loss: 0.4268781094463264 test_loss: 0.45962813252762164\n",
      "iteration 2710train_loss: 0.42687810483576266 test_loss: 0.4596286431137616\n",
      "iteration 2711train_loss: 0.42687810023077155 test_loss: 0.45962915339870064\n",
      "iteration 2712train_loss: 0.42687809563134643 test_loss: 0.4596296633826138\n",
      "iteration 2713train_loss: 0.4268780910374804 test_loss: 0.45963017306567605\n",
      "iteration 2714train_loss: 0.42687808644916675 test_loss: 0.45963068244806243\n",
      "iteration 2715train_loss: 0.4268780818663987 test_loss: 0.4596311915299475\n",
      "iteration 2716train_loss: 0.42687807728916943 test_loss: 0.4596317003115062\n",
      "iteration 2717train_loss: 0.42687807271747213 test_loss: 0.4596322087929129\n",
      "iteration 2718train_loss: 0.4268780681513001 test_loss: 0.4596327169743424\n",
      "iteration 2719train_loss: 0.4268780635906467 test_loss: 0.4596332248559687\n",
      "iteration 2720train_loss: 0.42687805903550485 test_loss: 0.4596337324379663\n",
      "iteration 2721train_loss: 0.426878054485868 test_loss: 0.4596342397205094\n",
      "iteration 2722train_loss: 0.42687804994172956 test_loss: 0.45963474670377197\n",
      "iteration 2723train_loss: 0.42687804540308266 test_loss: 0.459635253387928\n",
      "iteration 2724train_loss: 0.42687804086992065 test_loss: 0.4596357597731515\n",
      "iteration 2725train_loss: 0.42687803634223664 test_loss: 0.45963626585961603\n",
      "iteration 2726train_loss: 0.42687803182002426 test_loss: 0.4596367716474955\n",
      "iteration 2727train_loss: 0.4268780273032765 test_loss: 0.45963727713696334\n",
      "iteration 2728train_loss: 0.42687802279198694 test_loss: 0.45963778232819297\n",
      "iteration 2729train_loss: 0.42687801828614885 test_loss: 0.45963828722135786\n",
      "iteration 2730train_loss: 0.42687801378575535 test_loss: 0.4596387918166312\n",
      "iteration 2731train_loss: 0.42687800929080016 test_loss: 0.4596392961141861\n",
      "iteration 2732train_loss: 0.4268780048012763 test_loss: 0.45963980011419575\n",
      "iteration 2733train_loss: 0.42687800031717726 test_loss: 0.4596403038168331\n",
      "iteration 2734train_loss: 0.4268779958384965 test_loss: 0.4596408072222709\n",
      "iteration 2735train_loss: 0.4268779913652273 test_loss: 0.4596413103306819\n",
      "iteration 2736train_loss: 0.4268779868973631 test_loss: 0.4596418131422389\n",
      "iteration 2737train_loss: 0.42687798243489716 test_loss: 0.4596423156571142\n",
      "iteration 2738train_loss: 0.4268779779778231 test_loss: 0.45964281787548056\n",
      "iteration 2739train_loss: 0.42687797352613427 test_loss: 0.4596433197975101\n",
      "iteration 2740train_loss: 0.42687796907982406 test_loss: 0.45964382142337523\n",
      "iteration 2741train_loss: 0.4268779646388858 test_loss: 0.4596443227532479\n",
      "iteration 2742train_loss: 0.4268779602033131 test_loss: 0.4596448237873003\n",
      "iteration 2743train_loss: 0.42687795577309934 test_loss: 0.4596453245257044\n",
      "iteration 2744train_loss: 0.42687795134823797 test_loss: 0.45964582496863193\n",
      "iteration 2745train_loss: 0.4268779469287225 test_loss: 0.45964632511625475\n",
      "iteration 2746train_loss: 0.42687794251454625 test_loss: 0.45964682496874437\n",
      "iteration 2747train_loss: 0.42687793810570285 test_loss: 0.45964732452627244\n",
      "iteration 2748train_loss: 0.42687793370218574 test_loss: 0.45964782378901037\n",
      "iteration 2749train_loss: 0.42687792930398843 test_loss: 0.45964832275712947\n",
      "iteration 2750train_loss: 0.42687792491110454 test_loss: 0.45964882143080105\n",
      "iteration 2751train_loss: 0.4268779205235274 test_loss: 0.4596493198101962\n",
      "iteration 2752train_loss: 0.42687791614125054 test_loss: 0.459649817895486\n",
      "iteration 2753train_loss: 0.4268779117642676 test_loss: 0.45965031568684145\n",
      "iteration 2754train_loss: 0.4268779073925721 test_loss: 0.4596508131844333\n",
      "iteration 2755train_loss: 0.42687790302615747 test_loss: 0.45965131038843227\n",
      "iteration 2756train_loss: 0.4268778986650175 test_loss: 0.459651807299009\n",
      "iteration 2757train_loss: 0.42687789430914547 test_loss: 0.4596523039163341\n",
      "iteration 2758train_loss: 0.42687788995853515 test_loss: 0.4596528002405781\n",
      "iteration 2759train_loss: 0.42687788561318 test_loss: 0.45965329627191115\n",
      "iteration 2760train_loss: 0.42687788127307374 test_loss: 0.45965379201050366\n",
      "iteration 2761train_loss: 0.4268778769382098 test_loss: 0.45965428745652565\n",
      "iteration 2762train_loss: 0.42687787260858195 test_loss: 0.45965478261014725\n",
      "iteration 2763train_loss: 0.42687786828418367 test_loss: 0.45965527747153845\n",
      "iteration 2764train_loss: 0.4268778639650086 test_loss: 0.4596557720408689\n",
      "iteration 2765train_loss: 0.4268778596510504 test_loss: 0.45965626631830864\n",
      "iteration 2766train_loss: 0.4268778553423028 test_loss: 0.45965676030402697\n",
      "iteration 2767train_loss: 0.4268778510387593 test_loss: 0.45965725399819385\n",
      "iteration 2768train_loss: 0.4268778467404136 test_loss: 0.4596577474009784\n",
      "iteration 2769train_loss: 0.42687784244725935 test_loss: 0.45965824051255016\n",
      "iteration 2770train_loss: 0.4268778381592902 test_loss: 0.4596587333330784\n",
      "iteration 2771train_loss: 0.4268778338764998 test_loss: 0.45965922586273206\n",
      "iteration 2772train_loss: 0.426877829598882 test_loss: 0.4596597181016804\n",
      "iteration 2773train_loss: 0.42687782532643026 test_loss: 0.4596602100500924\n",
      "iteration 2774train_loss: 0.4268778210591385 test_loss: 0.4596607017081368\n",
      "iteration 2775train_loss: 0.42687781679700026 test_loss: 0.4596611930759824\n",
      "iteration 2776train_loss: 0.4268778125400094 test_loss: 0.45966168415379793\n",
      "iteration 2777train_loss: 0.42687780828815947 test_loss: 0.45966217494175193\n",
      "iteration 2778train_loss: 0.4268778040414443 test_loss: 0.45966266544001294\n",
      "iteration 2779train_loss: 0.4268777997998575 test_loss: 0.45966315564874916\n",
      "iteration 2780train_loss: 0.42687779556339317 test_loss: 0.45966364556812894\n",
      "iteration 2781train_loss: 0.42687779133204473 test_loss: 0.4596641351983206\n",
      "iteration 2782train_loss: 0.42687778710580604 test_loss: 0.4596646245394921\n",
      "iteration 2783train_loss: 0.42687778288467076 test_loss: 0.4596651135918114\n",
      "iteration 2784train_loss: 0.4268777786686329 test_loss: 0.4596656023554464\n",
      "iteration 2785train_loss: 0.42687777445768615 test_loss: 0.4596660908305649\n",
      "iteration 2786train_loss: 0.4268777702518242 test_loss: 0.45966657901733465\n",
      "iteration 2787train_loss: 0.42687776605104094 test_loss: 0.45966706691592324\n",
      "iteration 2788train_loss: 0.42687776185533016 test_loss: 0.45966755452649805\n",
      "iteration 2789train_loss: 0.4268777576646857 test_loss: 0.45966804184922655\n",
      "iteration 2790train_loss: 0.4268777534791014 test_loss: 0.45966852888427606\n",
      "iteration 2791train_loss: 0.426877749298571 test_loss: 0.4596690156318138\n",
      "iteration 2792train_loss: 0.42687774512308846 test_loss: 0.45966950209200685\n",
      "iteration 2793train_loss: 0.4268777409526476 test_loss: 0.45966998826502226\n",
      "iteration 2794train_loss: 0.4268777367872421 test_loss: 0.4596704741510269\n",
      "iteration 2795train_loss: 0.4268777326268661 test_loss: 0.4596709597501875\n",
      "iteration 2796train_loss: 0.4268777284715133 test_loss: 0.459671445062671\n",
      "iteration 2797train_loss: 0.42687772432117754 test_loss: 0.45967193008864393\n",
      "iteration 2798train_loss: 0.4268777201758529 test_loss: 0.45967241482827276\n",
      "iteration 2799train_loss: 0.4268777160355331 test_loss: 0.459672899281724\n",
      "iteration 2800train_loss: 0.42687771190021206 test_loss: 0.4596733834491639\n",
      "iteration 2801train_loss: 0.42687770776988376 test_loss: 0.45967386733075893\n",
      "iteration 2802train_loss: 0.4268777036445421 test_loss: 0.4596743509266749\n",
      "iteration 2803train_loss: 0.42687769952418086 test_loss: 0.45967483423707806\n",
      "iteration 2804train_loss: 0.42687769540879417 test_loss: 0.45967531726213434\n",
      "iteration 2805train_loss: 0.42687769129837605 test_loss: 0.45967580000200964\n",
      "iteration 2806train_loss: 0.42687768719292013 test_loss: 0.45967628245686953\n",
      "iteration 2807train_loss: 0.4268776830924206 test_loss: 0.4596767646268799\n",
      "iteration 2808train_loss: 0.4268776789968714 test_loss: 0.4596772465122063\n",
      "iteration 2809train_loss: 0.4268776749062663 test_loss: 0.4596777281130141\n",
      "iteration 2810train_loss: 0.4268776708205995 test_loss: 0.45967820942946874\n",
      "iteration 2811train_loss: 0.426877666739865 test_loss: 0.45967869046173543\n",
      "iteration 2812train_loss: 0.4268776626640565 test_loss: 0.4596791712099795\n",
      "iteration 2813train_loss: 0.42687765859316834 test_loss: 0.4596796516743659\n",
      "iteration 2814train_loss: 0.42687765452719445 test_loss: 0.4596801318550596\n",
      "iteration 2815train_loss: 0.42687765046612874 test_loss: 0.45968061175222574\n",
      "iteration 2816train_loss: 0.4268776464099653 test_loss: 0.45968109136602897\n",
      "iteration 2817train_loss: 0.42687764235869796 test_loss: 0.45968157069663396\n",
      "iteration 2818train_loss: 0.4268776383123212 test_loss: 0.45968204974420535\n",
      "iteration 2819train_loss: 0.42687763427082864 test_loss: 0.45968252850890773\n",
      "iteration 2820train_loss: 0.42687763023421443 test_loss: 0.4596830069909055\n",
      "iteration 2821train_loss: 0.4268776262024729 test_loss: 0.45968348519036295\n",
      "iteration 2822train_loss: 0.42687762217559777 test_loss: 0.4596839631074443\n",
      "iteration 2823train_loss: 0.4268776181535834 test_loss: 0.4596844407423139\n",
      "iteration 2824train_loss: 0.42687761413642356 test_loss: 0.45968491809513556\n",
      "iteration 2825train_loss: 0.42687761012411257 test_loss: 0.45968539516607326\n",
      "iteration 2826train_loss: 0.42687760611664444 test_loss: 0.45968587195529104\n",
      "iteration 2827train_loss: 0.4268776021140134 test_loss: 0.4596863484629524\n",
      "iteration 2828train_loss: 0.42687759811621323 test_loss: 0.4596868246892213\n",
      "iteration 2829train_loss: 0.42687759412323845 test_loss: 0.45968730063426116\n",
      "iteration 2830train_loss: 0.426877590135083 test_loss: 0.4596877762982354\n",
      "iteration 2831train_loss: 0.426877586151741 test_loss: 0.45968825168130756\n",
      "iteration 2832train_loss: 0.4268775821732066 test_loss: 0.45968872678364087\n",
      "iteration 2833train_loss: 0.426877578199474 test_loss: 0.45968920160539845\n",
      "iteration 2834train_loss: 0.4268775742305373 test_loss: 0.4596896761467436\n",
      "iteration 2835train_loss: 0.42687757026639067 test_loss: 0.4596901504078392\n",
      "iteration 2836train_loss: 0.4268775663070283 test_loss: 0.4596906243888481\n",
      "iteration 2837train_loss: 0.42687756235244434 test_loss: 0.4596910980899333\n",
      "iteration 2838train_loss: 0.4268775584026331 test_loss: 0.45969157151125745\n",
      "iteration 2839train_loss: 0.4268775544575885 test_loss: 0.45969204465298324\n",
      "iteration 2840train_loss: 0.426877550517305 test_loss: 0.45969251751527307\n",
      "iteration 2841train_loss: 0.4268775465817766 test_loss: 0.4596929900982895\n",
      "iteration 2842train_loss: 0.42687754265099775 test_loss: 0.45969346240219494\n",
      "iteration 2843train_loss: 0.4268775387249625 test_loss: 0.4596939344271515\n",
      "iteration 2844train_loss: 0.42687753480366514 test_loss: 0.4596944061733215\n",
      "iteration 2845train_loss: 0.42687753088709984 test_loss: 0.45969487764086703\n",
      "iteration 2846train_loss: 0.42687752697526093 test_loss: 0.45969534882995\n",
      "iteration 2847train_loss: 0.4268775230681426 test_loss: 0.4596958197407323\n",
      "iteration 2848train_loss: 0.42687751916573907 test_loss: 0.45969629037337584\n",
      "iteration 2849train_loss: 0.4268775152680447 test_loss: 0.4596967607280421\n",
      "iteration 2850train_loss: 0.4268775113750537 test_loss: 0.45969723080489283\n",
      "iteration 2851train_loss: 0.4268775074867604 test_loss: 0.45969770060408977\n",
      "iteration 2852train_loss: 0.42687750360315896 test_loss: 0.45969817012579395\n",
      "iteration 2853train_loss: 0.4268774997242439 test_loss: 0.459698639370167\n",
      "iteration 2854train_loss: 0.4268774958500092 test_loss: 0.45969910833737\n",
      "iteration 2855train_loss: 0.42687749198044955 test_loss: 0.45969957702756425\n",
      "iteration 2856train_loss: 0.4268774881155589 test_loss: 0.45970004544091064\n",
      "iteration 2857train_loss: 0.42687748425533184 test_loss: 0.4597005135775702\n",
      "iteration 2858train_loss: 0.42687748039976253 test_loss: 0.4597009814377039\n",
      "iteration 2859train_loss: 0.42687747654884534 test_loss: 0.4597014490214724\n",
      "iteration 2860train_loss: 0.42687747270257465 test_loss: 0.45970191632903645\n",
      "iteration 2861train_loss: 0.42687746886094474 test_loss: 0.4597023833605567\n",
      "iteration 2862train_loss: 0.42687746502395013 test_loss: 0.4597028501161936\n",
      "iteration 2863train_loss: 0.426877461191585 test_loss: 0.4597033165961075\n",
      "iteration 2864train_loss: 0.42687745736384375 test_loss: 0.4597037828004588\n",
      "iteration 2865train_loss: 0.4268774535407208 test_loss: 0.4597042487294078\n",
      "iteration 2866train_loss: 0.4268774497222106 test_loss: 0.45970471438311444\n",
      "iteration 2867train_loss: 0.42687744590830745 test_loss: 0.459705179761739\n",
      "iteration 2868train_loss: 0.42687744209900574 test_loss: 0.45970564486544124\n",
      "iteration 2869train_loss: 0.4268774382942999 test_loss: 0.45970610969438125\n",
      "iteration 2870train_loss: 0.42687743449418436 test_loss: 0.4597065742487185\n",
      "iteration 2871train_loss: 0.4268774306986536 test_loss: 0.45970703852861294\n",
      "iteration 2872train_loss: 0.4268774269077018 test_loss: 0.45970750253422404\n",
      "iteration 2873train_loss: 0.42687742312132365 test_loss: 0.4597079662657113\n",
      "iteration 2874train_loss: 0.42687741933951345 test_loss: 0.45970842972323417\n",
      "iteration 2875train_loss: 0.4268774155622657 test_loss: 0.459708892906952\n",
      "iteration 2876train_loss: 0.42687741178957495 test_loss: 0.4597093558170239\n",
      "iteration 2877train_loss: 0.42687740802143537 test_loss: 0.45970981845360903\n",
      "iteration 2878train_loss: 0.42687740425784176 test_loss: 0.4597102808168665\n",
      "iteration 2879train_loss: 0.42687740049878836 test_loss: 0.4597107429069552\n",
      "iteration 2880train_loss: 0.4268773967442698 test_loss: 0.45971120472403415\n",
      "iteration 2881train_loss: 0.4268773929942804 test_loss: 0.4597116662682618\n",
      "iteration 2882train_loss: 0.4268773892488149 test_loss: 0.4597121275397972\n",
      "iteration 2883train_loss: 0.4268773855078676 test_loss: 0.45971258853879865\n",
      "iteration 2884train_loss: 0.42687738177143314 test_loss: 0.4597130492654248\n",
      "iteration 2885train_loss: 0.42687737803950576 test_loss: 0.459713509719834\n",
      "iteration 2886train_loss: 0.42687737431208034 test_loss: 0.4597139699021846\n",
      "iteration 2887train_loss: 0.42687737058915126 test_loss: 0.4597144298126349\n",
      "iteration 2888train_loss: 0.4268773668707128 test_loss: 0.4597148894513429\n",
      "iteration 2889train_loss: 0.4268773631567601 test_loss: 0.4597153488184667\n",
      "iteration 2890train_loss: 0.42687735944728716 test_loss: 0.4597158079141643\n",
      "iteration 2891train_loss: 0.42687735574228874 test_loss: 0.45971626673859345\n",
      "iteration 2892train_loss: 0.42687735204175936 test_loss: 0.45971672529191204\n",
      "iteration 2893train_loss: 0.42687734834569374 test_loss: 0.4597171835742777\n",
      "iteration 2894train_loss: 0.4268773446540863 test_loss: 0.45971764158584805\n",
      "iteration 2895train_loss: 0.42687734096693153 test_loss: 0.4597180993267807\n",
      "iteration 2896train_loss: 0.4268773372842243 test_loss: 0.45971855679723284\n",
      "iteration 2897train_loss: 0.426877333605959 test_loss: 0.459719013997362\n",
      "iteration 2898train_loss: 0.42687732993213023 test_loss: 0.45971947092732524\n",
      "iteration 2899train_loss: 0.4268773262627327 test_loss: 0.45971992758727986\n",
      "iteration 2900train_loss: 0.426877322597761 test_loss: 0.4597203839773828\n",
      "iteration 2901train_loss: 0.4268773189372097 test_loss: 0.4597208400977912\n",
      "iteration 2902train_loss: 0.4268773152810734 test_loss: 0.45972129594866173\n",
      "iteration 2903train_loss: 0.426877311629347 test_loss: 0.4597217515301513\n",
      "iteration 2904train_loss: 0.4268773079820248 test_loss: 0.4597222068424166\n",
      "iteration 2905train_loss: 0.42687730433910154 test_loss: 0.4597226618856143\n",
      "iteration 2906train_loss: 0.42687730070057195 test_loss: 0.45972311665990073\n",
      "iteration 2907train_loss: 0.4268772970664308 test_loss: 0.4597235711654325\n",
      "iteration 2908train_loss: 0.42687729343667247 test_loss: 0.45972402540236584\n",
      "iteration 2909train_loss: 0.4268772898112919 test_loss: 0.45972447937085703\n",
      "iteration 2910train_loss: 0.4268772861902835 test_loss: 0.4597249330710624\n",
      "iteration 2911train_loss: 0.4268772825736422 test_loss: 0.4597253865031377\n",
      "iteration 2912train_loss: 0.4268772789613626 test_loss: 0.4597258396672393\n",
      "iteration 2913train_loss: 0.4268772753534395 test_loss: 0.45972629256352276\n",
      "iteration 2914train_loss: 0.42687727174986745 test_loss: 0.45972674519214396\n",
      "iteration 2915train_loss: 0.42687726815064125 test_loss: 0.4597271975532588\n",
      "iteration 2916train_loss: 0.42687726455575564 test_loss: 0.45972764964702284\n",
      "iteration 2917train_loss: 0.42687726096520523 test_loss: 0.4597281014735915\n",
      "iteration 2918train_loss: 0.4268772573789849 test_loss: 0.4597285530331204\n",
      "iteration 2919train_loss: 0.4268772537970894 test_loss: 0.4597290043257648\n",
      "iteration 2920train_loss: 0.4268772502195133 test_loss: 0.4597294553516799\n",
      "iteration 2921train_loss: 0.42687724664625154 test_loss: 0.4597299061110211\n",
      "iteration 2922train_loss: 0.4268772430772988 test_loss: 0.45973035660394335\n",
      "iteration 2923train_loss: 0.42687723951264983 test_loss: 0.4597308068306017\n",
      "iteration 2924train_loss: 0.42687723595229937 test_loss: 0.459731256791151\n",
      "iteration 2925train_loss: 0.4268772323962423 test_loss: 0.4597317064857463\n",
      "iteration 2926train_loss: 0.42687722884447343 test_loss: 0.4597321559145421\n",
      "iteration 2927train_loss: 0.4268772252969874 test_loss: 0.4597326050776933\n",
      "iteration 2928train_loss: 0.4268772217537791 test_loss: 0.4597330539753542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2929train_loss: 0.4268772182148434 test_loss: 0.45973350260767953\n",
      "iteration 2930train_loss: 0.42687721468017487 test_loss: 0.4597339509748235\n",
      "iteration 2931train_loss: 0.4268772111497687 test_loss: 0.4597343990769405\n",
      "iteration 2932train_loss: 0.4268772076236194 test_loss: 0.45973484691418487\n",
      "iteration 2933train_loss: 0.426877204101722 test_loss: 0.45973529448671047\n",
      "iteration 2934train_loss: 0.42687720058407114 test_loss: 0.4597357417946716\n",
      "iteration 2935train_loss: 0.4268771970706619 test_loss: 0.459736188838222\n",
      "iteration 2936train_loss: 0.4268771935614889 test_loss: 0.4597366356175157\n",
      "iteration 2937train_loss: 0.42687719005654706 test_loss: 0.4597370821327065\n",
      "iteration 2938train_loss: 0.4268771865558314 test_loss: 0.45973752838394805\n",
      "iteration 2939train_loss: 0.4268771830593367 test_loss: 0.4597379743713938\n",
      "iteration 2940train_loss: 0.4268771795670577 test_loss: 0.4597384200951975\n",
      "iteration 2941train_loss: 0.4268771760789894 test_loss: 0.4597388655555123\n",
      "iteration 2942train_loss: 0.4268771725951268 test_loss: 0.459739310752492\n",
      "iteration 2943train_loss: 0.42687716911546447 test_loss: 0.45973975568628944\n",
      "iteration 2944train_loss: 0.4268771656399976 test_loss: 0.4597402003570579\n",
      "iteration 2945train_loss: 0.4268771621687211 test_loss: 0.45974064476495063\n",
      "iteration 2946train_loss: 0.4268771587016298 test_loss: 0.4597410889101205\n",
      "iteration 2947train_loss: 0.4268771552387186 test_loss: 0.4597415327927203\n",
      "iteration 2948train_loss: 0.4268771517799823 test_loss: 0.4597419764129031\n",
      "iteration 2949train_loss: 0.4268771483254161 test_loss: 0.4597424197708214\n",
      "iteration 2950train_loss: 0.4268771448750149 test_loss: 0.4597428628666282\n",
      "iteration 2951train_loss: 0.42687714142877337 test_loss: 0.4597433057004756\n",
      "iteration 2952train_loss: 0.42687713798668675 test_loss: 0.4597437482725163\n",
      "iteration 2953train_loss: 0.42687713454874987 test_loss: 0.4597441905829028\n",
      "iteration 2954train_loss: 0.42687713111495773 test_loss: 0.4597446326317873\n",
      "iteration 2955train_loss: 0.42687712768530534 test_loss: 0.45974507441932194\n",
      "iteration 2956train_loss: 0.42687712425978763 test_loss: 0.45974551594565893\n",
      "iteration 2957train_loss: 0.4268771208383994 test_loss: 0.4597459572109503\n",
      "iteration 2958train_loss: 0.426877117421136 test_loss: 0.4597463982153481\n",
      "iteration 2959train_loss: 0.4268771140079923 test_loss: 0.459746838959004\n",
      "iteration 2960train_loss: 0.42687711059896294 test_loss: 0.45974727944207\n",
      "iteration 2961train_loss: 0.4268771071940434 test_loss: 0.45974771966469763\n",
      "iteration 2962train_loss: 0.42687710379322863 test_loss: 0.4597481596270386\n",
      "iteration 2963train_loss: 0.42687710039651344 test_loss: 0.45974859932924445\n",
      "iteration 2964train_loss: 0.4268770970038929 test_loss: 0.4597490387714665\n",
      "iteration 2965train_loss: 0.4268770936153622 test_loss: 0.4597494779538562\n",
      "iteration 2966train_loss: 0.42687709023091625 test_loss: 0.4597499168765648\n",
      "iteration 2967train_loss: 0.4268770868505501 test_loss: 0.45975035553974347\n",
      "iteration 2968train_loss: 0.42687708347425884 test_loss: 0.45975079394354335\n",
      "iteration 2969train_loss: 0.4268770801020375 test_loss: 0.45975123208811547\n",
      "iteration 2970train_loss: 0.42687707673388103 test_loss: 0.4597516699736107\n",
      "iteration 2971train_loss: 0.42687707336978475 test_loss: 0.4597521076001797\n",
      "iteration 2972train_loss: 0.4268770700097435 test_loss: 0.4597525449679737\n",
      "iteration 2973train_loss: 0.4268770666537525 test_loss: 0.45975298207714294\n",
      "iteration 2974train_loss: 0.42687706330180686 test_loss: 0.4597534189278381\n",
      "iteration 2975train_loss: 0.42687705995390157 test_loss: 0.45975385552020986\n",
      "iteration 2976train_loss: 0.4268770566100317 test_loss: 0.4597542918544083\n",
      "iteration 2977train_loss: 0.42687705327019243 test_loss: 0.45975472793058414\n",
      "iteration 2978train_loss: 0.426877049934379 test_loss: 0.45975516374888725\n",
      "iteration 2979train_loss: 0.42687704660258635 test_loss: 0.45975559930946813\n",
      "iteration 2980train_loss: 0.42687704327480946 test_loss: 0.4597560346124766\n",
      "iteration 2981train_loss: 0.4268770399510437 test_loss: 0.4597564696580627\n",
      "iteration 2982train_loss: 0.42687703663128423 test_loss: 0.4597569044463765\n",
      "iteration 2983train_loss: 0.42687703331552607 test_loss: 0.45975733897756754\n",
      "iteration 2984train_loss: 0.4268770300037644 test_loss: 0.45975777325178574\n",
      "iteration 2985train_loss: 0.4268770266959944 test_loss: 0.45975820726918065\n",
      "iteration 2986train_loss: 0.4268770233922111 test_loss: 0.45975864102990194\n",
      "iteration 2987train_loss: 0.42687702009240985 test_loss: 0.45975907453409903\n",
      "iteration 2988train_loss: 0.42687701679658563 test_loss: 0.45975950778192126\n",
      "iteration 2989train_loss: 0.4268770135047339 test_loss: 0.45975994077351795\n",
      "iteration 2990train_loss: 0.4268770102168495 test_loss: 0.4597603735090386\n",
      "iteration 2991train_loss: 0.4268770069329278 test_loss: 0.4597608059886319\n",
      "iteration 2992train_loss: 0.4268770036529641 test_loss: 0.459761238212447\n",
      "iteration 2993train_loss: 0.4268770003769533 test_loss: 0.4597616701806332\n",
      "iteration 2994train_loss: 0.426876997104891 test_loss: 0.45976210189333905\n",
      "iteration 2995train_loss: 0.426876993836772 test_loss: 0.4597625333507136\n",
      "iteration 2996train_loss: 0.4268769905725917 test_loss: 0.45976296455290533\n",
      "iteration 2997train_loss: 0.4268769873123454 test_loss: 0.4597633955000629\n",
      "iteration 2998train_loss: 0.4268769840560283 test_loss: 0.45976382619233513\n",
      "iteration 2999train_loss: 0.42687698080363556 test_loss: 0.45976425662987014\n",
      "iteration 3000train_loss: 0.42687697755516246 test_loss: 0.45976468681281646\n",
      "iteration 3001train_loss: 0.4268769743106042 test_loss: 0.45976511674132237\n",
      "iteration 3002train_loss: 0.4268769710699561 test_loss: 0.45976554641553613\n",
      "iteration 3003train_loss: 0.42687696783321344 test_loss: 0.45976597583560586\n",
      "iteration 3004train_loss: 0.4268769646003714 test_loss: 0.4597664050016796\n",
      "iteration 3005train_loss: 0.4268769613714254 test_loss: 0.4597668339139052\n",
      "iteration 3006train_loss: 0.42687695814637044 test_loss: 0.45976726257243067\n",
      "iteration 3007train_loss: 0.42687695492520206 test_loss: 0.4597676909774038\n",
      "iteration 3008train_loss: 0.42687695170791545 test_loss: 0.4597681191289721\n",
      "iteration 3009train_loss: 0.4268769484945059 test_loss: 0.4597685470272834\n",
      "iteration 3010train_loss: 0.4268769452849687 test_loss: 0.45976897467248523\n",
      "iteration 3011train_loss: 0.42687694207929927 test_loss: 0.45976940206472494\n",
      "iteration 3012train_loss: 0.4268769388774927 test_loss: 0.45976982920415005\n",
      "iteration 3013train_loss: 0.42687693567954443 test_loss: 0.4597702560909077\n",
      "iteration 3014train_loss: 0.42687693248544994 test_loss: 0.45977068272514515\n",
      "iteration 3015train_loss: 0.4268769292952042 test_loss: 0.4597711091070096\n",
      "iteration 3016train_loss: 0.42687692610880296 test_loss: 0.45977153523664804\n",
      "iteration 3017train_loss: 0.4268769229262412 test_loss: 0.45977196111420743\n",
      "iteration 3018train_loss: 0.4268769197475144 test_loss: 0.4597723867398346\n",
      "iteration 3019train_loss: 0.42687691657261806 test_loss: 0.4597728121136762\n",
      "iteration 3020train_loss: 0.42687691340154726 test_loss: 0.4597732372358794\n",
      "iteration 3021train_loss: 0.4268769102342976 test_loss: 0.4597736621065904\n",
      "iteration 3022train_loss: 0.42687690707086423 test_loss: 0.4597740867259558\n",
      "iteration 3023train_loss: 0.4268769039112428 test_loss: 0.4597745110941223\n",
      "iteration 3024train_loss: 0.42687690075542833 test_loss: 0.4597749352112361\n",
      "iteration 3025train_loss: 0.4268768976034166 test_loss: 0.4597753590774433\n",
      "iteration 3026train_loss: 0.4268768944552027 test_loss: 0.4597757826928904\n",
      "iteration 3027train_loss: 0.42687689131078216 test_loss: 0.4597762060577235\n",
      "iteration 3028train_loss: 0.42687688817015035 test_loss: 0.45977662917208856\n",
      "iteration 3029train_loss: 0.42687688503330273 test_loss: 0.45977705203613156\n",
      "iteration 3030train_loss: 0.42687688190023465 test_loss: 0.45977747464999846\n",
      "iteration 3031train_loss: 0.4268768787709416 test_loss: 0.4597778970138349\n",
      "iteration 3032train_loss: 0.4268768756454189 test_loss: 0.4597783191277867\n",
      "iteration 3033train_loss: 0.42687687252366197 test_loss: 0.4597787409919995\n",
      "iteration 3034train_loss: 0.42687686940566644 test_loss: 0.4597791626066188\n",
      "iteration 3035train_loss: 0.42687686629142757 test_loss: 0.45977958397179003\n",
      "iteration 3036train_loss: 0.42687686318094087 test_loss: 0.4597800050876587\n",
      "iteration 3037train_loss: 0.4268768600742017 test_loss: 0.45978042595436996\n",
      "iteration 3038train_loss: 0.4268768569712057 test_loss: 0.4597808465720692\n",
      "iteration 3039train_loss: 0.4268768538719482 test_loss: 0.4597812669409015\n",
      "iteration 3040train_loss: 0.4268768507764247 test_loss: 0.4597816870610118\n",
      "iteration 3041train_loss: 0.42687684768463063 test_loss: 0.4597821069325452\n",
      "iteration 3042train_loss: 0.42687684459656156 test_loss: 0.45978252655564655\n",
      "iteration 3043train_loss: 0.426876841512213 test_loss: 0.45978294593046054\n",
      "iteration 3044train_loss: 0.4268768384315803 test_loss: 0.45978336505713224\n",
      "iteration 3045train_loss: 0.4268768353546591 test_loss: 0.45978378393580593\n",
      "iteration 3046train_loss: 0.42687683228144474 test_loss: 0.4597842025666264\n",
      "iteration 3047train_loss: 0.42687682921193293 test_loss: 0.45978462094973804\n",
      "iteration 3048train_loss: 0.426876826146119 test_loss: 0.4597850390852852\n",
      "iteration 3049train_loss: 0.42687682308399855 test_loss: 0.4597854569734123\n",
      "iteration 3050train_loss: 0.4268768200255671 test_loss: 0.4597858746142635\n",
      "iteration 3051train_loss: 0.4268768169708202 test_loss: 0.45978629200798316\n",
      "iteration 3052train_loss: 0.42687681391975335 test_loss: 0.45978670915471503\n",
      "iteration 3053train_loss: 0.4268768108723622 test_loss: 0.45978712605460326\n",
      "iteration 3054train_loss: 0.4268768078286422 test_loss: 0.4597875427077919\n",
      "iteration 3055train_loss: 0.4268768047885888 test_loss: 0.4597879591144245\n",
      "iteration 3056train_loss: 0.4268768017521977 test_loss: 0.45978837527464506\n",
      "iteration 3057train_loss: 0.4268767987194645 test_loss: 0.4597887911885971\n",
      "iteration 3058train_loss: 0.42687679569038456 test_loss: 0.45978920685642416\n",
      "iteration 3059train_loss: 0.4268767926649537 test_loss: 0.4597896222782699\n",
      "iteration 3060train_loss: 0.4268767896431674 test_loss: 0.45979003745427777\n",
      "iteration 3061train_loss: 0.4268767866250212 test_loss: 0.4597904523845908\n",
      "iteration 3062train_loss: 0.42687678361051073 test_loss: 0.45979086706935274\n",
      "iteration 3063train_loss: 0.4268767805996316 test_loss: 0.45979128150870635\n",
      "iteration 3064train_loss: 0.4268767775923794 test_loss: 0.45979169570279477\n",
      "iteration 3065train_loss: 0.4268767745887498 test_loss: 0.4597921096517613\n",
      "iteration 3066train_loss: 0.42687677158873827 test_loss: 0.45979252335574866\n",
      "iteration 3067train_loss: 0.42687676859234036 test_loss: 0.45979293681489974\n",
      "iteration 3068train_loss: 0.42687676559955195 test_loss: 0.4597933500293573\n",
      "iteration 3069train_loss: 0.4268767626103686 test_loss: 0.4597937629992641\n",
      "iteration 3070train_loss: 0.42687675962478583 test_loss: 0.45979417572476267\n",
      "iteration 3071train_loss: 0.42687675664279934 test_loss: 0.45979458820599567\n",
      "iteration 3072train_loss: 0.4268767536644048 test_loss: 0.45979500044310545\n",
      "iteration 3073train_loss: 0.42687675068959785 test_loss: 0.4597954124362343\n",
      "iteration 3074train_loss: 0.4268767477183741 test_loss: 0.45979582418552467\n",
      "iteration 3075train_loss: 0.4268767447507292 test_loss: 0.4597962356911187\n",
      "iteration 3076train_loss: 0.42687674178665896 test_loss: 0.4597966469531587\n",
      "iteration 3077train_loss: 0.4268767388261589 test_loss: 0.4597970579717864\n",
      "iteration 3078train_loss: 0.42687673586922475 test_loss: 0.45979746874714394\n",
      "iteration 3079train_loss: 0.4268767329158521 test_loss: 0.4597978792793732\n",
      "iteration 3080train_loss: 0.42687672996603676 test_loss: 0.4597982895686161\n",
      "iteration 3081train_loss: 0.4268767270197744 test_loss: 0.4597986996150142\n",
      "iteration 3082train_loss: 0.4268767240770608 test_loss: 0.45979910941870916\n",
      "iteration 3083train_loss: 0.42687672113789144 test_loss: 0.4597995189798427\n",
      "iteration 3084train_loss: 0.42687671820226214 test_loss: 0.4597999282985562\n",
      "iteration 3085train_loss: 0.4268767152701686 test_loss: 0.45980033737499104\n",
      "iteration 3086train_loss: 0.4268767123416067 test_loss: 0.4598007462092886\n",
      "iteration 3087train_loss: 0.42687670941657185 test_loss: 0.45980115480159006\n",
      "iteration 3088train_loss: 0.42687670649506004 test_loss: 0.45980156315203674\n",
      "iteration 3089train_loss: 0.426876703577067 test_loss: 0.45980197126076966\n",
      "iteration 3090train_loss: 0.4268767006625882 test_loss: 0.4598023791279297\n",
      "iteration 3091train_loss: 0.4268766977516197 test_loss: 0.45980278675365793\n",
      "iteration 3092train_loss: 0.4268766948441571 test_loss: 0.4598031941380953\n",
      "iteration 3093train_loss: 0.42687669194019606 test_loss: 0.45980360128138226\n",
      "iteration 3094train_loss: 0.4268766890397325 test_loss: 0.45980400818365985\n",
      "iteration 3095train_loss: 0.4268766861427622 test_loss: 0.45980441484506845\n",
      "iteration 3096train_loss: 0.42687668324928085 test_loss: 0.4598048212657487\n",
      "iteration 3097train_loss: 0.4268766803592842 test_loss: 0.45980522744584107\n",
      "iteration 3098train_loss: 0.42687667747276814 test_loss: 0.4598056333854858\n",
      "iteration 3099train_loss: 0.4268766745897284 test_loss: 0.45980603908482337\n",
      "iteration 3100train_loss: 0.4268766717101607 test_loss: 0.4598064445439937\n",
      "iteration 3101train_loss: 0.4268766688340609 test_loss: 0.45980684976313735\n",
      "iteration 3102train_loss: 0.4268766659614249 test_loss: 0.45980725474239403\n",
      "iteration 3103train_loss: 0.4268766630922483 test_loss: 0.45980765948190394\n",
      "iteration 3104train_loss: 0.42687666022652704 test_loss: 0.45980806398180674\n",
      "iteration 3105train_loss: 0.4268766573642569 test_loss: 0.4598084682422424\n",
      "iteration 3106train_loss: 0.4268766545054337 test_loss: 0.4598088722633507\n",
      "iteration 3107train_loss: 0.4268766516500533 test_loss: 0.4598092760452713\n",
      "iteration 3108train_loss: 0.4268766487981115 test_loss: 0.4598096795881437\n",
      "iteration 3109train_loss: 0.42687664594960417 test_loss: 0.4598100828921074\n",
      "iteration 3110train_loss: 0.4268766431045271 test_loss: 0.45981048595730184\n",
      "iteration 3111train_loss: 0.42687664026287625 test_loss: 0.4598108887838664\n",
      "iteration 3112train_loss: 0.4268766374246472 test_loss: 0.4598112913719403\n",
      "iteration 3113train_loss: 0.42687663458983627 test_loss: 0.4598116937216628\n",
      "iteration 3114train_loss: 0.4268766317584388 test_loss: 0.45981209583317295\n",
      "iteration 3115train_loss: 0.4268766289304511 test_loss: 0.45981249770660976\n",
      "iteration 3116train_loss: 0.4268766261058687 test_loss: 0.45981289934211234\n",
      "iteration 3117train_loss: 0.42687662328468773 test_loss: 0.45981330073981935\n",
      "iteration 3118train_loss: 0.426876620466904 test_loss: 0.45981370189986975\n",
      "iteration 3119train_loss: 0.42687661765251333 test_loss: 0.45981410282240215\n",
      "iteration 3120train_loss: 0.4268766148415116 test_loss: 0.4598145035075552\n",
      "iteration 3121train_loss: 0.42687661203389493 test_loss: 0.45981490395546737\n",
      "iteration 3122train_loss: 0.4268766092296589 test_loss: 0.45981530416627747\n",
      "iteration 3123train_loss: 0.4268766064287996 test_loss: 0.45981570414012357\n",
      "iteration 3124train_loss: 0.4268766036313128 test_loss: 0.45981610387714417\n",
      "iteration 3125train_loss: 0.4268766008371947 test_loss: 0.4598165033774774\n",
      "iteration 3126train_loss: 0.42687659804644096 test_loss: 0.4598169026412616\n",
      "iteration 3127train_loss: 0.42687659525904764 test_loss: 0.4598173016686346\n",
      "iteration 3128train_loss: 0.42687659247501075 test_loss: 0.45981770045973464\n",
      "iteration 3129train_loss: 0.42687658969432596 test_loss: 0.4598180990146996\n",
      "iteration 3130train_loss: 0.42687658691698943 test_loss: 0.45981849733366725\n",
      "iteration 3131train_loss: 0.426876584142997 test_loss: 0.4598188954167756\n",
      "iteration 3132train_loss: 0.4268765813723447 test_loss: 0.45981929326416204\n",
      "iteration 3133train_loss: 0.4268765786050285 test_loss: 0.4598196908759644\n",
      "iteration 3134train_loss: 0.42687657584104427 test_loss: 0.45982008825232007\n",
      "iteration 3135train_loss: 0.42687657308038796 test_loss: 0.4598204853933667\n",
      "iteration 3136train_loss: 0.4268765703230557 test_loss: 0.4598208822992415\n",
      "iteration 3137train_loss: 0.4268765675690433 test_loss: 0.4598212789700819\n",
      "iteration 3138train_loss: 0.42687656481834685 test_loss: 0.4598216754060252\n",
      "iteration 3139train_loss: 0.42687656207096236 test_loss: 0.45982207160720834\n",
      "iteration 3140train_loss: 0.42687655932688573 test_loss: 0.45982246757376855\n",
      "iteration 3141train_loss: 0.42687655658611295 test_loss: 0.4598228633058428\n",
      "iteration 3142train_loss: 0.4268765538486401 test_loss: 0.45982325880356806\n",
      "iteration 3143train_loss: 0.42687655111446315 test_loss: 0.4598236540670812\n",
      "iteration 3144train_loss: 0.42687654838357814 test_loss: 0.45982404909651875\n",
      "iteration 3145train_loss: 0.426876545655981 test_loss: 0.4598244438920177\n",
      "iteration 3146train_loss: 0.4268765429316679 test_loss: 0.4598248384537145\n",
      "iteration 3147train_loss: 0.42687654021063465 test_loss: 0.45982523278174575\n",
      "iteration 3148train_loss: 0.4268765374928775 test_loss: 0.45982562687624784\n",
      "iteration 3149train_loss: 0.4268765347783923 test_loss: 0.4598260207373573\n",
      "iteration 3150train_loss: 0.4268765320671753 test_loss: 0.45982641436521027\n",
      "iteration 3151train_loss: 0.4268765293592223 test_loss: 0.45982680775994306\n",
      "iteration 3152train_loss: 0.42687652665452974 test_loss: 0.45982720092169177\n",
      "iteration 3153train_loss: 0.4268765239530931 test_loss: 0.4598275938505926\n",
      "iteration 3154train_loss: 0.42687652125490905 test_loss: 0.4598279865467814\n",
      "iteration 3155train_loss: 0.4268765185599731 test_loss: 0.4598283790103942\n",
      "iteration 3156train_loss: 0.4268765158682818 test_loss: 0.4598287712415667\n",
      "iteration 3157train_loss: 0.42687651317983083 test_loss: 0.4598291632404348\n",
      "iteration 3158train_loss: 0.42687651049461656 test_loss: 0.4598295550071343\n",
      "iteration 3159train_loss: 0.4268765078126349 test_loss: 0.45982994654180054\n",
      "iteration 3160train_loss: 0.42687650513388203 test_loss: 0.4598303378445693\n",
      "iteration 3161train_loss: 0.426876502458354 test_loss: 0.4598307289155758\n",
      "iteration 3162train_loss: 0.4268764997860469 test_loss: 0.45983111975495566\n",
      "iteration 3163train_loss: 0.42687649711695685 test_loss: 0.459831510362844\n",
      "iteration 3164train_loss: 0.4268764944510801 test_loss: 0.45983190073937613\n",
      "iteration 3165train_loss: 0.42687649178841247 test_loss: 0.4598322908846873\n",
      "iteration 3166train_loss: 0.4268764891289503 test_loss: 0.45983268079891243\n",
      "iteration 3167train_loss: 0.42687648647268966 test_loss: 0.4598330704821866\n",
      "iteration 3168train_loss: 0.4268764838196266 test_loss: 0.45983345993464475\n",
      "iteration 3169train_loss: 0.42687648116975724 test_loss: 0.45983384915642167\n",
      "iteration 3170train_loss: 0.4268764785230779 test_loss: 0.45983423814765223\n",
      "iteration 3171train_loss: 0.4268764758795847 test_loss: 0.459834626908471\n",
      "iteration 3172train_loss: 0.4268764732392736 test_loss: 0.4598350154390127\n",
      "iteration 3173train_loss: 0.4268764706021408 test_loss: 0.4598354037394118\n",
      "iteration 3174train_loss: 0.4268764679681825 test_loss: 0.4598357918098029\n",
      "iteration 3175train_loss: 0.42687646533739504 test_loss: 0.45983617965032025\n",
      "iteration 3176train_loss: 0.4268764627097742 test_loss: 0.4598365672610981\n",
      "iteration 3177train_loss: 0.4268764600853164 test_loss: 0.459836954642271\n",
      "iteration 3178train_loss: 0.4268764574640178 test_loss: 0.4598373417939729\n",
      "iteration 3179train_loss: 0.42687645484587455 test_loss: 0.4598377287163379\n",
      "iteration 3180train_loss: 0.42687645223088294 test_loss: 0.45983811540949987\n",
      "iteration 3181train_loss: 0.42687644961903887 test_loss: 0.459838501873593\n",
      "iteration 3182train_loss: 0.42687644701033883 test_loss: 0.45983888810875106\n",
      "iteration 3183train_loss: 0.42687644440477884 test_loss: 0.4598392741151078\n",
      "iteration 3184train_loss: 0.4268764418023552 test_loss: 0.45983965989279707\n",
      "iteration 3185train_loss: 0.426876439203064 test_loss: 0.45984004544195234\n",
      "iteration 3186train_loss: 0.4268764366069016 test_loss: 0.45984043076270714\n",
      "iteration 3187train_loss: 0.42687643401386427 test_loss: 0.4598408158551951\n",
      "iteration 3188train_loss: 0.426876431423948 test_loss: 0.4598412007195496\n",
      "iteration 3189train_loss: 0.4268764288371491 test_loss: 0.45984158535590386\n",
      "iteration 3190train_loss: 0.42687642625346367 test_loss: 0.45984196976439123\n",
      "iteration 3191train_loss: 0.42687642367288836 test_loss: 0.45984235394514494\n",
      "iteration 3192train_loss: 0.42687642109541907 test_loss: 0.45984273789829794\n",
      "iteration 3193train_loss: 0.4268764185210521 test_loss: 0.45984312162398344\n",
      "iteration 3194train_loss: 0.4268764159497837 test_loss: 0.45984350512233424\n",
      "iteration 3195train_loss: 0.42687641338161014 test_loss: 0.4598438883934832\n",
      "iteration 3196train_loss: 0.4268764108165277 test_loss: 0.4598442714375634\n",
      "iteration 3197train_loss: 0.4268764082545326 test_loss: 0.45984465425470733\n",
      "iteration 3198train_loss: 0.4268764056956212 test_loss: 0.45984503684504763\n",
      "iteration 3199train_loss: 0.4268764031397896 test_loss: 0.459845419208717\n",
      "iteration 3200train_loss: 0.42687640058703413 test_loss: 0.45984580134584796\n",
      "iteration 3201train_loss: 0.4268763980373512 test_loss: 0.45984618325657267\n",
      "iteration 3202train_loss: 0.42687639549073697 test_loss: 0.4598465649410238\n",
      "iteration 3203train_loss: 0.42687639294718777 test_loss: 0.4598469463993336\n",
      "iteration 3204train_loss: 0.4268763904066999 test_loss: 0.4598473276316341\n",
      "iteration 3205train_loss: 0.42687638786926974 test_loss: 0.45984770863805763\n",
      "iteration 3206train_loss: 0.4268763853348934 test_loss: 0.459848089418736\n",
      "iteration 3207train_loss: 0.42687638280356727 test_loss: 0.4598484699738014\n",
      "iteration 3208train_loss: 0.4268763802752877 test_loss: 0.4598488503033857\n",
      "iteration 3209train_loss: 0.426876377750051 test_loss: 0.45984923040762066\n",
      "iteration 3210train_loss: 0.42687637522785343 test_loss: 0.459849610286638\n",
      "iteration 3211train_loss: 0.4268763727086914 test_loss: 0.4598499899405696\n",
      "iteration 3212train_loss: 0.42687637019256125 test_loss: 0.45985036936954693\n",
      "iteration 3213train_loss: 0.42687636767945925 test_loss: 0.4598507485737015\n",
      "iteration 3214train_loss: 0.4268763651693817 test_loss: 0.4598511275531648\n",
      "iteration 3215train_loss: 0.426876362662325 test_loss: 0.4598515063080683\n",
      "iteration 3216train_loss: 0.4268763601582855 test_loss: 0.4598518848385432\n",
      "iteration 3217train_loss: 0.42687635765725956 test_loss: 0.4598522631447208\n",
      "iteration 3218train_loss: 0.4268763551592435 test_loss: 0.45985264122673225\n",
      "iteration 3219train_loss: 0.4268763526642337 test_loss: 0.4598530190847085\n",
      "iteration 3220train_loss: 0.4268763501722266 test_loss: 0.4598533967187807\n",
      "iteration 3221train_loss: 0.4268763476832184 test_loss: 0.45985377412907996\n",
      "iteration 3222train_loss: 0.42687634519720563 test_loss: 0.45985415131573687\n",
      "iteration 3223train_loss: 0.4268763427141846 test_loss: 0.4598545282788823\n",
      "iteration 3224train_loss: 0.4268763402341518 test_loss: 0.45985490501864695\n",
      "iteration 3225train_loss: 0.4268763377571034 test_loss: 0.4598552815351615\n",
      "iteration 3226train_loss: 0.42687633528303587 test_loss: 0.4598556578285565\n",
      "iteration 3227train_loss: 0.4268763328119457 test_loss: 0.4598560338989625\n",
      "iteration 3228train_loss: 0.42687633034382927 test_loss: 0.4598564097465099\n",
      "iteration 3229train_loss: 0.42687632787868296 test_loss: 0.45985678537132896\n",
      "iteration 3230train_loss: 0.42687632541650317 test_loss: 0.4598571607735501\n",
      "iteration 3231train_loss: 0.42687632295728634 test_loss: 0.45985753595330336\n",
      "iteration 3232train_loss: 0.4268763205010287 test_loss: 0.459857910910719\n",
      "iteration 3233train_loss: 0.42687631804772697 test_loss: 0.459858285645927\n",
      "iteration 3234train_loss: 0.42687631559737754 test_loss: 0.4598586601590574\n",
      "iteration 3235train_loss: 0.42687631314997665 test_loss: 0.45985903445024\n",
      "iteration 3236train_loss: 0.4268763107055209 test_loss: 0.4598594085196047\n",
      "iteration 3237train_loss: 0.4268763082640065 test_loss: 0.4598597823672813\n",
      "iteration 3238train_loss: 0.42687630582543024 test_loss: 0.4598601559933994\n",
      "iteration 3239train_loss: 0.4268763033897882 test_loss: 0.4598605293980888\n",
      "iteration 3240train_loss: 0.4268763009570772 test_loss: 0.4598609025814787\n",
      "iteration 3241train_loss: 0.4268762985272935 test_loss: 0.4598612755436988\n",
      "iteration 3242train_loss: 0.42687629610043343 test_loss: 0.45986164828487863\n",
      "iteration 3243train_loss: 0.42687629367649366 test_loss: 0.45986202080514715\n",
      "iteration 3244train_loss: 0.42687629125547066 test_loss: 0.45986239310463384\n",
      "iteration 3245train_loss: 0.42687628883736084 test_loss: 0.45986276518346786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3246train_loss: 0.4268762864221607 test_loss: 0.4598631370417781\n",
      "iteration 3247train_loss: 0.4268762840098666 test_loss: 0.45986350867969383\n",
      "iteration 3248train_loss: 0.4268762816004752 test_loss: 0.459863880097344\n",
      "iteration 3249train_loss: 0.426876279193983 test_loss: 0.4598642512948573\n",
      "iteration 3250train_loss: 0.42687627679038637 test_loss: 0.45986462227236274\n",
      "iteration 3251train_loss: 0.42687627438968184 test_loss: 0.45986499302998896\n",
      "iteration 3252train_loss: 0.42687627199186595 test_loss: 0.4598653635678646\n",
      "iteration 3253train_loss: 0.42687626959693525 test_loss: 0.45986573388611823\n",
      "iteration 3254train_loss: 0.4268762672048862 test_loss: 0.45986610398487854\n",
      "iteration 3255train_loss: 0.4268762648157152 test_loss: 0.45986647386427365\n",
      "iteration 3256train_loss: 0.426876262429419 test_loss: 0.45986684352443236\n",
      "iteration 3257train_loss: 0.426876260045994 test_loss: 0.45986721296548255\n",
      "iteration 3258train_loss: 0.4268762576654367 test_loss: 0.45986758218755264\n",
      "iteration 3259train_loss: 0.4268762552877438 test_loss: 0.45986795119077073\n",
      "iteration 3260train_loss: 0.42687625291291154 test_loss: 0.45986831997526495\n",
      "iteration 3261train_loss: 0.42687625054093675 test_loss: 0.4598686885411634\n",
      "iteration 3262train_loss: 0.42687624817181585 test_loss: 0.4598690568885938\n",
      "iteration 3263train_loss: 0.4268762458055454 test_loss: 0.4598694250176842\n",
      "iteration 3264train_loss: 0.42687624344212194 test_loss: 0.4598697929285621\n",
      "iteration 3265train_loss: 0.42687624108154215 test_loss: 0.4598701606213557\n",
      "iteration 3266train_loss: 0.42687623872380237 test_loss: 0.4598705280961923\n",
      "iteration 3267train_loss: 0.42687623636889943 test_loss: 0.4598708953531994\n",
      "iteration 3268train_loss: 0.4268762340168296 test_loss: 0.4598712623925047\n",
      "iteration 3269train_loss: 0.42687623166758976 test_loss: 0.4598716292142356\n",
      "iteration 3270train_loss: 0.42687622932117636 test_loss: 0.45987199581851934\n",
      "iteration 3271train_loss: 0.42687622697758587 test_loss: 0.4598723622054834\n",
      "iteration 3272train_loss: 0.42687622463681507 test_loss: 0.4598727283752548\n",
      "iteration 3273train_loss: 0.4268762222988604 test_loss: 0.45987309432796075\n",
      "iteration 3274train_loss: 0.42687621996371866 test_loss: 0.4598734600637283\n",
      "iteration 3275train_loss: 0.4268762176313861 test_loss: 0.4598738255826846\n",
      "iteration 3276train_loss: 0.4268762153018598 test_loss: 0.45987419088495624\n",
      "iteration 3277train_loss: 0.42687621297513595 test_loss: 0.4598745559706704\n",
      "iteration 3278train_loss: 0.4268762106512114 test_loss: 0.4598749208399537\n",
      "iteration 3279train_loss: 0.4268762083300826 test_loss: 0.45987528549293283\n",
      "iteration 3280train_loss: 0.4268762060117463 test_loss: 0.4598756499297345\n",
      "iteration 3281train_loss: 0.4268762036961991 test_loss: 0.4598760141504853\n",
      "iteration 3282train_loss: 0.4268762013834376 test_loss: 0.4598763781553116\n",
      "iteration 3283train_loss: 0.4268761990734586 test_loss: 0.4598767419443399\n",
      "iteration 3284train_loss: 0.4268761967662585 test_loss: 0.45987710551769656\n",
      "iteration 3285train_loss: 0.42687619446183395 test_loss: 0.4598774688755078\n",
      "iteration 3286train_loss: 0.4268761921601818 test_loss: 0.45987783201789995\n",
      "iteration 3287train_loss: 0.42687618986129844 test_loss: 0.459878194944999\n",
      "iteration 3288train_loss: 0.4268761875651808 test_loss: 0.459878557656931\n",
      "iteration 3289train_loss: 0.4268761852718254 test_loss: 0.4598789201538221\n",
      "iteration 3290train_loss: 0.42687618298122887 test_loss: 0.4598792824357981\n",
      "iteration 3291train_loss: 0.42687618069338795 test_loss: 0.45987964450298485\n",
      "iteration 3292train_loss: 0.42687617840829917 test_loss: 0.459880006355508\n",
      "iteration 3293train_loss: 0.42687617612595935 test_loss: 0.45988036799349363\n",
      "iteration 3294train_loss: 0.42687617384636517 test_loss: 0.45988072941706715\n",
      "iteration 3295train_loss: 0.42687617156951324 test_loss: 0.45988109062635407\n",
      "iteration 3296train_loss: 0.4268761692954004 test_loss: 0.45988145162148\n",
      "iteration 3297train_loss: 0.42687616702402315 test_loss: 0.45988181240257014\n",
      "iteration 3298train_loss: 0.4268761647553782 test_loss: 0.45988217296975015\n",
      "iteration 3299train_loss: 0.4268761624894623 test_loss: 0.45988253332314505\n",
      "iteration 3300train_loss: 0.42687616022627217 test_loss: 0.45988289346288014\n",
      "iteration 3301train_loss: 0.42687615796580464 test_loss: 0.4598832533890806\n",
      "iteration 3302train_loss: 0.42687615570805615 test_loss: 0.45988361310187154\n",
      "iteration 3303train_loss: 0.42687615345302354 test_loss: 0.4598839726013778\n",
      "iteration 3304train_loss: 0.4268761512007036 test_loss: 0.45988433188772443\n",
      "iteration 3305train_loss: 0.426876148951093 test_loss: 0.4598846909610362\n",
      "iteration 3306train_loss: 0.42687614670418844 test_loss: 0.4598850498214379\n",
      "iteration 3307train_loss: 0.4268761444599866 test_loss: 0.4598854084690543\n",
      "iteration 3308train_loss: 0.42687614221848436 test_loss: 0.45988576690401\n",
      "iteration 3309train_loss: 0.4268761399796784 test_loss: 0.4598861251264296\n",
      "iteration 3310train_loss: 0.4268761377435653 test_loss: 0.45988648313643765\n",
      "iteration 3311train_loss: 0.42687613551014225 test_loss: 0.45988684093415844\n",
      "iteration 3312train_loss: 0.4268761332794055 test_loss: 0.4598871985197163\n",
      "iteration 3313train_loss: 0.426876131051352 test_loss: 0.4598875558932358\n",
      "iteration 3314train_loss: 0.42687612882597864 test_loss: 0.4598879130548408\n",
      "iteration 3315train_loss: 0.426876126603282 test_loss: 0.4598882700046557\n",
      "iteration 3316train_loss: 0.42687612438325895 test_loss: 0.45988862674280445\n",
      "iteration 3317train_loss: 0.42687612216590615 test_loss: 0.4598889832694111\n",
      "iteration 3318train_loss: 0.4268761199512204 test_loss: 0.45988933958459965\n",
      "iteration 3319train_loss: 0.42687611773919865 test_loss: 0.4598896956884938\n",
      "iteration 3320train_loss: 0.42687611552983756 test_loss: 0.4598900515812175\n",
      "iteration 3321train_loss: 0.42687611332313385 test_loss: 0.45989040726289443\n",
      "iteration 3322train_loss: 0.4268761111190843 test_loss: 0.45989076273364815\n",
      "iteration 3323train_loss: 0.42687610891768585 test_loss: 0.4598911179936025\n",
      "iteration 3324train_loss: 0.42687610671893517 test_loss: 0.4598914730428807\n",
      "iteration 3325train_loss: 0.42687610452282926 test_loss: 0.4598918278816062\n",
      "iteration 3326train_loss: 0.4268761023293646 test_loss: 0.45989218250990266\n",
      "iteration 3327train_loss: 0.42687610013853844 test_loss: 0.4598925369278931\n",
      "iteration 3328train_loss: 0.4268760979503471 test_loss: 0.45989289113570103\n",
      "iteration 3329train_loss: 0.42687609576478774 test_loss: 0.45989324513344926\n",
      "iteration 3330train_loss: 0.426876093581857 test_loss: 0.4598935989212611\n",
      "iteration 3331train_loss: 0.42687609140155186 test_loss: 0.4598939524992596\n",
      "iteration 3332train_loss: 0.426876089223869 test_loss: 0.4598943058675676\n",
      "iteration 3333train_loss: 0.4268760870488053 test_loss: 0.45989465902630805\n",
      "iteration 3334train_loss: 0.42687608487635775 test_loss: 0.4598950119756039\n",
      "iteration 3335train_loss: 0.426876082706523 test_loss: 0.4598953647155775\n",
      "iteration 3336train_loss: 0.426876080539298 test_loss: 0.45989571724635203\n",
      "iteration 3337train_loss: 0.4268760783746794 test_loss: 0.4598960695680497\n",
      "iteration 3338train_loss: 0.42687607621266427 test_loss: 0.45989642168079325\n",
      "iteration 3339train_loss: 0.42687607405324957 test_loss: 0.45989677358470504\n",
      "iteration 3340train_loss: 0.4268760718964318 test_loss: 0.4598971252799075\n",
      "iteration 3341train_loss: 0.426876069742208 test_loss: 0.4598974767665231\n",
      "iteration 3342train_loss: 0.4268760675905751 test_loss: 0.45989782804467383\n",
      "iteration 3343train_loss: 0.42687606544153 test_loss: 0.45989817911448205\n",
      "iteration 3344train_loss: 0.4268760632950694 test_loss: 0.45989852997606984\n",
      "iteration 3345train_loss: 0.42687606115119037 test_loss: 0.4598988806295593\n",
      "iteration 3346train_loss: 0.42687605900988956 test_loss: 0.45989923107507236\n",
      "iteration 3347train_loss: 0.42687605687116414 test_loss: 0.4598995813127309\n",
      "iteration 3348train_loss: 0.42687605473501083 test_loss: 0.45989993134265683\n",
      "iteration 3349train_loss: 0.42687605260142647 test_loss: 0.45990028116497195\n",
      "iteration 3350train_loss: 0.42687605047040816 test_loss: 0.4599006307797978\n",
      "iteration 3351train_loss: 0.4268760483419526 test_loss: 0.45990098018725617\n",
      "iteration 3352train_loss: 0.42687604621605674 test_loss: 0.4599013293874685\n",
      "iteration 3353train_loss: 0.4268760440927176 test_loss: 0.4599016783805565\n",
      "iteration 3354train_loss: 0.42687604197193196 test_loss: 0.45990202716664136\n",
      "iteration 3355train_loss: 0.42687603985369676 test_loss: 0.45990237574584447\n",
      "iteration 3356train_loss: 0.42687603773800914 test_loss: 0.45990272411828736\n",
      "iteration 3357train_loss: 0.42687603562486565 test_loss: 0.459903072284091\n",
      "iteration 3358train_loss: 0.4268760335142634 test_loss: 0.45990342024337655\n",
      "iteration 3359train_loss: 0.42687603140619945 test_loss: 0.45990376799626526\n",
      "iteration 3360train_loss: 0.4268760293006705 test_loss: 0.459904115542878\n",
      "iteration 3361train_loss: 0.4268760271976737 test_loss: 0.45990446288333575\n",
      "iteration 3362train_loss: 0.4268760250972059 test_loss: 0.4599048100177594\n",
      "iteration 3363train_loss: 0.4268760229992639 test_loss: 0.45990515694626977\n",
      "iteration 3364train_loss: 0.42687602090384497 test_loss: 0.4599055036689877\n",
      "iteration 3365train_loss: 0.4268760188109457 test_loss: 0.4599058501860336\n",
      "iteration 3366train_loss: 0.4268760167205634 test_loss: 0.4599061964975282\n",
      "iteration 3367train_loss: 0.4268760146326948 test_loss: 0.45990654260359215\n",
      "iteration 3368train_loss: 0.4268760125473369 test_loss: 0.4599068885043457\n",
      "iteration 3369train_loss: 0.42687601046448675 test_loss: 0.4599072341999095\n",
      "iteration 3370train_loss: 0.4268760083841413 test_loss: 0.4599075796904036\n",
      "iteration 3371train_loss: 0.42687600630629746 test_loss: 0.45990792497594846\n",
      "iteration 3372train_loss: 0.4268760042309522 test_loss: 0.4599082700566643\n",
      "iteration 3373train_loss: 0.4268760021581026 test_loss: 0.45990861493267093\n",
      "iteration 3374train_loss: 0.42687600008774546 test_loss: 0.4599089596040888\n",
      "iteration 3375train_loss: 0.4268759980198781 test_loss: 0.4599093040710377\n",
      "iteration 3376train_loss: 0.42687599595449727 test_loss: 0.4599096483336377\n",
      "iteration 3377train_loss: 0.4268759938916 test_loss: 0.45990999239200825\n",
      "iteration 3378train_loss: 0.42687599183118324 test_loss: 0.45991033624626954\n",
      "iteration 3379train_loss: 0.42687598977324415 test_loss: 0.4599106798965412\n",
      "iteration 3380train_loss: 0.4268759877177797 test_loss: 0.45991102334294276\n",
      "iteration 3381train_loss: 0.4268759856647868 test_loss: 0.45991136658559395\n",
      "iteration 3382train_loss: 0.4268759836142625 test_loss: 0.459911709624614\n",
      "iteration 3383train_loss: 0.42687598156620393 test_loss: 0.45991205246012273\n",
      "iteration 3384train_loss: 0.4268759795206079 test_loss: 0.45991239509223925\n",
      "iteration 3385train_loss: 0.4268759774774718 test_loss: 0.459912737521083\n",
      "iteration 3386train_loss: 0.4268759754367921 test_loss: 0.4599130797467731\n",
      "iteration 3387train_loss: 0.42687597339856637 test_loss: 0.4599134217694288\n",
      "iteration 3388train_loss: 0.4268759713627915 test_loss: 0.4599137635891692\n",
      "iteration 3389train_loss: 0.42687596932946426 test_loss: 0.4599141052061132\n",
      "iteration 3390train_loss: 0.426875967298582 test_loss: 0.45991444662038006\n",
      "iteration 3391train_loss: 0.42687596527014177 test_loss: 0.4599147878320884\n",
      "iteration 3392train_loss: 0.4268759632441404 test_loss: 0.4599151288413572\n",
      "iteration 3393train_loss: 0.426875961220575 test_loss: 0.4599154696483052\n",
      "iteration 3394train_loss: 0.4268759591994429 test_loss: 0.45991581025305106\n",
      "iteration 3395train_loss: 0.42687595718074084 test_loss: 0.45991615065571345\n",
      "iteration 3396train_loss: 0.4268759551644661 test_loss: 0.4599164908564109\n",
      "iteration 3397train_loss: 0.4268759531506156 test_loss: 0.45991683085526186\n",
      "iteration 3398train_loss: 0.4268759511391864 test_loss: 0.45991717065238485\n",
      "iteration 3399train_loss: 0.4268759491301758 test_loss: 0.4599175102478982\n",
      "iteration 3400train_loss: 0.42687594712358073 test_loss: 0.4599178496419202\n",
      "iteration 3401train_loss: 0.4268759451193982 test_loss: 0.45991818883456903\n",
      "iteration 3402train_loss: 0.4268759431176254 test_loss: 0.4599185278259628\n",
      "iteration 3403train_loss: 0.4268759411182594 test_loss: 0.4599188666162199\n",
      "iteration 3404train_loss: 0.4268759391212973 test_loss: 0.459919205205458\n",
      "iteration 3405train_loss: 0.4268759371267362 test_loss: 0.4599195435937952\n",
      "iteration 3406train_loss: 0.4268759351345731 test_loss: 0.45991988178134935\n",
      "iteration 3407train_loss: 0.4268759331448053 test_loss: 0.4599202197682384\n",
      "iteration 3408train_loss: 0.42687593115742983 test_loss: 0.4599205575545799\n",
      "iteration 3409train_loss: 0.42687592917244366 test_loss: 0.4599208951404917\n",
      "iteration 3410train_loss: 0.42687592718984413 test_loss: 0.4599212325260914\n",
      "iteration 3411train_loss: 0.42687592520962814 test_loss: 0.4599215697114965\n",
      "iteration 3412train_loss: 0.4268759232317931 test_loss: 0.4599219066968245\n",
      "iteration 3413train_loss: 0.4268759212563359 test_loss: 0.45992224348219274\n",
      "iteration 3414train_loss: 0.42687591928325364 test_loss: 0.4599225800677188\n",
      "iteration 3415train_loss: 0.4268759173125436 test_loss: 0.4599229164535198\n",
      "iteration 3416train_loss: 0.42687591534420294 test_loss: 0.4599232526397129\n",
      "iteration 3417train_loss: 0.42687591337822867 test_loss: 0.45992358862641536\n",
      "iteration 3418train_loss: 0.426875911414618 test_loss: 0.4599239244137443\n",
      "iteration 3419train_loss: 0.4268759094533681 test_loss: 0.45992426000181674\n",
      "iteration 3420train_loss: 0.426875907494476 test_loss: 0.4599245953907494\n",
      "iteration 3421train_loss: 0.42687590553793897 test_loss: 0.4599249305806594\n",
      "iteration 3422train_loss: 0.4268759035837543 test_loss: 0.4599252655716635\n",
      "iteration 3423train_loss: 0.4268759016319188 test_loss: 0.4599256003638786\n",
      "iteration 3424train_loss: 0.4268758996824299 test_loss: 0.4599259349574211\n",
      "iteration 3425train_loss: 0.42687589773528467 test_loss: 0.45992626935240777\n",
      "iteration 3426train_loss: 0.42687589579048035 test_loss: 0.4599266035489552\n",
      "iteration 3427train_loss: 0.42687589384801405 test_loss: 0.45992693754717984\n",
      "iteration 3428train_loss: 0.42687589190788305 test_loss: 0.4599272713471981\n",
      "iteration 3429train_loss: 0.4268758899700843 test_loss: 0.45992760494912627\n",
      "iteration 3430train_loss: 0.42687588803461535 test_loss: 0.45992793835308066\n",
      "iteration 3431train_loss: 0.426875886101473 test_loss: 0.4599282715591777\n",
      "iteration 3432train_loss: 0.4268758841706547 test_loss: 0.45992860456753326\n",
      "iteration 3433train_loss: 0.42687588224215745 test_loss: 0.4599289373782636\n",
      "iteration 3434train_loss: 0.42687588031597873 test_loss: 0.4599292699914847\n",
      "iteration 3435train_loss: 0.4268758783921155 test_loss: 0.45992960240731257\n",
      "iteration 3436train_loss: 0.426875876470565 test_loss: 0.459929934625863\n",
      "iteration 3437train_loss: 0.42687587455132453 test_loss: 0.45993026664725184\n",
      "iteration 3438train_loss: 0.4268758726343912 test_loss: 0.45993059847159484\n",
      "iteration 3439train_loss: 0.4268758707197624 test_loss: 0.45993093009900765\n",
      "iteration 3440train_loss: 0.4268758688074351 test_loss: 0.4599312615296062\n",
      "iteration 3441train_loss: 0.42687586689740675 test_loss: 0.4599315927635057\n",
      "iteration 3442train_loss: 0.4268758649896744 test_loss: 0.45993192380082176\n",
      "iteration 3443train_loss: 0.4268758630842354 test_loss: 0.45993225464166987\n",
      "iteration 3444train_loss: 0.426875861181087 test_loss: 0.4599325852861653\n",
      "iteration 3445train_loss: 0.4268758592802263 test_loss: 0.4599329157344234\n",
      "iteration 3446train_loss: 0.4268758573816506 test_loss: 0.4599332459865594\n",
      "iteration 3447train_loss: 0.42687585548535717 test_loss: 0.4599335760426884\n",
      "iteration 3448train_loss: 0.42687585359134333 test_loss: 0.4599339059029257\n",
      "iteration 3449train_loss: 0.42687585169960623 test_loss: 0.45993423556738605\n",
      "iteration 3450train_loss: 0.4268758498101431 test_loss: 0.45993456503618474\n",
      "iteration 3451train_loss: 0.4268758479229512 test_loss: 0.45993489430943635\n",
      "iteration 3452train_loss: 0.42687584603802786 test_loss: 0.45993522338725584\n",
      "iteration 3453train_loss: 0.4268758441553703 test_loss: 0.4599355522697581\n",
      "iteration 3454train_loss: 0.4268758422749758 test_loss: 0.45993588095705773\n",
      "iteration 3455train_loss: 0.4268758403968416 test_loss: 0.4599362094492694\n",
      "iteration 3456train_loss: 0.4268758385209652 test_loss: 0.4599365377465078\n",
      "iteration 3457train_loss: 0.4268758366473435 test_loss: 0.4599368658488872\n",
      "iteration 3458train_loss: 0.42687583477597385 test_loss: 0.4599371937565222\n",
      "iteration 3459train_loss: 0.42687583290685377 test_loss: 0.45993752146952704\n",
      "iteration 3460train_loss: 0.4268758310399804 test_loss: 0.4599378489880164\n",
      "iteration 3461train_loss: 0.42687582917535105 test_loss: 0.45993817631210415\n",
      "iteration 3462train_loss: 0.426875827312963 test_loss: 0.4599385034419046\n",
      "iteration 3463train_loss: 0.42687582545281355 test_loss: 0.4599388303775319\n",
      "iteration 3464train_loss: 0.42687582359489995 test_loss: 0.45993915711910005\n",
      "iteration 3465train_loss: 0.4268758217392195 test_loss: 0.45993948366672305\n",
      "iteration 3466train_loss: 0.42687581988576967 test_loss: 0.45993981002051487\n",
      "iteration 3467train_loss: 0.4268758180345476 test_loss: 0.45994013618058943\n",
      "iteration 3468train_loss: 0.4268758161855507 test_loss: 0.45994046214706036\n",
      "iteration 3469train_loss: 0.4268758143387763 test_loss: 0.4599407879200415\n",
      "iteration 3470train_loss: 0.4268758124942216 test_loss: 0.4599411134996465\n",
      "iteration 3471train_loss: 0.42687581065188396 test_loss: 0.459941438885989\n",
      "iteration 3472train_loss: 0.42687580881176074 test_loss: 0.45994176407918247\n",
      "iteration 3473train_loss: 0.4268758069738493 test_loss: 0.45994208907934037\n",
      "iteration 3474train_loss: 0.42687580513814694 test_loss: 0.4599424138865761\n",
      "iteration 3475train_loss: 0.42687580330465086 test_loss: 0.45994273850100303\n",
      "iteration 3476train_loss: 0.4268758014733587 test_loss: 0.45994306292273457\n",
      "iteration 3477train_loss: 0.42687579964426753 test_loss: 0.4599433871518838\n",
      "iteration 3478train_loss: 0.4268757978173747 test_loss: 0.45994371118856386\n",
      "iteration 3479train_loss: 0.42687579599267783 test_loss: 0.45994403503288783\n",
      "iteration 3480train_loss: 0.42687579417017396 test_loss: 0.4599443586849687\n",
      "iteration 3481train_loss: 0.42687579234986067 test_loss: 0.45994468214491946\n",
      "iteration 3482train_loss: 0.4268757905317352 test_loss: 0.4599450054128532\n",
      "iteration 3483train_loss: 0.4268757887157949 test_loss: 0.45994532848888237\n",
      "iteration 3484train_loss: 0.4268757869020372 test_loss: 0.4599456513731201\n",
      "iteration 3485train_loss: 0.4268757850904594 test_loss: 0.4599459740656787\n",
      "iteration 3486train_loss: 0.42687578328105896 test_loss: 0.4599462965666712\n",
      "iteration 3487train_loss: 0.4268757814738332 test_loss: 0.45994661887620997\n",
      "iteration 3488train_loss: 0.4268757796687795 test_loss: 0.4599469409944074\n",
      "iteration 3489train_loss: 0.4268757778658951 test_loss: 0.4599472629213761\n",
      "iteration 3490train_loss: 0.4268757760651776 test_loss: 0.4599475846572285\n",
      "iteration 3491train_loss: 0.4268757742666244 test_loss: 0.4599479062020767\n",
      "iteration 3492train_loss: 0.42687577247023273 test_loss: 0.4599482275560331\n",
      "iteration 3493train_loss: 0.4268757706760001 test_loss: 0.45994854871920987\n",
      "iteration 3494train_loss: 0.4268757688839238 test_loss: 0.45994886969171905\n",
      "iteration 3495train_loss: 0.42687576709400116 test_loss: 0.4599491904736728\n",
      "iteration 3496train_loss: 0.42687576530622984 test_loss: 0.459949511065183\n",
      "iteration 3497train_loss: 0.4268757635206072 test_loss: 0.45994983146636154\n",
      "iteration 3498train_loss: 0.4268757617371305 test_loss: 0.45995015167732056\n",
      "iteration 3499train_loss: 0.42687575995579713 test_loss: 0.45995047169817166\n",
      "iteration 3500train_loss: 0.4268757581766046 test_loss: 0.4599507915290265\n",
      "iteration 3501train_loss: 0.42687575639955033 test_loss: 0.45995111116999693\n",
      "iteration 3502train_loss: 0.42687575462463173 test_loss: 0.4599514306211944\n",
      "iteration 3503train_loss: 0.42687575285184626 test_loss: 0.4599517498827306\n",
      "iteration 3504train_loss: 0.4268757510811913 test_loss: 0.4599520689547168\n",
      "iteration 3505train_loss: 0.42687574931266425 test_loss: 0.4599523878372648\n",
      "iteration 3506train_loss: 0.42687574754626256 test_loss: 0.45995270653048553\n",
      "iteration 3507train_loss: 0.42687574578198373 test_loss: 0.4599530250344905\n",
      "iteration 3508train_loss: 0.42687574401982514 test_loss: 0.45995334334939086\n",
      "iteration 3509train_loss: 0.4268757422597843 test_loss: 0.45995366147529776\n",
      "iteration 3510train_loss: 0.4268757405018586 test_loss: 0.45995397941232236\n",
      "iteration 3511train_loss: 0.4268757387460455 test_loss: 0.4599542971605758\n",
      "iteration 3512train_loss: 0.42687573699234227 test_loss: 0.4599546147201688\n",
      "iteration 3513train_loss: 0.4268757352407467 test_loss: 0.45995493209121247\n",
      "iteration 3514train_loss: 0.42687573349125607 test_loss: 0.45995524927381765\n",
      "iteration 3515train_loss: 0.42687573174386784 test_loss: 0.4599555662680948\n",
      "iteration 3516train_loss: 0.42687572999857953 test_loss: 0.45995588307415514\n",
      "iteration 3517train_loss: 0.4268757282553886 test_loss: 0.459956199692109\n",
      "iteration 3518train_loss: 0.4268757265142924 test_loss: 0.459956516122067\n",
      "iteration 3519train_loss: 0.4268757247752886 test_loss: 0.45995683236413987\n",
      "iteration 3520train_loss: 0.42687572303837457 test_loss: 0.4599571484184377\n",
      "iteration 3521train_loss: 0.42687572130354773 test_loss: 0.4599574642850713\n",
      "iteration 3522train_loss: 0.4268757195708057 test_loss: 0.4599577799641507\n",
      "iteration 3523train_loss: 0.42687571784014583 test_loss: 0.4599580954557864\n",
      "iteration 3524train_loss: 0.4268757161115657 test_loss: 0.4599584107600884\n",
      "iteration 3525train_loss: 0.4268757143850628 test_loss: 0.4599587258771671\n",
      "iteration 3526train_loss: 0.4268757126606345 test_loss: 0.4599590408071324\n",
      "iteration 3527train_loss: 0.4268757109382786 test_loss: 0.4599593555500944\n",
      "iteration 3528train_loss: 0.4268757092179922 test_loss: 0.4599596701061629\n",
      "iteration 3529train_loss: 0.42687570749977305 test_loss: 0.45995998447544817\n",
      "iteration 3530train_loss: 0.4268757057836187 test_loss: 0.4599602986580596\n",
      "iteration 3531train_loss: 0.4268757040695266 test_loss: 0.4599606126541073\n",
      "iteration 3532train_loss: 0.42687570235749395 test_loss: 0.4599609264637008\n",
      "iteration 3533train_loss: 0.42687570064751884 test_loss: 0.45996124008694983\n",
      "iteration 3534train_loss: 0.42687569893959837 test_loss: 0.459961553523964\n",
      "iteration 3535train_loss: 0.42687569723373026 test_loss: 0.4599618667748526\n",
      "iteration 3536train_loss: 0.426875695529912 test_loss: 0.4599621798397254\n",
      "iteration 3537train_loss: 0.42687569382814106 test_loss: 0.45996249271869155\n",
      "iteration 3538train_loss: 0.42687569212841486 test_loss: 0.45996280541186074\n",
      "iteration 3539train_loss: 0.42687569043073126 test_loss: 0.45996311791934175\n",
      "iteration 3540train_loss: 0.4268756887350875 test_loss: 0.4599634302412441\n",
      "iteration 3541train_loss: 0.4268756870414814 test_loss: 0.45996374237767695\n",
      "iteration 3542train_loss: 0.4268756853499101 test_loss: 0.45996405432874926\n",
      "iteration 3543train_loss: 0.4268756836603715 test_loss: 0.4599643660945701\n",
      "iteration 3544train_loss: 0.4268756819728629 test_loss: 0.45996467767524835\n",
      "iteration 3545train_loss: 0.4268756802873821 test_loss: 0.4599649890708931\n",
      "iteration 3546train_loss: 0.42687567860392656 test_loss: 0.45996530028161303\n",
      "iteration 3547train_loss: 0.4268756769224938 test_loss: 0.4599656113075169\n",
      "iteration 3548train_loss: 0.4268756752430814 test_loss: 0.4599659221487135\n",
      "iteration 3549train_loss: 0.4268756735656869 test_loss: 0.45996623280531146\n",
      "iteration 3550train_loss: 0.42687567189030784 test_loss: 0.4599665432774194\n",
      "iteration 3551train_loss: 0.4268756702169419 test_loss: 0.4599668535651459\n",
      "iteration 3552train_loss: 0.4268756685455865 test_loss: 0.45996716366859913\n",
      "iteration 3553train_loss: 0.42687566687623946 test_loss: 0.45996747358788775\n",
      "iteration 3554train_loss: 0.4268756652088981 test_loss: 0.45996778332311994\n",
      "iteration 3555train_loss: 0.4268756635435601 test_loss: 0.45996809287440416\n",
      "iteration 3556train_loss: 0.42687566188022313 test_loss: 0.45996840224184854\n",
      "iteration 3557train_loss: 0.4268756602188846 test_loss: 0.4599687114255611\n",
      "iteration 3558train_loss: 0.4268756585595423 test_loss: 0.45996902042565013\n",
      "iteration 3559train_loss: 0.42687565690219365 test_loss: 0.4599693292422235\n",
      "iteration 3560train_loss: 0.4268756552468364 test_loss: 0.4599696378753894\n",
      "iteration 3561train_loss: 0.4268756535934679 test_loss: 0.4599699463252555\n",
      "iteration 3562train_loss: 0.426875651942086 test_loss: 0.4599702545919297\n",
      "iteration 3563train_loss: 0.42687565029268826 test_loss: 0.45997056267551983\n",
      "iteration 3564train_loss: 0.4268756486452723 test_loss: 0.4599708705761336\n",
      "iteration 3565train_loss: 0.4268756469998356 test_loss: 0.4599711782938786\n",
      "iteration 3566train_loss: 0.42687564535637584 test_loss: 0.45997148582886255\n",
      "iteration 3567train_loss: 0.4268756437148907 test_loss: 0.4599717931811928\n",
      "iteration 3568train_loss: 0.4268756420753777 test_loss: 0.4599721003509771\n",
      "iteration 3569train_loss: 0.4268756404378346 test_loss: 0.4599724073383225\n",
      "iteration 3570train_loss: 0.42687563880225904 test_loss: 0.4599727141433366\n",
      "iteration 3571train_loss: 0.42687563716864835 test_loss: 0.45997302076612673\n",
      "iteration 3572train_loss: 0.4268756355370004 test_loss: 0.45997332720679984\n",
      "iteration 3573train_loss: 0.4268756339073128 test_loss: 0.45997363346546344\n",
      "iteration 3574train_loss: 0.4268756322795833 test_loss: 0.45997393954222443\n",
      "iteration 3575train_loss: 0.42687563065380923 test_loss: 0.4599742454371898\n",
      "iteration 3576train_loss: 0.42687562902998843 test_loss: 0.4599745511504667\n",
      "iteration 3577train_loss: 0.42687562740811863 test_loss: 0.45997485668216187\n",
      "iteration 3578train_loss: 0.4268756257881973 test_loss: 0.45997516203238237\n",
      "iteration 3579train_loss: 0.42687562417022223 test_loss: 0.45997546720123483\n",
      "iteration 3580train_loss: 0.42687562255419104 test_loss: 0.45997577218882607\n",
      "iteration 3581train_loss: 0.42687562094010123 test_loss: 0.4599760769952627\n",
      "iteration 3582train_loss: 0.42687561932795065 test_loss: 0.45997638162065135\n",
      "iteration 3583train_loss: 0.42687561771773697 test_loss: 0.4599766860650987\n",
      "iteration 3584train_loss: 0.42687561610945773 test_loss: 0.45997699032871103\n",
      "iteration 3585train_loss: 0.4268756145031106 test_loss: 0.45997729441159496\n",
      "iteration 3586train_loss: 0.4268756128986934 test_loss: 0.4599775983138567\n",
      "iteration 3587train_loss: 0.42687561129620366 test_loss: 0.4599779020356027\n",
      "iteration 3588train_loss: 0.42687560969563915 test_loss: 0.4599782055769391\n",
      "iteration 3589train_loss: 0.42687560809699754 test_loss: 0.45997850893797215\n",
      "iteration 3590train_loss: 0.4268756065002764 test_loss: 0.45997881211880787\n",
      "iteration 3591train_loss: 0.42687560490547344 test_loss: 0.45997911511955253\n",
      "iteration 3592train_loss: 0.4268756033125865 test_loss: 0.45997941794031194\n",
      "iteration 3593train_loss: 0.42687560172161315 test_loss: 0.45997972058119213\n",
      "iteration 3594train_loss: 0.42687560013255105 test_loss: 0.45998002304229896\n",
      "iteration 3595train_loss: 0.42687559854539797 test_loss: 0.4599803253237382\n",
      "iteration 3596train_loss: 0.4268755969601517 test_loss: 0.45998062742561574\n",
      "iteration 3597train_loss: 0.4268755953768097 test_loss: 0.45998092934803725\n",
      "iteration 3598train_loss: 0.42687559379536993 test_loss: 0.4599812310911082\n",
      "iteration 3599train_loss: 0.4268755922158299 test_loss: 0.4599815326549344\n",
      "iteration 3600train_loss: 0.42687559063818736 test_loss: 0.45998183403962134\n",
      "iteration 3601train_loss: 0.42687558906244005 test_loss: 0.4599821352452742\n",
      "iteration 3602train_loss: 0.42687558748858584 test_loss: 0.45998243627199875\n",
      "iteration 3603train_loss: 0.42687558591662217 test_loss: 0.45998273711990006\n",
      "iteration 3604train_loss: 0.4268755843465469 test_loss: 0.4599830377890836\n",
      "iteration 3605train_loss: 0.42687558277835785 test_loss: 0.4599833382796544\n",
      "iteration 3606train_loss: 0.4268755812120525 test_loss: 0.45998363859171776\n",
      "iteration 3607train_loss: 0.42687557964762884 test_loss: 0.45998393872537874\n",
      "iteration 3608train_loss: 0.42687557808508436 test_loss: 0.45998423868074223\n",
      "iteration 3609train_loss: 0.426875576524417 test_loss: 0.4599845384579135\n",
      "iteration 3610train_loss: 0.4268755749656244 test_loss: 0.45998483805699725\n",
      "iteration 3611train_loss: 0.42687557340870436 test_loss: 0.45998513747809827\n",
      "iteration 3612train_loss: 0.4268755718536546 test_loss: 0.4599854367213216\n",
      "iteration 3613train_loss: 0.42687557030047274 test_loss: 0.4599857357867719\n",
      "iteration 3614train_loss: 0.4268755687491567 test_loss: 0.4599860346745537\n",
      "iteration 3615train_loss: 0.42687556719970415 test_loss: 0.4599863333847716\n",
      "iteration 3616train_loss: 0.42687556565211293 test_loss: 0.4599866319175304\n",
      "iteration 3617train_loss: 0.4268755641063807 test_loss: 0.4599869302729343\n",
      "iteration 3618train_loss: 0.42687556256250514 test_loss: 0.4599872284510881\n",
      "iteration 3619train_loss: 0.42687556102048424 test_loss: 0.45998752645209584\n",
      "iteration 3620train_loss: 0.42687555948031564 test_loss: 0.45998782427606183\n",
      "iteration 3621train_loss: 0.42687555794199705 test_loss: 0.45998812192309063\n",
      "iteration 3622train_loss: 0.4268755564055264 test_loss: 0.4599884193932862\n",
      "iteration 3623train_loss: 0.42687555487090123 test_loss: 0.45998871668675273\n",
      "iteration 3624train_loss: 0.42687555333811966 test_loss: 0.4599890138035943\n",
      "iteration 3625train_loss: 0.4268755518071791 test_loss: 0.4599893107439149\n",
      "iteration 3626train_loss: 0.42687555027807744 test_loss: 0.45998960750781853\n",
      "iteration 3627train_loss: 0.4268755487508127 test_loss: 0.4599899040954091\n",
      "iteration 3628train_loss: 0.4268755472253824 test_loss: 0.4599902005067904\n",
      "iteration 3629train_loss: 0.42687554570178443 test_loss: 0.4599904967420661\n",
      "iteration 3630train_loss: 0.4268755441800165 test_loss: 0.45999079280134014\n",
      "iteration 3631train_loss: 0.4268755426600765 test_loss: 0.45999108868471605\n",
      "iteration 3632train_loss: 0.4268755411419622 test_loss: 0.4599913843922974\n",
      "iteration 3633train_loss: 0.4268755396256715 test_loss: 0.4599916799241878\n",
      "iteration 3634train_loss: 0.42687553811120194 test_loss: 0.4599919752804908\n",
      "iteration 3635train_loss: 0.4268755365985516 test_loss: 0.4599922704613096\n",
      "iteration 3636train_loss: 0.42687553508771814 test_loss: 0.45999256546674766\n",
      "iteration 3637train_loss: 0.42687553357869934 test_loss: 0.4599928602969084\n",
      "iteration 3638train_loss: 0.4268755320714931 test_loss: 0.4599931549518949\n",
      "iteration 3639train_loss: 0.4268755305660972 test_loss: 0.4599934494318105\n",
      "iteration 3640train_loss: 0.42687552906250953 test_loss: 0.45999374373675816\n",
      "iteration 3641train_loss: 0.42687552756072783 test_loss: 0.45999403786684107\n",
      "iteration 3642train_loss: 0.4268755260607499 test_loss: 0.45999433182216215\n",
      "iteration 3643train_loss: 0.4268755245625737 test_loss: 0.45999462560282445\n",
      "iteration 3644train_loss: 0.4268755230661969 test_loss: 0.45999491920893065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3645train_loss: 0.42687552157161746 test_loss: 0.45999521264058385\n",
      "iteration 3646train_loss: 0.42687552007883317 test_loss: 0.4599955058978866\n",
      "iteration 3647train_loss: 0.4268755185878418 test_loss: 0.4599957989809417\n",
      "iteration 3648train_loss: 0.4268755170986412 test_loss: 0.4599960918898518\n",
      "iteration 3649train_loss: 0.42687551561122933 test_loss: 0.4599963846247195\n",
      "iteration 3650train_loss: 0.4268755141256039 test_loss: 0.4599966771856473\n",
      "iteration 3651train_loss: 0.426875512641763 test_loss: 0.4599969695727377\n",
      "iteration 3652train_loss: 0.4268755111597041 test_loss: 0.4599972617860931\n",
      "iteration 3653train_loss: 0.4268755096794252 test_loss: 0.4599975538258158\n",
      "iteration 3654train_loss: 0.42687550820092446 test_loss: 0.45999784569200824\n",
      "iteration 3655train_loss: 0.42687550672419927 test_loss: 0.4599981373847725\n",
      "iteration 3656train_loss: 0.4268755052492479 test_loss: 0.45999842890421094\n",
      "iteration 3657train_loss: 0.4268755037760678 test_loss: 0.4599987202504255\n",
      "iteration 3658train_loss: 0.42687550230465715 test_loss: 0.4599990114235183\n",
      "iteration 3659train_loss: 0.4268755008350138 test_loss: 0.4599993024235914\n",
      "iteration 3660train_loss: 0.4268754993671354 test_loss: 0.45999959325074663\n",
      "iteration 3661train_loss: 0.42687549790102003 test_loss: 0.45999988390508606\n",
      "iteration 3662train_loss: 0.4268754964366655 test_loss: 0.4600001743867113\n",
      "iteration 3663train_loss: 0.42687549497406974 test_loss: 0.46000046469572425\n",
      "iteration 3664train_loss: 0.4268754935132305 test_loss: 0.4600007548322267\n",
      "iteration 3665train_loss: 0.4268754920541458 test_loss: 0.46000104479632015\n",
      "iteration 3666train_loss: 0.4268754905968134 test_loss: 0.46000133458810616\n",
      "iteration 3667train_loss: 0.42687548914123136 test_loss: 0.4600016242076864\n",
      "iteration 3668train_loss: 0.42687548768739736 test_loss: 0.46000191365516235\n",
      "iteration 3669train_loss: 0.42687548623530946 test_loss: 0.46000220293063526\n",
      "iteration 3670train_loss: 0.4268754847849656 test_loss: 0.4600024920342067\n",
      "iteration 3671train_loss: 0.42687548333636355 test_loss: 0.4600027809659778\n",
      "iteration 3672train_loss: 0.4268754818895011 test_loss: 0.46000306972604993\n",
      "iteration 3673train_loss: 0.42687548044437645 test_loss: 0.46000335831452416\n",
      "iteration 3674train_loss: 0.42687547900098727 test_loss: 0.4600036467315018\n",
      "iteration 3675train_loss: 0.4268754775593316 test_loss: 0.46000393497708375\n",
      "iteration 3676train_loss: 0.42687547611940724 test_loss: 0.4600042230513711\n",
      "iteration 3677train_loss: 0.4268754746812122 test_loss: 0.4600045109544647\n",
      "iteration 3678train_loss: 0.4268754732447443 test_loss: 0.4600047986864655\n",
      "iteration 3679train_loss: 0.4268754718100016 test_loss: 0.46000508624747444\n",
      "iteration 3680train_loss: 0.4268754703769819 test_loss: 0.46000537363759214\n",
      "iteration 3681train_loss: 0.4268754689456833 test_loss: 0.46000566085691946\n",
      "iteration 3682train_loss: 0.4268754675161035 test_loss: 0.4600059479055569\n",
      "iteration 3683train_loss: 0.4268754660882405 test_loss: 0.46000623478360525\n",
      "iteration 3684train_loss: 0.42687546466209225 test_loss: 0.46000652149116494\n",
      "iteration 3685train_loss: 0.42687546323765674 test_loss: 0.46000680802833643\n",
      "iteration 3686train_loss: 0.42687546181493186 test_loss: 0.46000709439522036\n",
      "iteration 3687train_loss: 0.42687546039391555 test_loss: 0.4600073805919168\n",
      "iteration 3688train_loss: 0.4268754589746058 test_loss: 0.46000766661852627\n",
      "iteration 3689train_loss: 0.4268754575570004 test_loss: 0.46000795247514903\n",
      "iteration 3690train_loss: 0.4268754561410974 test_loss: 0.4600082381618852\n",
      "iteration 3691train_loss: 0.42687545472689475 test_loss: 0.46000852367883494\n",
      "iteration 3692train_loss: 0.42687545331439053 test_loss: 0.4600088090260983\n",
      "iteration 3693train_loss: 0.4268754519035824 test_loss: 0.4600090942037754\n",
      "iteration 3694train_loss: 0.4268754504944686 test_loss: 0.46000937921196616\n",
      "iteration 3695train_loss: 0.42687544908704694 test_loss: 0.4600096640507705\n",
      "iteration 3696train_loss: 0.4268754476813154 test_loss: 0.4600099487202882\n",
      "iteration 3697train_loss: 0.426875446277272 test_loss: 0.4600102332206192\n",
      "iteration 3698train_loss: 0.4268754448749146 test_loss: 0.4600105175518632\n",
      "iteration 3699train_loss: 0.4268754434742413 test_loss: 0.4600108017141197\n",
      "iteration 3700train_loss: 0.4268754420752499 test_loss: 0.46001108570748855\n",
      "iteration 3701train_loss: 0.42687544067793864 test_loss: 0.46001136953206934\n",
      "iteration 3702train_loss: 0.4268754392823052 test_loss: 0.46001165318796144\n",
      "iteration 3703train_loss: 0.4268754378883478 test_loss: 0.4600119366752642\n",
      "iteration 3704train_loss: 0.42687543649606424 test_loss: 0.4600122199940774\n",
      "iteration 3705train_loss: 0.4268754351054526 test_loss: 0.46001250314449993\n",
      "iteration 3706train_loss: 0.4268754337165108 test_loss: 0.4600127861266313\n",
      "iteration 3707train_loss: 0.4268754323292369 test_loss: 0.4600130689405708\n",
      "iteration 3708train_loss: 0.4268754309436289 test_loss: 0.46001335158641754\n",
      "iteration 3709train_loss: 0.4268754295596847 test_loss: 0.4600136340642705\n",
      "iteration 3710train_loss: 0.4268754281774024 test_loss: 0.4600139163742289\n",
      "iteration 3711train_loss: 0.42687542679678 test_loss: 0.46001419851639164\n",
      "iteration 3712train_loss: 0.42687542541781537 test_loss: 0.4600144804908577\n",
      "iteration 3713train_loss: 0.42687542404050666 test_loss: 0.46001476229772603\n",
      "iteration 3714train_loss: 0.42687542266485173 test_loss: 0.4600150439370952\n",
      "iteration 3715train_loss: 0.42687542129084877 test_loss: 0.46001532540906426\n",
      "iteration 3716train_loss: 0.42687541991849565 test_loss: 0.4600156067137319\n",
      "iteration 3717train_loss: 0.42687541854779043 test_loss: 0.46001588785119657\n",
      "iteration 3718train_loss: 0.42687541717873106 test_loss: 0.46001616882155705\n",
      "iteration 3719train_loss: 0.4268754158113157 test_loss: 0.46001644962491184\n",
      "iteration 3720train_loss: 0.4268754144455423 test_loss: 0.4600167302613593\n",
      "iteration 3721train_loss: 0.42687541308140875 test_loss: 0.46001701073099793\n",
      "iteration 3722train_loss: 0.42687541171891324 test_loss: 0.4600172910339263\n",
      "iteration 3723train_loss: 0.4268754103580539 test_loss: 0.4600175711702424\n",
      "iteration 3724train_loss: 0.42687540899882837 test_loss: 0.46001785114004484\n",
      "iteration 3725train_loss: 0.42687540764123505 test_loss: 0.4600181309434314\n",
      "iteration 3726train_loss: 0.4268754062852718 test_loss: 0.46001841058050064\n",
      "iteration 3727train_loss: 0.42687540493093673 test_loss: 0.46001869005135027\n",
      "iteration 3728train_loss: 0.42687540357822795 test_loss: 0.4600189693560786\n",
      "iteration 3729train_loss: 0.4268754022271433 test_loss: 0.46001924849478343\n",
      "iteration 3730train_loss: 0.426875400877681 test_loss: 0.46001952746756286\n",
      "iteration 3731train_loss: 0.4268753995298389 test_loss: 0.46001980627451466\n",
      "iteration 3732train_loss: 0.42687539818361525 test_loss: 0.4600200849157365\n",
      "iteration 3733train_loss: 0.4268753968390081 test_loss: 0.46002036339132635\n",
      "iteration 3734train_loss: 0.4268753954960153 test_loss: 0.4600206417013818\n",
      "iteration 3735train_loss: 0.42687539415463505 test_loss: 0.4600209198460004\n",
      "iteration 3736train_loss: 0.42687539281486536 test_loss: 0.46002119782527995\n",
      "iteration 3737train_loss: 0.4268753914767045 test_loss: 0.4600214756393177\n",
      "iteration 3738train_loss: 0.42687539014015025 test_loss: 0.46002175328821143\n",
      "iteration 3739train_loss: 0.4268753888052007 test_loss: 0.4600220307720583\n",
      "iteration 3740train_loss: 0.4268753874718541 test_loss: 0.4600223080909557\n",
      "iteration 3741train_loss: 0.4268753861401084 test_loss: 0.46002258524500106\n",
      "iteration 3742train_loss: 0.4268753848099616 test_loss: 0.46002286223429145\n",
      "iteration 3743train_loss: 0.42687538348141196 test_loss: 0.4600231390589241\n",
      "iteration 3744train_loss: 0.42687538215445753 test_loss: 0.46002341571899635\n",
      "iteration 3745train_loss: 0.4268753808290962 test_loss: 0.46002369221460504\n",
      "iteration 3746train_loss: 0.4268753795053261 test_loss: 0.46002396854584726\n",
      "iteration 3747train_loss: 0.42687537818314547 test_loss: 0.4600242447128199\n",
      "iteration 3748train_loss: 0.4268753768625524 test_loss: 0.46002452071562\n",
      "iteration 3749train_loss: 0.42687537554354477 test_loss: 0.46002479655434436\n",
      "iteration 3750train_loss: 0.4268753742261209 test_loss: 0.4600250722290898\n",
      "iteration 3751train_loss: 0.4268753729102786 test_loss: 0.46002534773995285\n",
      "iteration 3752train_loss: 0.4268753715960163 test_loss: 0.4600256230870305\n",
      "iteration 3753train_loss: 0.4268753702833319 test_loss: 0.46002589827041923\n",
      "iteration 3754train_loss: 0.42687536897222345 test_loss: 0.4600261732902156\n",
      "iteration 3755train_loss: 0.42687536766268924 test_loss: 0.4600264481465162\n",
      "iteration 3756train_loss: 0.4268753663547273 test_loss: 0.46002672283941737\n",
      "iteration 3757train_loss: 0.42687536504833556 test_loss: 0.46002699736901564\n",
      "iteration 3758train_loss: 0.4268753637435123 test_loss: 0.4600272717354073\n",
      "iteration 3759train_loss: 0.4268753624402557 test_loss: 0.46002754593868866\n",
      "iteration 3760train_loss: 0.4268753611385637 test_loss: 0.460027819978956\n",
      "iteration 3761train_loss: 0.4268753598384346 test_loss: 0.46002809385630533\n",
      "iteration 3762train_loss: 0.4268753585398663 test_loss: 0.46002836757083293\n",
      "iteration 3763train_loss: 0.42687535724285713 test_loss: 0.46002864112263486\n",
      "iteration 3764train_loss: 0.4268753559474051 test_loss: 0.4600289145118071\n",
      "iteration 3765train_loss: 0.4268753546535084 test_loss: 0.46002918773844564\n",
      "iteration 3766train_loss: 0.42687535336116494 test_loss: 0.4600294608026464\n",
      "iteration 3767train_loss: 0.4268753520703732 test_loss: 0.4600297337045051\n",
      "iteration 3768train_loss: 0.42687535078113104 test_loss: 0.46003000644411773\n",
      "iteration 3769train_loss: 0.42687534949343675 test_loss: 0.46003027902157995\n",
      "iteration 3770train_loss: 0.4268753482072884 test_loss: 0.46003055143698734\n",
      "iteration 3771train_loss: 0.4268753469226841 test_loss: 0.4600308236904357\n",
      "iteration 3772train_loss: 0.42687534563962204 test_loss: 0.46003109578202045\n",
      "iteration 3773train_loss: 0.42687534435810026 test_loss: 0.46003136771183717\n",
      "iteration 3774train_loss: 0.42687534307811714 test_loss: 0.4600316394799814\n",
      "iteration 3775train_loss: 0.4268753417996706 test_loss: 0.4600319110865484\n",
      "iteration 3776train_loss: 0.42687534052275894 test_loss: 0.4600321825316337\n",
      "iteration 3777train_loss: 0.4268753392473802 test_loss: 0.4600324538153324\n",
      "iteration 3778train_loss: 0.4268753379735325 test_loss: 0.46003272493773995\n",
      "iteration 3779train_loss: 0.426875336701214 test_loss: 0.46003299589895136\n",
      "iteration 3780train_loss: 0.42687533543042316 test_loss: 0.46003326669906197\n",
      "iteration 3781train_loss: 0.4268753341611579 test_loss: 0.4600335373381666\n",
      "iteration 3782train_loss: 0.42687533289341617 test_loss: 0.46003380781636044\n",
      "iteration 3783train_loss: 0.42687533162719665 test_loss: 0.46003407813373837\n",
      "iteration 3784train_loss: 0.426875330362497 test_loss: 0.46003434829039536\n",
      "iteration 3785train_loss: 0.4268753290993157 test_loss: 0.4600346182864263\n",
      "iteration 3786train_loss: 0.4268753278376508 test_loss: 0.46003488812192583\n",
      "iteration 3787train_loss: 0.4268753265775005 test_loss: 0.46003515779698906\n",
      "iteration 3788train_loss: 0.426875325318863 test_loss: 0.46003542731171027\n",
      "iteration 3789train_loss: 0.4268753240617364 test_loss: 0.46003569666618427\n",
      "iteration 3790train_loss: 0.426875322806119 test_loss: 0.4600359658605057\n",
      "iteration 3791train_loss: 0.42687532155200897 test_loss: 0.4600362348947691\n",
      "iteration 3792train_loss: 0.42687532029940445 test_loss: 0.4600365037690688\n",
      "iteration 3793train_loss: 0.42687531904830345 test_loss: 0.46003677248349933\n",
      "iteration 3794train_loss: 0.4268753177987044 test_loss: 0.46003704103815507\n",
      "iteration 3795train_loss: 0.4268753165506055 test_loss: 0.46003730943313026\n",
      "iteration 3796train_loss: 0.42687531530400497 test_loss: 0.4600375776685192\n",
      "iteration 3797train_loss: 0.4268753140589007 test_loss: 0.4600378457444162\n",
      "iteration 3798train_loss: 0.42687531281529123 test_loss: 0.4600381136609151\n",
      "iteration 3799train_loss: 0.4268753115731745 test_loss: 0.4600383814181103\n",
      "iteration 3800train_loss: 0.426875310332549 test_loss: 0.4600386490160958\n",
      "iteration 3801train_loss: 0.42687530909341276 test_loss: 0.4600389164549655\n",
      "iteration 3802train_loss: 0.426875307855764 test_loss: 0.4600391837348133\n",
      "iteration 3803train_loss: 0.4268753066196008 test_loss: 0.46003945085573317\n",
      "iteration 3804train_loss: 0.42687530538492163 test_loss: 0.46003971781781877\n",
      "iteration 3805train_loss: 0.42687530415172453 test_loss: 0.4600399846211641\n",
      "iteration 3806train_loss: 0.4268753029200077 test_loss: 0.46004025126586273\n",
      "iteration 3807train_loss: 0.4268753016897695 test_loss: 0.46004051775200844\n",
      "iteration 3808train_loss: 0.426875300461008 test_loss: 0.4600407840796946\n",
      "iteration 3809train_loss: 0.4268752992337216 test_loss: 0.460041050249015\n",
      "iteration 3810train_loss: 0.42687529800790835 test_loss: 0.46004131626006306\n",
      "iteration 3811train_loss: 0.42687529678356656 test_loss: 0.4600415821129322\n",
      "iteration 3812train_loss: 0.42687529556069453 test_loss: 0.46004184780771584\n",
      "iteration 3813train_loss: 0.4268752943392902 test_loss: 0.46004211334450734\n",
      "iteration 3814train_loss: 0.4268752931193522 test_loss: 0.46004237872339987\n",
      "iteration 3815train_loss: 0.42687529190087836 test_loss: 0.4600426439444868\n",
      "iteration 3816train_loss: 0.42687529068386737 test_loss: 0.46004290900786116\n",
      "iteration 3817train_loss: 0.4268752894683171 test_loss: 0.4600431739136163\n",
      "iteration 3818train_loss: 0.4268752882542259 test_loss: 0.46004343866184494\n",
      "iteration 3819train_loss: 0.42687528704159217 test_loss: 0.46004370325264043\n",
      "iteration 3820train_loss: 0.4268752858304139 test_loss: 0.4600439676860955\n",
      "iteration 3821train_loss: 0.4268752846206894 test_loss: 0.4600442319623032\n",
      "iteration 3822train_loss: 0.42687528341241704 test_loss: 0.46004449608135634\n",
      "iteration 3823train_loss: 0.42687528220559506 test_loss: 0.4600447600433476\n",
      "iteration 3824train_loss: 0.4268752810002216 test_loss: 0.46004502384837\n",
      "iteration 3825train_loss: 0.426875279796295 test_loss: 0.4600452874965159\n",
      "iteration 3826train_loss: 0.4268752785938136 test_loss: 0.460045550987878\n",
      "iteration 3827train_loss: 0.4268752773927755 test_loss: 0.46004581432254926\n",
      "iteration 3828train_loss: 0.4268752761931789 test_loss: 0.4600460775006218\n",
      "iteration 3829train_loss: 0.42687527499502226 test_loss: 0.46004634052218807\n",
      "iteration 3830train_loss: 0.42687527379830387 test_loss: 0.4600466033873409\n",
      "iteration 3831train_loss: 0.4268752726030217 test_loss: 0.46004686609617224\n",
      "iteration 3832train_loss: 0.4268752714091743 test_loss: 0.4600471286487746\n",
      "iteration 3833train_loss: 0.42687527021675997 test_loss: 0.46004739104524034\n",
      "iteration 3834train_loss: 0.42687526902577677 test_loss: 0.4600476532856614\n",
      "iteration 3835train_loss: 0.4268752678362231 test_loss: 0.4600479153701301\n",
      "iteration 3836train_loss: 0.4268752666480973 test_loss: 0.46004817729873854\n",
      "iteration 3837train_loss: 0.4268752654613976 test_loss: 0.4600484390715787\n",
      "iteration 3838train_loss: 0.42687526427612216 test_loss: 0.46004870068874265\n",
      "iteration 3839train_loss: 0.42687526309226936 test_loss: 0.4600489621503223\n",
      "iteration 3840train_loss: 0.42687526190983754 test_loss: 0.4600492234564096\n",
      "iteration 3841train_loss: 0.4268752607288249 test_loss: 0.4600494846070964\n",
      "iteration 3842train_loss: 0.42687525954922995 test_loss: 0.4600497456024743\n",
      "iteration 3843train_loss: 0.4268752583710507 test_loss: 0.4600500064426351\n",
      "iteration 3844train_loss: 0.4268752571942855 test_loss: 0.46005026712767066\n",
      "iteration 3845train_loss: 0.4268752560189327 test_loss: 0.4600505276576724\n",
      "iteration 3846train_loss: 0.42687525484499067 test_loss: 0.46005078803273197\n",
      "iteration 3847train_loss: 0.42687525367245766 test_loss: 0.46005104825294096\n",
      "iteration 3848train_loss: 0.42687525250133196 test_loss: 0.46005130831839064\n",
      "iteration 3849train_loss: 0.4268752513316119 test_loss: 0.46005156822917254\n",
      "iteration 3850train_loss: 0.4268752501632957 test_loss: 0.46005182798537797\n",
      "iteration 3851train_loss: 0.4268752489963817 test_loss: 0.46005208758709837\n",
      "iteration 3852train_loss: 0.42687524783086833 test_loss: 0.460052347034425\n",
      "iteration 3853train_loss: 0.4268752466667537 test_loss: 0.4600526063274487\n",
      "iteration 3854train_loss: 0.4268752455040364 test_loss: 0.4600528654662611\n",
      "iteration 3855train_loss: 0.42687524434271457 test_loss: 0.4600531244509529\n",
      "iteration 3856train_loss: 0.4268752431827865 test_loss: 0.46005338328161555\n",
      "iteration 3857train_loss: 0.4268752420242506 test_loss: 0.46005364195833964\n",
      "iteration 3858train_loss: 0.42687524086710527 test_loss: 0.46005390048121636\n",
      "iteration 3859train_loss: 0.42687523971134855 test_loss: 0.4600541588503364\n",
      "iteration 3860train_loss: 0.42687523855697906 test_loss: 0.46005441706579087\n",
      "iteration 3861train_loss: 0.4268752374039951 test_loss: 0.4600546751276703\n",
      "iteration 3862train_loss: 0.42687523625239476 test_loss: 0.4600549330360654\n",
      "iteration 3863train_loss: 0.42687523510217656 test_loss: 0.4600551907910671\n",
      "iteration 3864train_loss: 0.4268752339533389 test_loss: 0.4600554483927658\n",
      "iteration 3865train_loss: 0.42687523280588 test_loss: 0.4600557058412521\n",
      "iteration 3866train_loss: 0.42687523165979824 test_loss: 0.46005596313661656\n",
      "iteration 3867train_loss: 0.42687523051509185 test_loss: 0.46005622027894955\n",
      "iteration 3868train_loss: 0.4268752293717594 test_loss: 0.4600564772683417\n",
      "iteration 3869train_loss: 0.426875228229799 test_loss: 0.4600567341048832\n",
      "iteration 3870train_loss: 0.4268752270892093 test_loss: 0.4600569907886644\n",
      "iteration 3871train_loss: 0.42687522594998845 test_loss: 0.4600572473197755\n",
      "iteration 3872train_loss: 0.42687522481213475 test_loss: 0.4600575036983067\n",
      "iteration 3873train_loss: 0.4268752236756466 test_loss: 0.4600577599243483\n",
      "iteration 3874train_loss: 0.42687522254052246 test_loss: 0.4600580159979902\n",
      "iteration 3875train_loss: 0.4268752214067605 test_loss: 0.4600582719193226\n",
      "iteration 3876train_loss: 0.4268752202743592 test_loss: 0.4600585276884355\n",
      "iteration 3877train_loss: 0.426875219143317 test_loss: 0.4600587833054187\n",
      "iteration 3878train_loss: 0.42687521801363215 test_loss: 0.46005903877036225\n",
      "iteration 3879train_loss: 0.426875216885303 test_loss: 0.4600592940833558\n",
      "iteration 3880train_loss: 0.4268752157583281 test_loss: 0.4600595492444892\n",
      "iteration 3881train_loss: 0.4268752146327056 test_loss: 0.4600598042538523\n",
      "iteration 3882train_loss: 0.4268752135084338 test_loss: 0.4600600591115346\n",
      "iteration 3883train_loss: 0.4268752123855115 test_loss: 0.460060313817626\n",
      "iteration 3884train_loss: 0.4268752112639367 test_loss: 0.4600605683722158\n",
      "iteration 3885train_loss: 0.42687521014370783 test_loss: 0.46006082277539373\n",
      "iteration 3886train_loss: 0.42687520902482334 test_loss: 0.460061077027249\n",
      "iteration 3887train_loss: 0.42687520790728173 test_loss: 0.46006133112787134\n",
      "iteration 3888train_loss: 0.42687520679108115 test_loss: 0.4600615850773499\n",
      "iteration 3889train_loss: 0.42687520567622 test_loss: 0.4600618388757741\n",
      "iteration 3890train_loss: 0.4268752045626969 test_loss: 0.4600620925232332\n",
      "iteration 3891train_loss: 0.42687520345051005 test_loss: 0.4600623460198163\n",
      "iteration 3892train_loss: 0.42687520233965787 test_loss: 0.46006259936561283\n",
      "iteration 3893train_loss: 0.4268752012301388 test_loss: 0.4600628525607117\n",
      "iteration 3894train_loss: 0.42687520012195124 test_loss: 0.46006310560520186\n",
      "iteration 3895train_loss: 0.4268751990150935 test_loss: 0.4600633584991727\n",
      "iteration 3896train_loss: 0.426875197909564 test_loss: 0.46006361124271267\n",
      "iteration 3897train_loss: 0.4268751968053613 test_loss: 0.4600638638359111\n",
      "iteration 3898train_loss: 0.4268751957024836 test_loss: 0.4600641162788565\n",
      "iteration 3899train_loss: 0.4268751946009293 test_loss: 0.46006436857163807\n",
      "iteration 3900train_loss: 0.42687519350069714 test_loss: 0.46006462071434423\n",
      "iteration 3901train_loss: 0.4268751924017851 test_loss: 0.4600648727070638\n",
      "iteration 3902train_loss: 0.42687519130419177 test_loss: 0.46006512454988546\n",
      "iteration 3903train_loss: 0.4268751902079157 test_loss: 0.46006537624289773\n",
      "iteration 3904train_loss: 0.4268751891129551 test_loss: 0.46006562778618915\n",
      "iteration 3905train_loss: 0.4268751880193085 test_loss: 0.46006587917984837\n",
      "iteration 3906train_loss: 0.4268751869269742 test_loss: 0.46006613042396366\n",
      "iteration 3907train_loss: 0.4268751858359507 test_loss: 0.4600663815186234\n",
      "iteration 3908train_loss: 0.42687518474623654 test_loss: 0.460066632463916\n",
      "iteration 3909train_loss: 0.42687518365782995 test_loss: 0.4600668832599298\n",
      "iteration 3910train_loss: 0.4268751825707294 test_loss: 0.46006713390675297\n",
      "iteration 3911train_loss: 0.4268751814849334 test_loss: 0.46006738440447376\n",
      "iteration 3912train_loss: 0.42687518040044037 test_loss: 0.46006763475318013\n",
      "iteration 3913train_loss: 0.4268751793172486 test_loss: 0.46006788495296036\n",
      "iteration 3914train_loss: 0.42687517823535676 test_loss: 0.4600681350039024\n",
      "iteration 3915train_loss: 0.4268751771547631 test_loss: 0.4600683849060943\n",
      "iteration 3916train_loss: 0.4268751760754662 test_loss: 0.4600686346596237\n",
      "iteration 3917train_loss: 0.4268751749974643 test_loss: 0.46006888426457887\n",
      "iteration 3918train_loss: 0.42687517392075597 test_loss: 0.46006913372104746\n",
      "iteration 3919train_loss: 0.4268751728453397 test_loss: 0.46006938302911726\n",
      "iteration 3920train_loss: 0.42687517177121376 test_loss: 0.460069632188876\n",
      "iteration 3921train_loss: 0.4268751706983768 test_loss: 0.46006988120041137\n",
      "iteration 3922train_loss: 0.42687516962682726 test_loss: 0.4600701300638109\n",
      "iteration 3923train_loss: 0.4268751685565633 test_loss: 0.4600703787791623\n",
      "iteration 3924train_loss: 0.42687516748758386 test_loss: 0.460070627346553\n",
      "iteration 3925train_loss: 0.4268751664198869 test_loss: 0.46007087576607053\n",
      "iteration 3926train_loss: 0.42687516535347125 test_loss: 0.4600711240378023\n",
      "iteration 3927train_loss: 0.42687516428833516 test_loss: 0.46007137216183563\n",
      "iteration 3928train_loss: 0.4268751632244771 test_loss: 0.46007162013825786\n",
      "iteration 3929train_loss: 0.42687516216189564 test_loss: 0.46007186796715627\n",
      "iteration 3930train_loss: 0.42687516110058926 test_loss: 0.46007211564861816\n",
      "iteration 3931train_loss: 0.4268751600405562 test_loss: 0.4600723631827306\n",
      "iteration 3932train_loss: 0.42687515898179523 test_loss: 0.46007261056958076\n",
      "iteration 3933train_loss: 0.4268751579243045 test_loss: 0.4600728578092556\n",
      "iteration 3934train_loss: 0.42687515686808286 test_loss: 0.4600731049018422\n",
      "iteration 3935train_loss: 0.4268751558131283 test_loss: 0.4600733518474276\n",
      "iteration 3936train_loss: 0.4268751547594398 test_loss: 0.46007359864609865\n",
      "iteration 3937train_loss: 0.42687515370701556 test_loss: 0.46007384529794226\n",
      "iteration 3938train_loss: 0.42687515265585413 test_loss: 0.4600740918030451\n",
      "iteration 3939train_loss: 0.4268751516059538 test_loss: 0.46007433816149407\n",
      "iteration 3940train_loss: 0.42687515055731334 test_loss: 0.46007458437337584\n",
      "iteration 3941train_loss: 0.4268751495099311 test_loss: 0.4600748304387772\n",
      "iteration 3942train_loss: 0.42687514846380564 test_loss: 0.4600750763577845\n",
      "iteration 3943train_loss: 0.4268751474189353 test_loss: 0.46007532213048447\n",
      "iteration 3944train_loss: 0.42687514637531876 test_loss: 0.4600755677569636\n",
      "iteration 3945train_loss: 0.42687514533295434 test_loss: 0.46007581323730845\n",
      "iteration 3946train_loss: 0.4268751442918406 test_loss: 0.4600760585716053\n",
      "iteration 3947train_loss: 0.426875143251976 test_loss: 0.46007630375994046\n",
      "iteration 3948train_loss: 0.4268751422133592 test_loss: 0.4600765488024004\n",
      "iteration 3949train_loss: 0.42687514117598846 test_loss: 0.4600767936990713\n",
      "iteration 3950train_loss: 0.4268751401398625 test_loss: 0.4600770384500394\n",
      "iteration 3951train_loss: 0.4268751391049796 test_loss: 0.4600772830553908\n",
      "iteration 3952train_loss: 0.42687513807133853 test_loss: 0.46007752751521164\n",
      "iteration 3953train_loss: 0.4268751370389376 test_loss: 0.4600777718295881\n",
      "iteration 3954train_loss: 0.4268751360077753 test_loss: 0.460078015998606\n",
      "iteration 3955train_loss: 0.42687513497785023 test_loss: 0.46007826002235136\n",
      "iteration 3956train_loss: 0.4268751339491609 test_loss: 0.4600785039009102\n",
      "iteration 3957train_loss: 0.42687513292170576 test_loss: 0.4600787476343682\n",
      "iteration 3958train_loss: 0.42687513189548343 test_loss: 0.46007899122281143\n",
      "iteration 3959train_loss: 0.4268751308704923 test_loss: 0.46007923466632533\n",
      "iteration 3960train_loss: 0.42687512984673104 test_loss: 0.4600794779649959\n",
      "iteration 3961train_loss: 0.4268751288241981 test_loss: 0.46007972111890855\n",
      "iteration 3962train_loss: 0.42687512780289194 test_loss: 0.46007996412814917\n",
      "iteration 3963train_loss: 0.42687512678281114 test_loss: 0.4600802069928031\n",
      "iteration 3964train_loss: 0.4268751257639542 test_loss: 0.46008044971295603\n",
      "iteration 3965train_loss: 0.42687512474631967 test_loss: 0.46008069228869314\n",
      "iteration 3966train_loss: 0.42687512372990605 test_loss: 0.4600809347201001\n",
      "iteration 3967train_loss: 0.42687512271471206 test_loss: 0.46008117700726225\n",
      "iteration 3968train_loss: 0.4268751217007359 test_loss: 0.46008141915026485\n",
      "iteration 3969train_loss: 0.42687512068797623 test_loss: 0.4600816611491931\n",
      "iteration 3970train_loss: 0.42687511967643177 test_loss: 0.4600819030041323\n",
      "iteration 3971train_loss: 0.4268751186661009 test_loss: 0.4600821447151676\n",
      "iteration 3972train_loss: 0.4268751176569821 test_loss: 0.4600823862823842\n",
      "iteration 3973train_loss: 0.4268751166490741 test_loss: 0.46008262770586694\n",
      "iteration 3974train_loss: 0.42687511564237524 test_loss: 0.4600828689857011\n",
      "iteration 3975train_loss: 0.4268751146368843 test_loss: 0.4600831101219714\n",
      "iteration 3976train_loss: 0.42687511363259956 test_loss: 0.460083351114763\n",
      "iteration 3977train_loss: 0.42687511262951977 test_loss: 0.4600835919641606\n",
      "iteration 3978train_loss: 0.42687511162764336 test_loss: 0.4600838326702491\n",
      "iteration 3979train_loss: 0.4268751106269689 test_loss: 0.46008407323311323\n",
      "iteration 3980train_loss: 0.4268751096274951 test_loss: 0.46008431365283775\n",
      "iteration 3981train_loss: 0.42687510862922035 test_loss: 0.46008455392950726\n",
      "iteration 3982train_loss: 0.4268751076321432 test_loss: 0.46008479406320646\n",
      "iteration 3983train_loss: 0.42687510663626227 test_loss: 0.46008503405402\n",
      "iteration 3984train_loss: 0.42687510564157616 test_loss: 0.4600852739020322\n",
      "iteration 3985train_loss: 0.4268751046480833 test_loss: 0.4600855136073276\n",
      "iteration 3986train_loss: 0.42687510365578235 test_loss: 0.4600857531699907\n",
      "iteration 3987train_loss: 0.426875102664672 test_loss: 0.46008599259010585\n",
      "iteration 3988train_loss: 0.4268751016747505 test_loss: 0.46008623186775743\n",
      "iteration 3989train_loss: 0.42687510068601675 test_loss: 0.4600864710030295\n",
      "iteration 3990train_loss: 0.4268750996984691 test_loss: 0.4600867099960066\n",
      "iteration 3991train_loss: 0.4268750987121062 test_loss: 0.46008694884677265\n",
      "iteration 3992train_loss: 0.4268750977269267 test_loss: 0.46008718755541195\n",
      "iteration 3993train_loss: 0.426875096742929 test_loss: 0.4600874261220085\n",
      "iteration 3994train_loss: 0.42687509576011184 test_loss: 0.46008766454664635\n",
      "iteration 3995train_loss: 0.42687509477847374 test_loss: 0.4600879028294095\n",
      "iteration 3996train_loss: 0.42687509379801325 test_loss: 0.46008814097038186\n",
      "iteration 3997train_loss: 0.426875092818729 test_loss: 0.4600883789696474\n",
      "iteration 3998train_loss: 0.4268750918406195 test_loss: 0.46008861682728985\n",
      "iteration 3999train_loss: 0.4268750908636835 test_loss: 0.460088854543393\n",
      "iteration 4000train_loss: 0.42687508988791945 test_loss: 0.4600890921180408\n",
      "iteration 4001train_loss: 0.426875088913326 test_loss: 0.4600893295513166\n",
      "iteration 4002train_loss: 0.4268750879399017 test_loss: 0.4600895668433044\n",
      "iteration 4003train_loss: 0.4268750869676451 test_loss: 0.46008980399408744\n",
      "iteration 4004train_loss: 0.42687508599655494 test_loss: 0.46009004100374956\n",
      "iteration 4005train_loss: 0.42687508502662963 test_loss: 0.46009027787237416\n",
      "iteration 4006train_loss: 0.4268750840578679 test_loss: 0.4600905146000446\n",
      "iteration 4007train_loss: 0.42687508309026845 test_loss: 0.4600907511868444\n",
      "iteration 4008train_loss: 0.4268750821238297 test_loss: 0.46009098763285694\n",
      "iteration 4009train_loss: 0.42687508115855033 test_loss: 0.4600912239381653\n",
      "iteration 4010train_loss: 0.42687508019442894 test_loss: 0.460091460102853\n",
      "iteration 4011train_loss: 0.426875079231464 test_loss: 0.4600916961270031\n",
      "iteration 4012train_loss: 0.42687507826965426 test_loss: 0.4600919320106988\n",
      "iteration 4013train_loss: 0.4268750773089984 test_loss: 0.4600921677540231\n",
      "iteration 4014train_loss: 0.426875076349495 test_loss: 0.46009240335705937\n",
      "iteration 4015train_loss: 0.4268750753911425 test_loss: 0.46009263881989027\n",
      "iteration 4016train_loss: 0.4268750744339397 test_loss: 0.460092874142599\n",
      "iteration 4017train_loss: 0.4268750734778851 test_loss: 0.4600931093252683\n",
      "iteration 4018train_loss: 0.4268750725229774 test_loss: 0.46009334436798127\n",
      "iteration 4019train_loss: 0.4268750715692153 test_loss: 0.46009357927082056\n",
      "iteration 4020train_loss: 0.42687507061659713 test_loss: 0.460093814033869\n",
      "iteration 4021train_loss: 0.4268750696651218 test_loss: 0.4600940486572092\n",
      "iteration 4022train_loss: 0.42687506871478786 test_loss: 0.460094283140924\n",
      "iteration 4023train_loss: 0.42687506776559386 test_loss: 0.4600945174850959\n",
      "iteration 4024train_loss: 0.4268750668175385 test_loss: 0.46009475168980757\n",
      "iteration 4025train_loss: 0.4268750658706204 test_loss: 0.4600949857551415\n",
      "iteration 4026train_loss: 0.42687506492483823 test_loss: 0.4600952196811801\n",
      "iteration 4027train_loss: 0.4268750639801906 test_loss: 0.460095453468006\n",
      "iteration 4028train_loss: 0.42687506303667605 test_loss: 0.46009568711570153\n",
      "iteration 4029train_loss: 0.42687506209429327 test_loss: 0.4600959206243489\n",
      "iteration 4030train_loss: 0.4268750611530411 test_loss: 0.46009615399403037\n",
      "iteration 4031train_loss: 0.4268750602129177 test_loss: 0.46009638722482843\n",
      "iteration 4032train_loss: 0.4268750592739222 test_loss: 0.46009662031682513\n",
      "iteration 4033train_loss: 0.42687505833605316 test_loss: 0.46009685327010263\n",
      "iteration 4034train_loss: 0.42687505739930903 test_loss: 0.4600970860847429\n",
      "iteration 4035train_loss: 0.4268750564636885 test_loss: 0.46009731876082827\n",
      "iteration 4036train_loss: 0.42687505552919025 test_loss: 0.46009755129844054\n",
      "iteration 4037train_loss: 0.4268750545958131 test_loss: 0.4600977836976617\n",
      "iteration 4038train_loss: 0.42687505366355544 test_loss: 0.4600980159585737\n",
      "iteration 4039train_loss: 0.42687505273241605 test_loss: 0.4600982480812584\n",
      "iteration 4040train_loss: 0.4268750518023936 test_loss: 0.46009848006579757\n",
      "iteration 4041train_loss: 0.4268750508734867 test_loss: 0.46009871191227303\n",
      "iteration 4042train_loss: 0.4268750499456941 test_loss: 0.4600989436207664\n",
      "iteration 4043train_loss: 0.42687504901901424 test_loss: 0.46009917519135957\n",
      "iteration 4044train_loss: 0.42687504809344606 test_loss: 0.4600994066241339\n",
      "iteration 4045train_loss: 0.42687504716898805 test_loss: 0.4600996379191711\n",
      "iteration 4046train_loss: 0.426875046245639 test_loss: 0.46009986907655265\n",
      "iteration 4047train_loss: 0.4268750453233974 test_loss: 0.4601001000963602\n",
      "iteration 4048train_loss: 0.42687504440226204 test_loss: 0.4601003309786749\n",
      "iteration 4049train_loss: 0.4268750434822315 test_loss: 0.4601005617235784\n",
      "iteration 4050train_loss: 0.4268750425633046 test_loss: 0.4601007923311518\n",
      "iteration 4051train_loss: 0.4268750416454799 test_loss: 0.46010102280147663\n",
      "iteration 4052train_loss: 0.42687504072875615 test_loss: 0.460101253134634\n",
      "iteration 4053train_loss: 0.42687503981313196 test_loss: 0.4601014833307051\n",
      "iteration 4054train_loss: 0.426875038898606 test_loss: 0.4601017133897711\n",
      "iteration 4055train_loss: 0.426875037985177 test_loss: 0.4601019433119131\n",
      "iteration 4056train_loss: 0.42687503707284347 test_loss: 0.46010217309721213\n",
      "iteration 4057train_loss: 0.42687503616160444 test_loss: 0.4601024027457493\n",
      "iteration 4058train_loss: 0.42687503525145837 test_loss: 0.46010263225760545\n",
      "iteration 4059train_loss: 0.42687503434240387 test_loss: 0.4601028616328616\n",
      "iteration 4060train_loss: 0.42687503343443983 test_loss: 0.46010309087159856\n",
      "iteration 4061train_loss: 0.4268750325275648 test_loss: 0.4601033199738971\n",
      "iteration 4062train_loss: 0.4268750316217775 test_loss: 0.460103548939838\n",
      "iteration 4063train_loss: 0.4268750307170766 test_loss: 0.4601037777695022\n",
      "iteration 4064train_loss: 0.426875029813461 test_loss: 0.4601040064629701\n",
      "iteration 4065train_loss: 0.4268750289109291 test_loss: 0.4601042350203224\n",
      "iteration 4066train_loss: 0.4268750280094797 test_loss: 0.46010446344163974\n",
      "iteration 4067train_loss: 0.4268750271091115 test_loss: 0.46010469172700263\n",
      "iteration 4068train_loss: 0.42687502620982315 test_loss: 0.46010491987649166\n",
      "iteration 4069train_loss: 0.4268750253116136 test_loss: 0.4601051478901871\n",
      "iteration 4070train_loss: 0.42687502441448116 test_loss: 0.46010537576816946\n",
      "iteration 4071train_loss: 0.4268750235184249 test_loss: 0.46010560351051893\n",
      "iteration 4072train_loss: 0.4268750226234433 test_loss: 0.4601058311173161\n",
      "iteration 4073train_loss: 0.4268750217295352 test_loss: 0.46010605858864106\n",
      "iteration 4074train_loss: 0.4268750208366992 test_loss: 0.4601062859245739\n",
      "iteration 4075train_loss: 0.42687501994493404 test_loss: 0.460106513125195\n",
      "iteration 4076train_loss: 0.42687501905423847 test_loss: 0.4601067401905844\n",
      "iteration 4077train_loss: 0.4268750181646112 test_loss: 0.4601069671208221\n",
      "iteration 4078train_loss: 0.426875017276051 test_loss: 0.4601071939159882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4079train_loss: 0.4268750163885565 test_loss: 0.4601074205761626\n",
      "iteration 4080train_loss: 0.42687501550212625 test_loss: 0.4601076471014253\n",
      "iteration 4081train_loss: 0.42687501461675936 test_loss: 0.4601078734918563\n",
      "iteration 4082train_loss: 0.4268750137324543 test_loss: 0.4601080997475351\n",
      "iteration 4083train_loss: 0.42687501284920987 test_loss: 0.46010832586854167\n",
      "iteration 4084train_loss: 0.4268750119670247 test_loss: 0.4601085518549559\n",
      "iteration 4085train_loss: 0.42687501108589765 test_loss: 0.4601087777068574\n",
      "iteration 4086train_loss: 0.4268750102058273 test_loss: 0.46010900342432565\n",
      "iteration 4087train_loss: 0.42687500932681255 test_loss: 0.4601092290074404\n",
      "iteration 4088train_loss: 0.42687500844885207 test_loss: 0.4601094544562812\n",
      "iteration 4089train_loss: 0.4268750075719444 test_loss: 0.46010967977092754\n",
      "iteration 4090train_loss: 0.4268750066960885 test_loss: 0.460109904951459\n",
      "iteration 4091train_loss: 0.4268750058212832 test_loss: 0.46011012999795464\n",
      "iteration 4092train_loss: 0.42687500494752695 test_loss: 0.4601103549104943\n",
      "iteration 4093train_loss: 0.4268750040748186 test_loss: 0.4601105796891569\n",
      "iteration 4094train_loss: 0.42687500320315697 test_loss: 0.46011080433402207\n",
      "iteration 4095train_loss: 0.4268750023325408 test_loss: 0.46011102884516875\n",
      "iteration 4096train_loss: 0.4268750014629687 test_loss: 0.4601112532226764\n",
      "iteration 4097train_loss: 0.42687500059443945 test_loss: 0.46011147746662384\n",
      "iteration 4098train_loss: 0.42687499972695186 test_loss: 0.4601117015770904\n",
      "iteration 4099train_loss: 0.42687499886050473 test_loss: 0.4601119255541551\n",
      "iteration 4100train_loss: 0.4268749979950967 test_loss: 0.4601121493978969\n",
      "iteration 4101train_loss: 0.4268749971307265 test_loss: 0.46011237310839476\n",
      "iteration 4102train_loss: 0.42687499626739306 test_loss: 0.46011259668572757\n",
      "iteration 4103train_loss: 0.42687499540509494 test_loss: 0.46011282012997423\n",
      "iteration 4104train_loss: 0.4268749945438309 test_loss: 0.46011304344121357\n",
      "iteration 4105train_loss: 0.4268749936835998 test_loss: 0.4601132666195244\n",
      "iteration 4106train_loss: 0.4268749928244004 test_loss: 0.46011348966498516\n",
      "iteration 4107train_loss: 0.42687499196623147 test_loss: 0.4601137125776749\n",
      "iteration 4108train_loss: 0.42687499110909166 test_loss: 0.4601139353576721\n",
      "iteration 4109train_loss: 0.42687499025297976 test_loss: 0.46011415800505545\n",
      "iteration 4110train_loss: 0.42687498939789476 test_loss: 0.4601143805199032\n",
      "iteration 4111train_loss: 0.42687498854383504 test_loss: 0.46011460290229417\n",
      "iteration 4112train_loss: 0.4268749876907996 test_loss: 0.4601148251523066\n",
      "iteration 4113train_loss: 0.4268749868387872 test_loss: 0.460115047270019\n",
      "iteration 4114train_loss: 0.42687498598779655 test_loss: 0.4601152692555098\n",
      "iteration 4115train_loss: 0.42687498513782646 test_loss: 0.4601154911088571\n",
      "iteration 4116train_loss: 0.42687498428887577 test_loss: 0.46011571283013936\n",
      "iteration 4117train_loss: 0.4268749834409431 test_loss: 0.46011593441943477\n",
      "iteration 4118train_loss: 0.42687498259402734 test_loss: 0.4601161558768213\n",
      "iteration 4119train_loss: 0.4268749817481273 test_loss: 0.4601163772023774\n",
      "iteration 4120train_loss: 0.4268749809032416 test_loss: 0.4601165983961809\n",
      "iteration 4121train_loss: 0.4268749800593692 test_loss: 0.46011681945831007\n",
      "iteration 4122train_loss: 0.4268749792165087 test_loss: 0.46011704038884266\n",
      "iteration 4123train_loss: 0.4268749783746591 test_loss: 0.46011726118785684\n",
      "iteration 4124train_loss: 0.4268749775338189 test_loss: 0.46011748185543033\n",
      "iteration 4125train_loss: 0.4268749766939872 test_loss: 0.4601177023916411\n",
      "iteration 4126train_loss: 0.4268749758551626 test_loss: 0.4601179227965669\n",
      "iteration 4127train_loss: 0.42687497501734395 test_loss: 0.4601181430702855\n",
      "iteration 4128train_loss: 0.42687497418053 test_loss: 0.4601183632128748\n",
      "iteration 4129train_loss: 0.42687497334471963 test_loss: 0.4601185832244121\n",
      "iteration 4130train_loss: 0.4268749725099115 test_loss: 0.4601188031049753\n",
      "iteration 4131train_loss: 0.4268749716761045 test_loss: 0.4601190228546421\n",
      "iteration 4132train_loss: 0.42687497084329734 test_loss: 0.46011924247348973\n",
      "iteration 4133train_loss: 0.4268749700114889 test_loss: 0.460119461961596\n",
      "iteration 4134train_loss: 0.42687496918067797 test_loss: 0.46011968131903797\n",
      "iteration 4135train_loss: 0.4268749683508634 test_loss: 0.4601199005458934\n",
      "iteration 4136train_loss: 0.42687496752204385 test_loss: 0.4601201196422395\n",
      "iteration 4137train_loss: 0.42687496669421826 test_loss: 0.46012033860815355\n",
      "iteration 4138train_loss: 0.4268749658673853 test_loss: 0.46012055744371294\n",
      "iteration 4139train_loss: 0.42687496504154393 test_loss: 0.4601207761489947\n",
      "iteration 4140train_loss: 0.4268749642166929 test_loss: 0.4601209947240762\n",
      "iteration 4141train_loss: 0.426874963392831 test_loss: 0.4601212131690346\n",
      "iteration 4142train_loss: 0.42687496256995705 test_loss: 0.46012143148394674\n",
      "iteration 4143train_loss: 0.4268749617480698 test_loss: 0.46012164966888996\n",
      "iteration 4144train_loss: 0.4268749609271682 test_loss: 0.460121867723941\n",
      "iteration 4145train_loss: 0.4268749601072509 test_loss: 0.460122085649177\n",
      "iteration 4146train_loss: 0.426874959288317 test_loss: 0.4601223034446747\n",
      "iteration 4147train_loss: 0.4268749584703649 test_loss: 0.46012252111051116\n",
      "iteration 4148train_loss: 0.42687495765339367 test_loss: 0.46012273864676306\n",
      "iteration 4149train_loss: 0.4268749568374022 test_loss: 0.4601229560535073\n",
      "iteration 4150train_loss: 0.4268749560223892 test_loss: 0.4601231733308204\n",
      "iteration 4151train_loss: 0.4268749552083535 test_loss: 0.4601233904787792\n",
      "iteration 4152train_loss: 0.4268749543952939 test_loss: 0.46012360749746034\n",
      "iteration 4153train_loss: 0.42687495358320926 test_loss: 0.4601238243869403\n",
      "iteration 4154train_loss: 0.4268749527720986 test_loss: 0.4601240411472959\n",
      "iteration 4155train_loss: 0.4268749519619603 test_loss: 0.4601242577786032\n",
      "iteration 4156train_loss: 0.4268749511527936 test_loss: 0.4601244742809391\n",
      "iteration 4157train_loss: 0.42687495034459705 test_loss: 0.4601246906543798\n",
      "iteration 4158train_loss: 0.4268749495373698 test_loss: 0.46012490689900165\n",
      "iteration 4159train_loss: 0.4268749487311103 test_loss: 0.46012512301488107\n",
      "iteration 4160train_loss: 0.4268749479258178 test_loss: 0.4601253390020943\n",
      "iteration 4161train_loss: 0.4268749471214908 test_loss: 0.46012555486071766\n",
      "iteration 4162train_loss: 0.42687494631812833 test_loss: 0.4601257705908271\n",
      "iteration 4163train_loss: 0.4268749455157292 test_loss: 0.460125986192499\n",
      "iteration 4164train_loss: 0.4268749447142922 test_loss: 0.4601262016658095\n",
      "iteration 4165train_loss: 0.42687494391381614 test_loss: 0.4601264170108344\n",
      "iteration 4166train_loss: 0.42687494311429997 test_loss: 0.46012663222764993\n",
      "iteration 4167train_loss: 0.42687494231574247 test_loss: 0.460126847316332\n",
      "iteration 4168train_loss: 0.4268749415181425 test_loss: 0.4601270622769566\n",
      "iteration 4169train_loss: 0.42687494072149895 test_loss: 0.46012727710959933\n",
      "iteration 4170train_loss: 0.42687493992581055 test_loss: 0.46012749181433654\n",
      "iteration 4171train_loss: 0.4268749391310763 test_loss: 0.4601277063912436\n",
      "iteration 4172train_loss: 0.426874938337295 test_loss: 0.46012792084039644\n",
      "iteration 4173train_loss: 0.4268749375444654 test_loss: 0.4601281351618706\n",
      "iteration 4174train_loss: 0.42687493675258664 test_loss: 0.46012834935574193\n",
      "iteration 4175train_loss: 0.42687493596165726 test_loss: 0.46012856342208597\n",
      "iteration 4176train_loss: 0.42687493517167624 test_loss: 0.46012877736097824\n",
      "iteration 4177train_loss: 0.4268749343826425 test_loss: 0.4601289911724944\n",
      "iteration 4178train_loss: 0.42687493359455486 test_loss: 0.46012920485670994\n",
      "iteration 4179train_loss: 0.4268749328074121 test_loss: 0.4601294184137\n",
      "iteration 4180train_loss: 0.42687493202121324 test_loss: 0.4601296318435403\n",
      "iteration 4181train_loss: 0.426874931235957 test_loss: 0.46012984514630606\n",
      "iteration 4182train_loss: 0.4268749304516424 test_loss: 0.4601300583220726\n",
      "iteration 4183train_loss: 0.4268749296682682 test_loss: 0.46013027137091517\n",
      "iteration 4184train_loss: 0.42687492888583317 test_loss: 0.4601304842929091\n",
      "iteration 4185train_loss: 0.42687492810433636 test_loss: 0.4601306970881294\n",
      "iteration 4186train_loss: 0.4268749273237766 test_loss: 0.46013090975665133\n",
      "iteration 4187train_loss: 0.42687492654415277 test_loss: 0.4601311222985499\n",
      "iteration 4188train_loss: 0.4268749257654636 test_loss: 0.4601313347139002\n",
      "iteration 4189train_loss: 0.42687492498770824 test_loss: 0.46013154700277714\n",
      "iteration 4190train_loss: 0.42687492421088535 test_loss: 0.4601317591652557\n",
      "iteration 4191train_loss: 0.42687492343499384 test_loss: 0.460131971201411\n",
      "iteration 4192train_loss: 0.42687492266003274 test_loss: 0.46013218311131776\n",
      "iteration 4193train_loss: 0.42687492188600085 test_loss: 0.46013239489505076\n",
      "iteration 4194train_loss: 0.4268749211128968 test_loss: 0.4601326065526846\n",
      "iteration 4195train_loss: 0.4268749203407199 test_loss: 0.4601328180842945\n",
      "iteration 4196train_loss: 0.4268749195694687 test_loss: 0.4601330294899547\n",
      "iteration 4197train_loss: 0.4268749187991423 test_loss: 0.4601332407697401\n",
      "iteration 4198train_loss: 0.4268749180297395 test_loss: 0.46013345192372523\n",
      "iteration 4199train_loss: 0.4268749172612592 test_loss: 0.4601336629519847\n",
      "iteration 4200train_loss: 0.42687491649370024 test_loss: 0.460133873854593\n",
      "iteration 4201train_loss: 0.42687491572706165 test_loss: 0.4601340846316245\n",
      "iteration 4202train_loss: 0.4268749149613422 test_loss: 0.4601342952831538\n",
      "iteration 4203train_loss: 0.42687491419654083 test_loss: 0.4601345058092552\n",
      "iteration 4204train_loss: 0.42687491343265643 test_loss: 0.46013471621000296\n",
      "iteration 4205train_loss: 0.4268749126696879 test_loss: 0.4601349264854716\n",
      "iteration 4206train_loss: 0.42687491190763416 test_loss: 0.46013513663573535\n",
      "iteration 4207train_loss: 0.42687491114649406 test_loss: 0.4601353466608682\n",
      "iteration 4208train_loss: 0.4268749103862665 test_loss: 0.46013555656094446\n",
      "iteration 4209train_loss: 0.42687490962695046 test_loss: 0.46013576633603825\n",
      "iteration 4210train_loss: 0.4268749088685448 test_loss: 0.4601359759862236\n",
      "iteration 4211train_loss: 0.42687490811104833 test_loss: 0.4601361855115747\n",
      "iteration 4212train_loss: 0.42687490735446015 test_loss: 0.46013639491216535\n",
      "iteration 4213train_loss: 0.42687490659877914 test_loss: 0.46013660418806973\n",
      "iteration 4214train_loss: 0.42687490584400406 test_loss: 0.4601368133393616\n",
      "iteration 4215train_loss: 0.42687490509013387 test_loss: 0.4601370223661148\n",
      "iteration 4216train_loss: 0.4268749043371676 test_loss: 0.4601372312684033\n",
      "iteration 4217train_loss: 0.426874903585104 test_loss: 0.46013744004630075\n",
      "iteration 4218train_loss: 0.42687490283394214 test_loss: 0.46013764869988094\n",
      "iteration 4219train_loss: 0.4268749020836808 test_loss: 0.46013785722921746\n",
      "iteration 4220train_loss: 0.42687490133431893 test_loss: 0.46013806563438414\n",
      "iteration 4221train_loss: 0.42687490058585553 test_loss: 0.4601382739154546\n",
      "iteration 4222train_loss: 0.4268748998382896 test_loss: 0.4601384820725022\n",
      "iteration 4223train_loss: 0.4268748990916198 test_loss: 0.46013869010560055\n",
      "iteration 4224train_loss: 0.4268748983458451 test_loss: 0.4601388980148231\n",
      "iteration 4225train_loss: 0.42687489760096453 test_loss: 0.4601391058002435\n",
      "iteration 4226train_loss: 0.42687489685697727 test_loss: 0.46013931346193493\n",
      "iteration 4227train_loss: 0.4268748961138817 test_loss: 0.4601395209999708\n",
      "iteration 4228train_loss: 0.42687489537167705 test_loss: 0.4601397284144244\n",
      "iteration 4229train_loss: 0.4268748946303622 test_loss: 0.460139935705369\n",
      "iteration 4230train_loss: 0.4268748938899362 test_loss: 0.46014014287287786\n",
      "iteration 4231train_loss: 0.42687489315039784 test_loss: 0.4601403499170241\n",
      "iteration 4232train_loss: 0.42687489241174614 test_loss: 0.46014055683788097\n",
      "iteration 4233train_loss: 0.42687489167398 test_loss: 0.4601407636355213\n",
      "iteration 4234train_loss: 0.42687489093709824 test_loss: 0.4601409703100185\n",
      "iteration 4235train_loss: 0.4268748902011 test_loss: 0.46014117686144534\n",
      "iteration 4236train_loss: 0.42687488946598406 test_loss: 0.4601413832898748\n",
      "iteration 4237train_loss: 0.4268748887317494 test_loss: 0.46014158959537993\n",
      "iteration 4238train_loss: 0.426874887998395 test_loss: 0.46014179577803344\n",
      "iteration 4239train_loss: 0.4268748872659199 test_loss: 0.4601420018379082\n",
      "iteration 4240train_loss: 0.4268748865343229 test_loss: 0.46014220777507725\n",
      "iteration 4241train_loss: 0.42687488580360294 test_loss: 0.460142413589613\n",
      "iteration 4242train_loss: 0.426874885073759 test_loss: 0.46014261928158845\n",
      "iteration 4243train_loss: 0.42687488434479004 test_loss: 0.4601428248510761\n",
      "iteration 4244train_loss: 0.4268748836166949 test_loss: 0.4601430302981485\n",
      "iteration 4245train_loss: 0.4268748828894728 test_loss: 0.4601432356228784\n",
      "iteration 4246train_loss: 0.42687488216312236 test_loss: 0.46014344082533826\n",
      "iteration 4247train_loss: 0.4268748814376428 test_loss: 0.46014364590560053\n",
      "iteration 4248train_loss: 0.42687488071303287 test_loss: 0.46014385086373777\n",
      "iteration 4249train_loss: 0.4268748799892917 test_loss: 0.4601440556998225\n",
      "iteration 4250train_loss: 0.42687487926641815 test_loss: 0.4601442604139268\n",
      "iteration 4251train_loss: 0.4268748785444112 test_loss: 0.46014446500612316\n",
      "iteration 4252train_loss: 0.42687487782326977 test_loss: 0.46014466947648386\n",
      "iteration 4253train_loss: 0.4268748771029928 test_loss: 0.460144873825081\n",
      "iteration 4254train_loss: 0.4268748763835795 test_loss: 0.460145078051987\n",
      "iteration 4255train_loss: 0.4268748756650285 test_loss: 0.4601452821572739\n",
      "iteration 4256train_loss: 0.42687487494733894 test_loss: 0.46014548614101386\n",
      "iteration 4257train_loss: 0.42687487423050974 test_loss: 0.4601456900032789\n",
      "iteration 4258train_loss: 0.4268748735145399 test_loss: 0.46014589374414117\n",
      "iteration 4259train_loss: 0.42687487279942826 test_loss: 0.46014609736367246\n",
      "iteration 4260train_loss: 0.42687487208517405 test_loss: 0.4601463008619449\n",
      "iteration 4261train_loss: 0.42687487137177593 test_loss: 0.4601465042390303\n",
      "iteration 4262train_loss: 0.4268748706592331 test_loss: 0.4601467074950006\n",
      "iteration 4263train_loss: 0.42687486994754437 test_loss: 0.4601469106299276\n",
      "iteration 4264train_loss: 0.4268748692367089 test_loss: 0.46014711364388283\n",
      "iteration 4265train_loss: 0.4268748685267255 test_loss: 0.4601473165369383\n",
      "iteration 4266train_loss: 0.42687486781759326 test_loss: 0.4601475193091658\n",
      "iteration 4267train_loss: 0.4268748671093112 test_loss: 0.46014772196063675\n",
      "iteration 4268train_loss: 0.426874866401878 test_loss: 0.46014792449142283\n",
      "iteration 4269train_loss: 0.4268748656952929 test_loss: 0.4601481269015957\n",
      "iteration 4270train_loss: 0.426874864989555 test_loss: 0.4601483291912267\n",
      "iteration 4271train_loss: 0.4268748642846629 test_loss: 0.4601485313603874\n",
      "iteration 4272train_loss: 0.4268748635806159 test_loss: 0.46014873340914936\n",
      "iteration 4273train_loss: 0.42687486287741283 test_loss: 0.4601489353375839\n",
      "iteration 4274train_loss: 0.4268748621750527 test_loss: 0.46014913714576233\n",
      "iteration 4275train_loss: 0.4268748614735346 test_loss: 0.46014933883375597\n",
      "iteration 4276train_loss: 0.42687486077285747 test_loss: 0.4601495404016361\n",
      "iteration 4277train_loss: 0.42687486007302017 test_loss: 0.4601497418494741\n",
      "iteration 4278train_loss: 0.42687485937402175 test_loss: 0.46014994317734104\n",
      "iteration 4279train_loss: 0.4268748586758614 test_loss: 0.46015014438530794\n",
      "iteration 4280train_loss: 0.4268748579785379 test_loss: 0.4601503454734461\n",
      "iteration 4281train_loss: 0.42687485728205027 test_loss: 0.4601505464418266\n",
      "iteration 4282train_loss: 0.42687485658639757 test_loss: 0.46015074729052036\n",
      "iteration 4283train_loss: 0.42687485589157875 test_loss: 0.4601509480195984\n",
      "iteration 4284train_loss: 0.4268748551975928 test_loss: 0.46015114862913165\n",
      "iteration 4285train_loss: 0.42687485450443874 test_loss: 0.4601513491191911\n",
      "iteration 4286train_loss: 0.42687485381211565 test_loss: 0.46015154948984743\n",
      "iteration 4287train_loss: 0.4268748531206224 test_loss: 0.4601517497411715\n",
      "iteration 4288train_loss: 0.4268748524299581 test_loss: 0.46015194987323443\n",
      "iteration 4289train_loss: 0.4268748517401216 test_loss: 0.46015214988610637\n",
      "iteration 4290train_loss: 0.4268748510511121 test_loss: 0.46015234977985847\n",
      "iteration 4291train_loss: 0.42687485036292844 test_loss: 0.46015254955456125\n",
      "iteration 4292train_loss: 0.4268748496755697 test_loss: 0.4601527492102852\n",
      "iteration 4293train_loss: 0.42687484898903494 test_loss: 0.4601529487471011\n",
      "iteration 4294train_loss: 0.42687484830332323 test_loss: 0.4601531481650793\n",
      "iteration 4295train_loss: 0.42687484761843336 test_loss: 0.4601533474642903\n",
      "iteration 4296train_loss: 0.42687484693436434 test_loss: 0.46015354664480473\n",
      "iteration 4297train_loss: 0.4268748462511155 test_loss: 0.4601537457066928\n",
      "iteration 4298train_loss: 0.4268748455686855 test_loss: 0.460153944650025\n",
      "iteration 4299train_loss: 0.4268748448870736 test_loss: 0.4601541434748715\n",
      "iteration 4300train_loss: 0.4268748442062787 test_loss: 0.46015434218130263\n",
      "iteration 4301train_loss: 0.42687484352629995 test_loss: 0.4601545407693888\n",
      "iteration 4302train_loss: 0.42687484284713606 test_loss: 0.4601547392392\n",
      "iteration 4303train_loss: 0.42687484216878646 test_loss: 0.4601549375908065\n",
      "iteration 4304train_loss: 0.4268748414912499 test_loss: 0.4601551358242783\n",
      "iteration 4305train_loss: 0.4268748408145254 test_loss: 0.4601553339396856\n",
      "iteration 4306train_loss: 0.42687484013861215 test_loss: 0.46015553193709835\n",
      "iteration 4307train_loss: 0.426874839463509 test_loss: 0.46015572981658664\n",
      "iteration 4308train_loss: 0.4268748387892152 test_loss: 0.46015592757822027\n",
      "iteration 4309train_loss: 0.4268748381157295 test_loss: 0.46015612522206933\n",
      "iteration 4310train_loss: 0.42687483744305116 test_loss: 0.46015632274820356\n",
      "iteration 4311train_loss: 0.4268748367711791 test_loss: 0.46015652015669284\n",
      "iteration 4312train_loss: 0.42687483610011234 test_loss: 0.46015671744760694\n",
      "iteration 4313train_loss: 0.42687483542985 test_loss: 0.46015691462101566\n",
      "iteration 4314train_loss: 0.4268748347603909 test_loss: 0.4601571116769886\n",
      "iteration 4315train_loss: 0.4268748340917344 test_loss: 0.4601573086155954\n",
      "iteration 4316train_loss: 0.4268748334238794 test_loss: 0.4601575054369058\n",
      "iteration 4317train_loss: 0.4268748327568248 test_loss: 0.46015770214098944\n",
      "iteration 4318train_loss: 0.42687483209056976 test_loss: 0.46015789872791574\n",
      "iteration 4319train_loss: 0.42687483142511323 test_loss: 0.46015809519775425\n",
      "iteration 4320train_loss: 0.42687483076045457 test_loss: 0.4601582915505744\n",
      "iteration 4321train_loss: 0.42687483009659233 test_loss: 0.46015848778644564\n",
      "iteration 4322train_loss: 0.4268748294335259 test_loss: 0.4601586839054372\n",
      "iteration 4323train_loss: 0.42687482877125416 test_loss: 0.4601588799076186\n",
      "iteration 4324train_loss: 0.42687482810977634 test_loss: 0.4601590757930592\n",
      "iteration 4325train_loss: 0.4268748274490913 test_loss: 0.4601592715618281\n",
      "iteration 4326train_loss: 0.42687482678919814 test_loss: 0.46015946721399453\n",
      "iteration 4327train_loss: 0.42687482613009586 test_loss: 0.4601596627496277\n",
      "iteration 4328train_loss: 0.42687482547178374 test_loss: 0.46015985816879673\n",
      "iteration 4329train_loss: 0.4268748248142605 test_loss: 0.4601600534715708\n",
      "iteration 4330train_loss: 0.42687482415752537 test_loss: 0.4601602486580189\n",
      "iteration 4331train_loss: 0.4268748235015775 test_loss: 0.46016044372821\n",
      "iteration 4332train_loss: 0.42687482284641576 test_loss: 0.460160638682213\n",
      "iteration 4333train_loss: 0.42687482219203926 test_loss: 0.4601608335200972\n",
      "iteration 4334train_loss: 0.42687482153844725 test_loss: 0.46016102824193106\n",
      "iteration 4335train_loss: 0.42687482088563844 test_loss: 0.4601612228477836\n",
      "iteration 4336train_loss: 0.4268748202336121 test_loss: 0.4601614173377237\n",
      "iteration 4337train_loss: 0.4268748195823672 test_loss: 0.46016161171182013\n",
      "iteration 4338train_loss: 0.426874818931903 test_loss: 0.4601618059701414\n",
      "iteration 4339train_loss: 0.42687481828221835 test_loss: 0.4601620001127566\n",
      "iteration 4340train_loss: 0.4268748176333123 test_loss: 0.4601621941397339\n",
      "iteration 4341train_loss: 0.4268748169851841 test_loss: 0.46016238805114235\n",
      "iteration 4342train_loss: 0.42687481633783264 test_loss: 0.46016258184705033\n",
      "iteration 4343train_loss: 0.4268748156912572 test_loss: 0.46016277552752616\n",
      "iteration 4344train_loss: 0.42687481504545666 test_loss: 0.46016296909263876\n",
      "iteration 4345train_loss: 0.42687481440043 test_loss: 0.4601631625424562\n",
      "iteration 4346train_loss: 0.4268748137561765 test_loss: 0.46016335587704715\n",
      "iteration 4347train_loss: 0.42687481311269526 test_loss: 0.46016354909647983\n",
      "iteration 4348train_loss: 0.42687481246998515 test_loss: 0.4601637422008225\n",
      "iteration 4349train_loss: 0.42687481182804543 test_loss: 0.46016393519014376\n",
      "iteration 4350train_loss: 0.426874811186875 test_loss: 0.46016412806451157\n",
      "iteration 4351train_loss: 0.426874810546473 test_loss: 0.4601643208239942\n",
      "iteration 4352train_loss: 0.4268748099068387 test_loss: 0.46016451346865994\n",
      "iteration 4353train_loss: 0.42687480926797083 test_loss: 0.46016470599857673\n",
      "iteration 4354train_loss: 0.4268748086298687 test_loss: 0.4601648984138129\n",
      "iteration 4355train_loss: 0.4268748079925314 test_loss: 0.46016509071443623\n",
      "iteration 4356train_loss: 0.42687480735595795 test_loss: 0.46016528290051484\n",
      "iteration 4357train_loss: 0.42687480672014744 test_loss: 0.46016547497211685\n",
      "iteration 4358train_loss: 0.42687480608509887 test_loss: 0.46016566692931\n",
      "iteration 4359train_loss: 0.42687480545081147 test_loss: 0.46016585877216215\n",
      "iteration 4360train_loss: 0.42687480481728424 test_loss: 0.46016605050074133\n",
      "iteration 4361train_loss: 0.4268748041845163 test_loss: 0.4601662421151152\n",
      "iteration 4362train_loss: 0.4268748035525068 test_loss: 0.4601664336153516\n",
      "iteration 4363train_loss: 0.4268748029212547 test_loss: 0.46016662500151817\n",
      "iteration 4364train_loss: 0.4268748022907591 test_loss: 0.4601668162736828\n",
      "iteration 4365train_loss: 0.4268748016610191 test_loss: 0.46016700743191297\n",
      "iteration 4366train_loss: 0.426874801032034 test_loss: 0.46016719847627624\n",
      "iteration 4367train_loss: 0.4268748004038026 test_loss: 0.4601673894068404\n",
      "iteration 4368train_loss: 0.4268747997763241 test_loss: 0.46016758022367277\n",
      "iteration 4369train_loss: 0.42687479914959764 test_loss: 0.4601677709268411\n",
      "iteration 4370train_loss: 0.4268747985236223 test_loss: 0.4601679615164124\n",
      "iteration 4371train_loss: 0.4268747978983973 test_loss: 0.4601681519924546\n",
      "iteration 4372train_loss: 0.4268747972739214 test_loss: 0.4601683423550346\n",
      "iteration 4373train_loss: 0.42687479665019407 test_loss: 0.46016853260422014\n",
      "iteration 4374train_loss: 0.4268747960272141 test_loss: 0.46016872274007825\n",
      "iteration 4375train_loss: 0.42687479540498086 test_loss: 0.4601689127626763\n",
      "iteration 4376train_loss: 0.42687479478349327 test_loss: 0.4601691026720815\n",
      "iteration 4377train_loss: 0.42687479416275054 test_loss: 0.460169292468361\n",
      "iteration 4378train_loss: 0.42687479354275176 test_loss: 0.460169482151582\n",
      "iteration 4379train_loss: 0.42687479292349595 test_loss: 0.46016967172181134\n",
      "iteration 4380train_loss: 0.4268747923049823 test_loss: 0.4601698611791164\n",
      "iteration 4381train_loss: 0.42687479168721 test_loss: 0.460170050523564\n",
      "iteration 4382train_loss: 0.42687479107017795 test_loss: 0.4601702397552213\n",
      "iteration 4383train_loss: 0.42687479045388543 test_loss: 0.4601704288741551\n",
      "iteration 4384train_loss: 0.42687478983833144 test_loss: 0.4601706178804322\n",
      "iteration 4385train_loss: 0.4268747892235152 test_loss: 0.46017080677411987\n",
      "iteration 4386train_loss: 0.4268747886094356 test_loss: 0.4601709955552844\n",
      "iteration 4387train_loss: 0.42687478799609213 test_loss: 0.46017118422399284\n",
      "iteration 4388train_loss: 0.42687478738348367 test_loss: 0.4601713727803119\n",
      "iteration 4389train_loss: 0.42687478677160934 test_loss: 0.4601715612243084\n",
      "iteration 4390train_loss: 0.42687478616046826 test_loss: 0.46017174955604895\n",
      "iteration 4391train_loss: 0.4268747855500596 test_loss: 0.4601719377756\n",
      "iteration 4392train_loss: 0.42687478494038245 test_loss: 0.46017212588302836\n",
      "iteration 4393train_loss: 0.426874784331436 test_loss: 0.46017231387840046\n",
      "iteration 4394train_loss: 0.42687478372321924 test_loss: 0.4601725017617828\n",
      "iteration 4395train_loss: 0.42687478311573135 test_loss: 0.46017268953324186\n",
      "iteration 4396train_loss: 0.4268747825089715 test_loss: 0.4601728771928441\n",
      "iteration 4397train_loss: 0.42687478190293876 test_loss: 0.460173064740656\n",
      "iteration 4398train_loss: 0.42687478129763234 test_loss: 0.4601732521767437\n",
      "iteration 4399train_loss: 0.42687478069305124 test_loss: 0.46017343950117373\n",
      "iteration 4400train_loss: 0.42687478008919477 test_loss: 0.4601736267140123\n",
      "iteration 4401train_loss: 0.4268747794860619 test_loss: 0.4601738138153255\n",
      "iteration 4402train_loss: 0.4268747788836518 test_loss: 0.46017400080517956\n",
      "iteration 4403train_loss: 0.4268747782819635 test_loss: 0.4601741876836409\n",
      "iteration 4404train_loss: 0.42687477768099646 test_loss: 0.4601743744507753\n",
      "iteration 4405train_loss: 0.4268747770807495 test_loss: 0.4601745611066491\n",
      "iteration 4406train_loss: 0.4268747764812219 test_loss: 0.4601747476513282\n",
      "iteration 4407train_loss: 0.4268747758824127 test_loss: 0.4601749340848786\n",
      "iteration 4408train_loss: 0.426874775284321 test_loss: 0.4601751204073664\n",
      "iteration 4409train_loss: 0.4268747746869461 test_loss: 0.4601753066188573\n",
      "iteration 4410train_loss: 0.4268747740902871 test_loss: 0.4601754927194173\n",
      "iteration 4411train_loss: 0.4268747734943431 test_loss: 0.4601756787091124\n",
      "iteration 4412train_loss: 0.4268747728991133 test_loss: 0.46017586458800824\n",
      "iteration 4413train_loss: 0.42687477230459675 test_loss: 0.4601760503561705\n",
      "iteration 4414train_loss: 0.4268747717107927 test_loss: 0.4601762360136652\n",
      "iteration 4415train_loss: 0.42687477111770006 test_loss: 0.46017642156055777\n",
      "iteration 4416train_loss: 0.42687477052531825 test_loss: 0.4601766069969139\n",
      "iteration 4417train_loss: 0.42687476993364637 test_loss: 0.46017679232279923\n",
      "iteration 4418train_loss: 0.4268747693426835 test_loss: 0.46017697753827946\n",
      "iteration 4419train_loss: 0.4268747687524287 test_loss: 0.46017716264342007\n",
      "iteration 4420train_loss: 0.42687476816288134 test_loss: 0.4601773476382865\n",
      "iteration 4421train_loss: 0.42687476757404025 test_loss: 0.46017753252294413\n",
      "iteration 4422train_loss: 0.42687476698590504 test_loss: 0.46017771729745865\n",
      "iteration 4423train_loss: 0.4268747663984744 test_loss: 0.4601779019618952\n",
      "iteration 4424train_loss: 0.4268747658117478 test_loss: 0.46017808651631914\n",
      "iteration 4425train_loss: 0.4268747652257243 test_loss: 0.460178270960796\n",
      "iteration 4426train_loss: 0.42687476464040297 test_loss: 0.46017845529539086\n",
      "iteration 4427train_loss: 0.426874764055783 test_loss: 0.4601786395201689\n",
      "iteration 4428train_loss: 0.4268747634718637 test_loss: 0.4601788236351955\n",
      "iteration 4429train_loss: 0.42687476288864407 test_loss: 0.4601790076405357\n",
      "iteration 4430train_loss: 0.4268747623061232 test_loss: 0.46017919153625464\n",
      "iteration 4431train_loss: 0.42687476172430056 test_loss: 0.46017937532241726\n",
      "iteration 4432train_loss: 0.42687476114317496 test_loss: 0.4601795589990889\n",
      "iteration 4433train_loss: 0.4268747605627458 test_loss: 0.46017974256633437\n",
      "iteration 4434train_loss: 0.42687475998301216 test_loss: 0.4601799260242186\n",
      "iteration 4435train_loss: 0.4268747594039732 test_loss: 0.46018010937280657\n",
      "iteration 4436train_loss: 0.4268747588256281 test_loss: 0.4601802926121632\n",
      "iteration 4437train_loss: 0.42687475824797594 test_loss: 0.4601804757423533\n",
      "iteration 4438train_loss: 0.42687475767101596 test_loss: 0.46018065876344183\n",
      "iteration 4439train_loss: 0.4268747570947475 test_loss: 0.46018084167549317\n",
      "iteration 4440train_loss: 0.4268747565191694 test_loss: 0.46018102447857245\n",
      "iteration 4441train_loss: 0.42687475594428115 test_loss: 0.46018120717274424\n",
      "iteration 4442train_loss: 0.4268747553700818 test_loss: 0.46018138975807316\n",
      "iteration 4443train_loss: 0.4268747547965704 test_loss: 0.46018157223462386\n",
      "iteration 4444train_loss: 0.4268747542237462 test_loss: 0.46018175460246086\n",
      "iteration 4445train_loss: 0.4268747536516083 test_loss: 0.4601819368616488\n",
      "iteration 4446train_loss: 0.42687475308015604 test_loss: 0.46018211901225214\n",
      "iteration 4447train_loss: 0.42687475250938856 test_loss: 0.4601823010543354\n",
      "iteration 4448train_loss: 0.42687475193930496 test_loss: 0.4601824829879629\n",
      "iteration 4449train_loss: 0.42687475136990455 test_loss: 0.4601826648131991\n",
      "iteration 4450train_loss: 0.4268747508011864 test_loss: 0.4601828465301084\n",
      "iteration 4451train_loss: 0.4268747502331497 test_loss: 0.46018302813875495\n",
      "iteration 4452train_loss: 0.4268747496657936 test_loss: 0.4601832096392033\n",
      "iteration 4453train_loss: 0.4268747490991174 test_loss: 0.46018339103151745\n",
      "iteration 4454train_loss: 0.42687474853312013 test_loss: 0.46018357231576174\n",
      "iteration 4455train_loss: 0.4268747479678011 test_loss: 0.46018375349200025\n",
      "iteration 4456train_loss: 0.42687474740315945 test_loss: 0.4601839345602972\n",
      "iteration 4457train_loss: 0.4268747468391943 test_loss: 0.46018411552071664\n",
      "iteration 4458train_loss: 0.426874746275905 test_loss: 0.4601842963733225\n",
      "iteration 4459train_loss: 0.42687474571329065 test_loss: 0.46018447711817906\n",
      "iteration 4460train_loss: 0.4268747451513505 test_loss: 0.46018465775535017\n",
      "iteration 4461train_loss: 0.4268747445900835 test_loss: 0.4601848382848996\n",
      "iteration 4462train_loss: 0.42687474402948905 test_loss: 0.46018501870689144\n",
      "iteration 4463train_loss: 0.4268747434695664 test_loss: 0.46018519902138955\n",
      "iteration 4464train_loss: 0.4268747429103147 test_loss: 0.4601853792284577\n",
      "iteration 4465train_loss: 0.426874742351733 test_loss: 0.46018555932815974\n",
      "iteration 4466train_loss: 0.4268747417938206 test_loss: 0.4601857393205594\n",
      "iteration 4467train_loss: 0.42687474123657665 test_loss: 0.4601859192057204\n",
      "iteration 4468train_loss: 0.42687474068000053 test_loss: 0.46018609898370627\n",
      "iteration 4469train_loss: 0.42687474012409127 test_loss: 0.46018627865458084\n",
      "iteration 4470train_loss: 0.4268747395688479 test_loss: 0.46018645821840765\n",
      "iteration 4471train_loss: 0.4268747390142699 test_loss: 0.4601866376752503\n",
      "iteration 4472train_loss: 0.42687473846035656 test_loss: 0.46018681702517217\n",
      "iteration 4473train_loss: 0.4268747379071068 test_loss: 0.4601869962682371\n",
      "iteration 4474train_loss: 0.4268747373545199 test_loss: 0.4601871754045081\n",
      "iteration 4475train_loss: 0.4268747368025951 test_loss: 0.4601873544340489\n",
      "iteration 4476train_loss: 0.4268747362513317 test_loss: 0.46018753335692264\n",
      "iteration 4477train_loss: 0.4268747357007287 test_loss: 0.4601877121731929\n",
      "iteration 4478train_loss: 0.42687473515078544 test_loss: 0.46018789088292283\n",
      "iteration 4479train_loss: 0.4268747346015011 test_loss: 0.4601880694861758\n",
      "iteration 4480train_loss: 0.42687473405287496 test_loss: 0.46018824798301494\n",
      "iteration 4481train_loss: 0.4268747335049061 test_loss: 0.46018842637350355\n",
      "iteration 4482train_loss: 0.4268747329575938 test_loss: 0.4601886046577046\n",
      "iteration 4483train_loss: 0.4268747324109373 test_loss: 0.4601887828356813\n",
      "iteration 4484train_loss: 0.42687473186493574 test_loss: 0.46018896090749695\n",
      "iteration 4485train_loss: 0.4268747313195885 test_loss: 0.4601891388732143\n",
      "iteration 4486train_loss: 0.42687473077489446 test_loss: 0.4601893167328965\n",
      "iteration 4487train_loss: 0.4268747302308532 test_loss: 0.4601894944866064\n",
      "iteration 4488train_loss: 0.4268747296874637 test_loss: 0.4601896721344071\n",
      "iteration 4489train_loss: 0.42687472914472524 test_loss: 0.4601898496763613\n",
      "iteration 4490train_loss: 0.4268747286026372 test_loss: 0.46019002711253204\n",
      "iteration 4491train_loss: 0.4268747280611986 test_loss: 0.46019020444298203\n",
      "iteration 4492train_loss: 0.42687472752040867 test_loss: 0.46019038166777415\n",
      "iteration 4493train_loss: 0.42687472698026663 test_loss: 0.46019055878697107\n",
      "iteration 4494train_loss: 0.42687472644077173 test_loss: 0.46019073580063546\n",
      "iteration 4495train_loss: 0.42687472590192344 test_loss: 0.46019091270883017\n",
      "iteration 4496train_loss: 0.4268747253637205 test_loss: 0.4601910895116176\n",
      "iteration 4497train_loss: 0.4268747248261626 test_loss: 0.4601912662090606\n",
      "iteration 4498train_loss: 0.4268747242892485 test_loss: 0.46019144280122154\n",
      "iteration 4499train_loss: 0.426874723752978 test_loss: 0.460191619288163\n",
      "iteration 4500train_loss: 0.4268747232173498 test_loss: 0.46019179566994745\n",
      "iteration 4501train_loss: 0.4268747226823634 test_loss: 0.4601919719466374\n",
      "iteration 4502train_loss: 0.42687472214801797 test_loss: 0.4601921481182953\n",
      "iteration 4503train_loss: 0.4268747216143127 test_loss: 0.4601923241849834\n",
      "iteration 4504train_loss: 0.4268747210812469 test_loss: 0.4601925001467641\n",
      "iteration 4505train_loss: 0.4268747205488198 test_loss: 0.4601926760036998\n",
      "iteration 4506train_loss: 0.4268747200170306 test_loss: 0.4601928517558526\n",
      "iteration 4507train_loss: 0.4268747194858785 test_loss: 0.46019302740328494\n",
      "iteration 4508train_loss: 0.4268747189553627 test_loss: 0.4601932029460589\n",
      "iteration 4509train_loss: 0.42687471842548264 test_loss: 0.4601933783842366\n",
      "iteration 4510train_loss: 0.4268747178962374 test_loss: 0.46019355371788007\n",
      "iteration 4511train_loss: 0.42687471736762617 test_loss: 0.4601937289470517\n",
      "iteration 4512train_loss: 0.42687471683964834 test_loss: 0.46019390407181326\n",
      "iteration 4513train_loss: 0.426874716312303 test_loss: 0.4601940790922269\n",
      "iteration 4514train_loss: 0.4268747157855895 test_loss: 0.4601942540083546\n",
      "iteration 4515train_loss: 0.42687471525950704 test_loss: 0.46019442882025824\n",
      "iteration 4516train_loss: 0.42687471473405486 test_loss: 0.4601946035279998\n",
      "iteration 4517train_loss: 0.4268747142092322 test_loss: 0.46019477813164106\n",
      "iteration 4518train_loss: 0.4268747136850383 test_loss: 0.4601949526312439\n",
      "iteration 4519train_loss: 0.42687471316147235 test_loss: 0.4601951270268701\n",
      "iteration 4520train_loss: 0.4268747126385338 test_loss: 0.4601953013185814\n",
      "iteration 4521train_loss: 0.4268747121162217 test_loss: 0.4601954755064396\n",
      "iteration 4522train_loss: 0.42687471159453544 test_loss: 0.46019564959050635\n",
      "iteration 4523train_loss: 0.4268747110734741 test_loss: 0.46019582357084315\n",
      "iteration 4524train_loss: 0.426874710553037 test_loss: 0.46019599744751183\n",
      "iteration 4525train_loss: 0.42687471003322347 test_loss: 0.460196171220574\n",
      "iteration 4526train_loss: 0.4268747095140326 test_loss: 0.46019634489009087\n",
      "iteration 4527train_loss: 0.4268747089954639 test_loss: 0.4601965184561243\n",
      "iteration 4528train_loss: 0.42687470847751635 test_loss: 0.4601966919187356\n",
      "iteration 4529train_loss: 0.4268747079601894 test_loss: 0.4601968652779862\n",
      "iteration 4530train_loss: 0.42687470744348216 test_loss: 0.4601970385339375\n",
      "iteration 4531train_loss: 0.42687470692739393 test_loss: 0.46019721168665084\n",
      "iteration 4532train_loss: 0.42687470641192404 test_loss: 0.4601973847361876\n",
      "iteration 4533train_loss: 0.42687470589707177 test_loss: 0.4601975576826091\n",
      "iteration 4534train_loss: 0.4268747053828362 test_loss: 0.46019773052597657\n",
      "iteration 4535train_loss: 0.42687470486921675 test_loss: 0.46019790326635107\n",
      "iteration 4536train_loss: 0.42687470435621255 test_loss: 0.46019807590379397\n",
      "iteration 4537train_loss: 0.426874703843823 test_loss: 0.4601982484383664\n",
      "iteration 4538train_loss: 0.42687470333204736 test_loss: 0.4601984208701293\n",
      "iteration 4539train_loss: 0.4268747028208847 test_loss: 0.4601985931991439\n",
      "iteration 4540train_loss: 0.4268747023103344 test_loss: 0.4601987654254712\n",
      "iteration 4541train_loss: 0.42687470180039583 test_loss: 0.46019893754917207\n",
      "iteration 4542train_loss: 0.42687470129106825 test_loss: 0.4601991095703078\n",
      "iteration 4543train_loss: 0.42687470078235074 test_loss: 0.460199281488939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4544train_loss: 0.4268747002742427 test_loss: 0.4601994533051267\n",
      "iteration 4545train_loss: 0.42687469976674336 test_loss: 0.46019962501893175\n",
      "iteration 4546train_loss: 0.42687469925985194 test_loss: 0.4601997966304149\n",
      "iteration 4547train_loss: 0.426874698753568 test_loss: 0.46019996813963693\n",
      "iteration 4548train_loss: 0.42687469824789026 test_loss: 0.4602001395466588\n",
      "iteration 4549train_loss: 0.4268746977428186 test_loss: 0.4602003108515409\n",
      "iteration 4550train_loss: 0.4268746972383518 test_loss: 0.4602004820543441\n",
      "iteration 4551train_loss: 0.42687469673448947 test_loss: 0.4602006531551291\n",
      "iteration 4552train_loss: 0.4268746962312307 test_loss: 0.46020082415395636\n",
      "iteration 4553train_loss: 0.42687469572857484 test_loss: 0.4602009950508865\n",
      "iteration 4554train_loss: 0.42687469522652116 test_loss: 0.4602011658459801\n",
      "iteration 4555train_loss: 0.42687469472506884 test_loss: 0.46020133653929757\n",
      "iteration 4556train_loss: 0.42687469422421737 test_loss: 0.4602015071308994\n",
      "iteration 4557train_loss: 0.4268746937239658 test_loss: 0.4602016776208462\n",
      "iteration 4558train_loss: 0.42687469322431365 test_loss: 0.460201848009198\n",
      "iteration 4559train_loss: 0.4268746927252599 test_loss: 0.4602020182960155\n",
      "iteration 4560train_loss: 0.42687469222680413 test_loss: 0.46020218848135885\n",
      "iteration 4561train_loss: 0.4268746917289455 test_loss: 0.4602023585652883\n",
      "iteration 4562train_loss: 0.4268746912316831 test_loss: 0.4602025285478642\n",
      "iteration 4563train_loss: 0.4268746907350165 test_loss: 0.4602026984291468\n",
      "iteration 4564train_loss: 0.426874690238945 test_loss: 0.4602028682091961\n",
      "iteration 4565train_loss: 0.42687468974346754 test_loss: 0.46020303788807243\n",
      "iteration 4566train_loss: 0.42687468924858385 test_loss: 0.46020320746583576\n",
      "iteration 4567train_loss: 0.4268746887542929 test_loss: 0.46020337694254626\n",
      "iteration 4568train_loss: 0.42687468826059405 test_loss: 0.4602035463182639\n",
      "iteration 4569train_loss: 0.42687468776748677 test_loss: 0.46020371559304873\n",
      "iteration 4570train_loss: 0.42687468727497 test_loss: 0.4602038847669607\n",
      "iteration 4571train_loss: 0.42687468678304324 test_loss: 0.46020405384005963\n",
      "iteration 4572train_loss: 0.4268746862917059 test_loss: 0.46020422281240575\n",
      "iteration 4573train_loss: 0.426874685800957 test_loss: 0.4602043916840585\n",
      "iteration 4574train_loss: 0.4268746853107962 test_loss: 0.46020456045507796\n",
      "iteration 4575train_loss: 0.4268746848212225 test_loss: 0.4602047291255239\n",
      "iteration 4576train_loss: 0.42687468433223524 test_loss: 0.4602048976954559\n",
      "iteration 4577train_loss: 0.42687468384383376 test_loss: 0.460205066164934\n",
      "iteration 4578train_loss: 0.4268746833560172 test_loss: 0.4602052345340175\n",
      "iteration 4579train_loss: 0.42687468286878516 test_loss: 0.46020540280276645\n",
      "iteration 4580train_loss: 0.4268746823821367 test_loss: 0.46020557097124015\n",
      "iteration 4581train_loss: 0.42687468189607125 test_loss: 0.4602057390394983\n",
      "iteration 4582train_loss: 0.426874681410588 test_loss: 0.4602059070076004\n",
      "iteration 4583train_loss: 0.4268746809256864 test_loss: 0.460206074875606\n",
      "iteration 4584train_loss: 0.4268746804413656 test_loss: 0.4602062426435747\n",
      "iteration 4585train_loss: 0.426874679957625 test_loss: 0.4602064103115656\n",
      "iteration 4586train_loss: 0.4268746794744639 test_loss: 0.46020657787963837\n",
      "iteration 4587train_loss: 0.42687467899188153 test_loss: 0.46020674534785233\n",
      "iteration 4588train_loss: 0.42687467850987726 test_loss: 0.46020691271626685\n",
      "iteration 4589train_loss: 0.42687467802845036 test_loss: 0.4602070799849411\n",
      "iteration 4590train_loss: 0.4268746775476001 test_loss: 0.4602072471539345\n",
      "iteration 4591train_loss: 0.42687467706732585 test_loss: 0.4602074142233062\n",
      "iteration 4592train_loss: 0.42687467658762696 test_loss: 0.4602075811931153\n",
      "iteration 4593train_loss: 0.42687467610850266 test_loss: 0.46020774806342113\n",
      "iteration 4594train_loss: 0.4268746756299523 test_loss: 0.4602079148342827\n",
      "iteration 4595train_loss: 0.4268746751519752 test_loss: 0.4602080815057592\n",
      "iteration 4596train_loss: 0.42687467467457074 test_loss: 0.4602082480779096\n",
      "iteration 4597train_loss: 0.42687467419773806 test_loss: 0.4602084145507929\n",
      "iteration 4598train_loss: 0.4268746737214765 test_loss: 0.46020858092446815\n",
      "iteration 4599train_loss: 0.4268746732457855 test_loss: 0.4602087471989943\n",
      "iteration 4600train_loss: 0.4268746727706643 test_loss: 0.46020891337443015\n",
      "iteration 4601train_loss: 0.42687467229611226 test_loss: 0.46020907945083483\n",
      "iteration 4602train_loss: 0.42687467182212857 test_loss: 0.4602092454282669\n",
      "iteration 4603train_loss: 0.42687467134871276 test_loss: 0.46020941130678544\n",
      "iteration 4604train_loss: 0.426874670875864 test_loss: 0.4602095770864489\n",
      "iteration 4605train_loss: 0.4268746704035816 test_loss: 0.4602097427673162\n",
      "iteration 4606train_loss: 0.4268746699318649 test_loss: 0.46020990834944625\n",
      "iteration 4607train_loss: 0.42687466946071334 test_loss: 0.46021007383289747\n",
      "iteration 4608train_loss: 0.4268746689901261 test_loss: 0.4602102392177285\n",
      "iteration 4609train_loss: 0.4268746685201025 test_loss: 0.46021040450399814\n",
      "iteration 4610train_loss: 0.426874668050642 test_loss: 0.4602105696917646\n",
      "iteration 4611train_loss: 0.42687466758174375 test_loss: 0.46021073478108676\n",
      "iteration 4612train_loss: 0.42687466711340716 test_loss: 0.460210899772023\n",
      "iteration 4613train_loss: 0.42687466664563156 test_loss: 0.46021106466463174\n",
      "iteration 4614train_loss: 0.4268746661784163 test_loss: 0.4602112294589715\n",
      "iteration 4615train_loss: 0.4268746657117607 test_loss: 0.4602113941551006\n",
      "iteration 4616train_loss: 0.42687466524566414 test_loss: 0.46021155875307745\n",
      "iteration 4617train_loss: 0.4268746647801257 test_loss: 0.46021172325296034\n",
      "iteration 4618train_loss: 0.426874664315145 test_loss: 0.46021188765480764\n",
      "iteration 4619train_loss: 0.4268746638507213 test_loss: 0.4602120519586775\n",
      "iteration 4620train_loss: 0.4268746633868538 test_loss: 0.4602122161646282\n",
      "iteration 4621train_loss: 0.42687466292354187 test_loss: 0.46021238027271794\n",
      "iteration 4622train_loss: 0.4268746624607851 test_loss: 0.46021254428300484\n",
      "iteration 4623train_loss: 0.4268746619985825 test_loss: 0.4602127081955471\n",
      "iteration 4624train_loss: 0.4268746615369335 test_loss: 0.4602128720104027\n",
      "iteration 4625train_loss: 0.42687466107583755 test_loss: 0.4602130357276298\n",
      "iteration 4626train_loss: 0.4268746606152938 test_loss: 0.46021319934728633\n",
      "iteration 4627train_loss: 0.4268746601553017 test_loss: 0.4602133628694303\n",
      "iteration 4628train_loss: 0.42687465969586064 test_loss: 0.46021352629411966\n",
      "iteration 4629train_loss: 0.4268746592369699 test_loss: 0.46021368962141246\n",
      "iteration 4630train_loss: 0.42687465877862873 test_loss: 0.46021385285136635\n",
      "iteration 4631train_loss: 0.42687465832083654 test_loss: 0.4602140159840393\n",
      "iteration 4632train_loss: 0.42687465786359274 test_loss: 0.4602141790194892\n",
      "iteration 4633train_loss: 0.4268746574068967 test_loss: 0.4602143419577737\n",
      "iteration 4634train_loss: 0.4268746569507475 test_loss: 0.46021450479895054\n",
      "iteration 4635train_loss: 0.42687465649514483 test_loss: 0.46021466754307755\n",
      "iteration 4636train_loss: 0.42687465604008784 test_loss: 0.46021483019021225\n",
      "iteration 4637train_loss: 0.42687465558557586 test_loss: 0.4602149927404126\n",
      "iteration 4638train_loss: 0.42687465513160827 test_loss: 0.46021515519373574\n",
      "iteration 4639train_loss: 0.42687465467818436 test_loss: 0.46021531755023953\n",
      "iteration 4640train_loss: 0.42687465422530363 test_loss: 0.46021547980998156\n",
      "iteration 4641train_loss: 0.42687465377296535 test_loss: 0.46021564197301923\n",
      "iteration 4642train_loss: 0.4268746533211688 test_loss: 0.4602158040394101\n",
      "iteration 4643train_loss: 0.42687465286991344 test_loss: 0.4602159660092114\n",
      "iteration 4644train_loss: 0.42687465241919864 test_loss: 0.46021612788248073\n",
      "iteration 4645train_loss: 0.42687465196902363 test_loss: 0.4602162896592756\n",
      "iteration 4646train_loss: 0.4268746515193878 test_loss: 0.46021645133965294\n",
      "iteration 4647train_loss: 0.4268746510702905 test_loss: 0.4602166129236704\n",
      "iteration 4648train_loss: 0.42687465062173113 test_loss: 0.46021677441138503\n",
      "iteration 4649train_loss: 0.426874650173709 test_loss: 0.4602169358028543\n",
      "iteration 4650train_loss: 0.4268746497262235 test_loss: 0.46021709709813513\n",
      "iteration 4651train_loss: 0.42687464927927393 test_loss: 0.46021725829728494\n",
      "iteration 4652train_loss: 0.42687464883285975 test_loss: 0.4602174194003608\n",
      "iteration 4653train_loss: 0.42687464838698025 test_loss: 0.46021758040741967\n",
      "iteration 4654train_loss: 0.42687464794163477 test_loss: 0.46021774131851884\n",
      "iteration 4655train_loss: 0.4268746474968227 test_loss: 0.4602179021337152\n",
      "iteration 4656train_loss: 0.4268746470525433 test_loss: 0.4602180628530659\n",
      "iteration 4657train_loss: 0.4268746466087962 test_loss: 0.4602182234766276\n",
      "iteration 4658train_loss: 0.42687464616558046 test_loss: 0.4602183840044576\n",
      "iteration 4659train_loss: 0.42687464572289563 test_loss: 0.4602185444366125\n",
      "iteration 4660train_loss: 0.4268746452807409 test_loss: 0.4602187047731494\n",
      "iteration 4661train_loss: 0.42687464483911586 test_loss: 0.460218865014125\n",
      "iteration 4662train_loss: 0.4268746443980197 test_loss: 0.4602190251595961\n",
      "iteration 4663train_loss: 0.4268746439574519 test_loss: 0.4602191852096196\n",
      "iteration 4664train_loss: 0.4268746435174117 test_loss: 0.46021934516425195\n",
      "iteration 4665train_loss: 0.42687464307789863 test_loss: 0.46021950502355025\n",
      "iteration 4666train_loss: 0.42687464263891184 test_loss: 0.4602196647875709\n",
      "iteration 4667train_loss: 0.4268746422004509 test_loss: 0.4602198244563705\n",
      "iteration 4668train_loss: 0.42687464176251505 test_loss: 0.4602199840300058\n",
      "iteration 4669train_loss: 0.4268746413251039 test_loss: 0.46022014350853324\n",
      "iteration 4670train_loss: 0.4268746408882164 test_loss: 0.4602203028920095\n",
      "iteration 4671train_loss: 0.4268746404518523 test_loss: 0.4602204621804909\n",
      "iteration 4672train_loss: 0.4268746400160107 test_loss: 0.46022062137403397\n",
      "iteration 4673train_loss: 0.4268746395806912 test_loss: 0.46022078047269527\n",
      "iteration 4674train_loss: 0.42687463914589296 test_loss: 0.46022093947653114\n",
      "iteration 4675train_loss: 0.42687463871161563 test_loss: 0.4602210983855978\n",
      "iteration 4676train_loss: 0.4268746382778584 test_loss: 0.46022125719995177\n",
      "iteration 4677train_loss: 0.42687463784462054 test_loss: 0.4602214159196492\n",
      "iteration 4678train_loss: 0.4268746374119016 test_loss: 0.46022157454474644\n",
      "iteration 4679train_loss: 0.42687463697970107 test_loss: 0.46022173307529973\n",
      "iteration 4680train_loss: 0.426874636548018 test_loss: 0.4602218915113652\n",
      "iteration 4681train_loss: 0.426874636116852 test_loss: 0.46022204985299914\n",
      "iteration 4682train_loss: 0.42687463568620243 test_loss: 0.4602222081002576\n",
      "iteration 4683train_loss: 0.42687463525606856 test_loss: 0.4602223662531966\n",
      "iteration 4684train_loss: 0.42687463482644983 test_loss: 0.4602225243118724\n",
      "iteration 4685train_loss: 0.4268746343973458 test_loss: 0.460222682276341\n",
      "iteration 4686train_loss: 0.4268746339687554 test_loss: 0.46022284014665826\n",
      "iteration 4687train_loss: 0.4268746335406785 test_loss: 0.4602229979228802\n",
      "iteration 4688train_loss: 0.42687463311311424 test_loss: 0.4602231556050627\n",
      "iteration 4689train_loss: 0.42687463268606207 test_loss: 0.46022331319326193\n",
      "iteration 4690train_loss: 0.4268746322595214 test_loss: 0.46022347068753344\n",
      "iteration 4691train_loss: 0.42687463183349145 test_loss: 0.46022362808793327\n",
      "iteration 4692train_loss: 0.4268746314079718 test_loss: 0.4602237853945171\n",
      "iteration 4693train_loss: 0.42687463098296186 test_loss: 0.4602239426073407\n",
      "iteration 4694train_loss: 0.4268746305584608 test_loss: 0.46022409972645995\n",
      "iteration 4695train_loss: 0.4268746301344683 test_loss: 0.46022425675193046\n",
      "iteration 4696train_loss: 0.42687462971098333 test_loss: 0.4602244136838079\n",
      "iteration 4697train_loss: 0.4268746292880056 test_loss: 0.4602245705221479\n",
      "iteration 4698train_loss: 0.42687462886553457 test_loss: 0.46022472726700614\n",
      "iteration 4699train_loss: 0.42687462844356944 test_loss: 0.4602248839184381\n",
      "iteration 4700train_loss: 0.4268746280221096 test_loss: 0.46022504047649926\n",
      "iteration 4701train_loss: 0.42687462760115463 test_loss: 0.4602251969412454\n",
      "iteration 4702train_loss: 0.4268746271807038 test_loss: 0.4602253533127317\n",
      "iteration 4703train_loss: 0.42687462676075644 test_loss: 0.46022550959101377\n",
      "iteration 4704train_loss: 0.42687462634131196 test_loss: 0.460225665776147\n",
      "iteration 4705train_loss: 0.42687462592236985 test_loss: 0.46022582186818667\n",
      "iteration 4706train_loss: 0.4268746255039296 test_loss: 0.46022597786718816\n",
      "iteration 4707train_loss: 0.4268746250859904 test_loss: 0.460226133773207\n",
      "iteration 4708train_loss: 0.42687462466855164 test_loss: 0.4602262895862981\n",
      "iteration 4709train_loss: 0.4268746242516128 test_loss: 0.4602264453065169\n",
      "iteration 4710train_loss: 0.42687462383517333 test_loss: 0.4602266009339187\n",
      "iteration 4711train_loss: 0.4268746234192326 test_loss: 0.46022675646855854\n",
      "iteration 4712train_loss: 0.42687462300379003 test_loss: 0.46022691191049164\n",
      "iteration 4713train_loss: 0.42687462258884495 test_loss: 0.4602270672597731\n",
      "iteration 4714train_loss: 0.42687462217439676 test_loss: 0.460227222516458\n",
      "iteration 4715train_loss: 0.426874621760445 test_loss: 0.46022737768060124\n",
      "iteration 4716train_loss: 0.42687462134698895 test_loss: 0.4602275327522581\n",
      "iteration 4717train_loss: 0.426874620934028 test_loss: 0.4602276877314835\n",
      "iteration 4718train_loss: 0.42687462052156167 test_loss: 0.46022784261833233\n",
      "iteration 4719train_loss: 0.4268746201095892 test_loss: 0.46022799741285947\n",
      "iteration 4720train_loss: 0.4268746196981102 test_loss: 0.46022815211511986\n",
      "iteration 4721train_loss: 0.426874619287124 test_loss: 0.46022830672516846\n",
      "iteration 4722train_loss: 0.4268746188766299 test_loss: 0.4602284612430598\n",
      "iteration 4723train_loss: 0.4268746184666274 test_loss: 0.4602286156688491\n",
      "iteration 4724train_loss: 0.42687461805711596 test_loss: 0.4602287700025907\n",
      "iteration 4725train_loss: 0.4268746176480949 test_loss: 0.46022892424433964\n",
      "iteration 4726train_loss: 0.4268746172395636 test_loss: 0.4602290783941504\n",
      "iteration 4727train_loss: 0.42687461683152167 test_loss: 0.4602292324520777\n",
      "iteration 4728train_loss: 0.42687461642396834 test_loss: 0.46022938641817634\n",
      "iteration 4729train_loss: 0.42687461601690313 test_loss: 0.46022954029250057\n",
      "iteration 4730train_loss: 0.4268746156103253 test_loss: 0.4602296940751054\n",
      "iteration 4731train_loss: 0.4268746152042345 test_loss: 0.460229847766045\n",
      "iteration 4732train_loss: 0.42687461479862987 test_loss: 0.460230001365374\n",
      "iteration 4733train_loss: 0.426874614393511 test_loss: 0.4602301548731469\n",
      "iteration 4734train_loss: 0.42687461398887727 test_loss: 0.460230308289418\n",
      "iteration 4735train_loss: 0.42687461358472817 test_loss: 0.46023046161424197\n",
      "iteration 4736train_loss: 0.42687461318106296 test_loss: 0.46023061484767297\n",
      "iteration 4737train_loss: 0.42687461277788125 test_loss: 0.46023076798976537\n",
      "iteration 4738train_loss: 0.4268746123751823 test_loss: 0.46023092104057345\n",
      "iteration 4739train_loss: 0.42687461197296567 test_loss: 0.4602310740001517\n",
      "iteration 4740train_loss: 0.4268746115712307 test_loss: 0.460231226868554\n",
      "iteration 4741train_loss: 0.4268746111699767 test_loss: 0.46023137964583494\n",
      "iteration 4742train_loss: 0.42687461076920336 test_loss: 0.4602315323320484\n",
      "iteration 4743train_loss: 0.42687461036890983 test_loss: 0.4602316849272487\n",
      "iteration 4744train_loss: 0.4268746099690957 test_loss: 0.46023183743148993\n",
      "iteration 4745train_loss: 0.4268746095697604 test_loss: 0.46023198984482605\n",
      "iteration 4746train_loss: 0.42687460917090325 test_loss: 0.46023214216731134\n",
      "iteration 4747train_loss: 0.4268746087725237 test_loss: 0.46023229439899965\n",
      "iteration 4748train_loss: 0.4268746083746213 test_loss: 0.460232446539945\n",
      "iteration 4749train_loss: 0.4268746079771954 test_loss: 0.4602325985902014\n",
      "iteration 4750train_loss: 0.4268746075802454 test_loss: 0.4602327505498227\n",
      "iteration 4751train_loss: 0.4268746071837708 test_loss: 0.46023290241886294\n",
      "iteration 4752train_loss: 0.42687460678777084 test_loss: 0.46023305419737576\n",
      "iteration 4753train_loss: 0.4268746063922452 test_loss: 0.4602332058854151\n",
      "iteration 4754train_loss: 0.4268746059971931 test_loss: 0.4602333574830347\n",
      "iteration 4755train_loss: 0.42687460560261425 test_loss: 0.4602335089902885\n",
      "iteration 4756train_loss: 0.4268746052085078 test_loss: 0.46023366040723007\n",
      "iteration 4757train_loss: 0.4268746048148732 test_loss: 0.46023381173391315\n",
      "iteration 4758train_loss: 0.42687460442171016 test_loss: 0.4602339629703915\n",
      "iteration 4759train_loss: 0.4268746040290178 test_loss: 0.4602341141167186\n",
      "iteration 4760train_loss: 0.42687460363679564 test_loss: 0.46023426517294813\n",
      "iteration 4761train_loss: 0.4268746032450434 test_loss: 0.46023441613913374\n",
      "iteration 4762train_loss: 0.42687460285376 test_loss: 0.46023456701532883\n",
      "iteration 4763train_loss: 0.4268746024629453 test_loss: 0.46023471780158703\n",
      "iteration 4764train_loss: 0.4268746020725985 test_loss: 0.4602348684979618\n",
      "iteration 4765train_loss: 0.42687460168271923 test_loss: 0.4602350191045066\n",
      "iteration 4766train_loss: 0.42687460129330673 test_loss: 0.4602351696212747\n",
      "iteration 4767train_loss: 0.4268746009043606 test_loss: 0.4602353200483197\n",
      "iteration 4768train_loss: 0.42687460051588016 test_loss: 0.4602354703856949\n",
      "iteration 4769train_loss: 0.42687460012786504 test_loss: 0.4602356206334534\n",
      "iteration 4770train_loss: 0.4268745997403144 test_loss: 0.46023577079164885\n",
      "iteration 4771train_loss: 0.42687459935322797 test_loss: 0.46023592086033427\n",
      "iteration 4772train_loss: 0.42687459896660496 test_loss: 0.46023607083956297\n",
      "iteration 4773train_loss: 0.42687459858044496 test_loss: 0.4602362207293881\n",
      "iteration 4774train_loss: 0.4268745981947474 test_loss: 0.46023637052986294\n",
      "iteration 4775train_loss: 0.4268745978095116 test_loss: 0.4602365202410404\n",
      "iteration 4776train_loss: 0.42687459742473716 test_loss: 0.4602366698629738\n",
      "iteration 4777train_loss: 0.42687459704042346 test_loss: 0.46023681939571615\n",
      "iteration 4778train_loss: 0.42687459665656996 test_loss: 0.4602369688393204\n",
      "iteration 4779train_loss: 0.42687459627317614 test_loss: 0.46023711819383967\n",
      "iteration 4780train_loss: 0.4268745958902413 test_loss: 0.460237267459327\n",
      "iteration 4781train_loss: 0.42687459550776513 test_loss: 0.4602374166358351\n",
      "iteration 4782train_loss: 0.42687459512574694 test_loss: 0.4602375657234171\n",
      "iteration 4783train_loss: 0.4268745947441861 test_loss: 0.46023771472212577\n",
      "iteration 4784train_loss: 0.42687459436308217 test_loss: 0.460237863632014\n",
      "iteration 4785train_loss: 0.42687459398243455 test_loss: 0.4602380124531346\n",
      "iteration 4786train_loss: 0.42687459360224284 test_loss: 0.4602381611855405\n",
      "iteration 4787train_loss: 0.4268745932225063 test_loss: 0.46023830982928415\n",
      "iteration 4788train_loss: 0.42687459284322454 test_loss: 0.46023845838441857\n",
      "iteration 4789train_loss: 0.4268745924643969 test_loss: 0.46023860685099627\n",
      "iteration 4790train_loss: 0.42687459208602285 test_loss: 0.4602387552290701\n",
      "iteration 4791train_loss: 0.4268745917081019 test_loss: 0.4602389035186925\n",
      "iteration 4792train_loss: 0.4268745913306334 test_loss: 0.46023905171991614\n",
      "iteration 4793train_loss: 0.426874590953617 test_loss: 0.4602391998327936\n",
      "iteration 4794train_loss: 0.42687459057705196 test_loss: 0.46023934785737747\n",
      "iteration 4795train_loss: 0.4268745902009379 test_loss: 0.46023949579372025\n",
      "iteration 4796train_loss: 0.42687458982527415 test_loss: 0.4602396436418743\n",
      "iteration 4797train_loss: 0.42687458945006024 test_loss: 0.46023979140189236\n",
      "iteration 4798train_loss: 0.4268745890752956 test_loss: 0.4602399390738265\n",
      "iteration 4799train_loss: 0.42687458870097966 test_loss: 0.4602400866577292\n",
      "iteration 4800train_loss: 0.426874588327112 test_loss: 0.4602402341536529\n",
      "iteration 4801train_loss: 0.426874587953692 test_loss: 0.46024038156164987\n",
      "iteration 4802train_loss: 0.4268745875807191 test_loss: 0.4602405288817725\n",
      "iteration 4803train_loss: 0.4268745872081927 test_loss: 0.46024067611407293\n",
      "iteration 4804train_loss: 0.42687458683611257 test_loss: 0.4602408232586035\n",
      "iteration 4805train_loss: 0.4268745864644778 test_loss: 0.4602409703154163\n",
      "iteration 4806train_loss: 0.4268745860932881 test_loss: 0.4602411172845635\n",
      "iteration 4807train_loss: 0.4268745857225427 test_loss: 0.4602412641660974\n",
      "iteration 4808train_loss: 0.42687458535224143 test_loss: 0.46024141096006993\n",
      "iteration 4809train_loss: 0.42687458498238345 test_loss: 0.46024155766653324\n",
      "iteration 4810train_loss: 0.4268745846129683 test_loss: 0.4602417042855394\n",
      "iteration 4811train_loss: 0.4268745842439956 test_loss: 0.4602418508171403\n",
      "iteration 4812train_loss: 0.4268745838754646 test_loss: 0.4602419972613881\n",
      "iteration 4813train_loss: 0.42687458350737495 test_loss: 0.4602421436183347\n",
      "iteration 4814train_loss: 0.4268745831397259 test_loss: 0.4602422898880319\n",
      "iteration 4815train_loss: 0.42687458277251716 test_loss: 0.4602424360705317\n",
      "iteration 4816train_loss: 0.4268745824057481 test_loss: 0.46024258216588604\n",
      "iteration 4817train_loss: 0.4268745820394182 test_loss: 0.46024272817414663\n",
      "iteration 4818train_loss: 0.4268745816735269 test_loss: 0.46024287409536535\n",
      "iteration 4819train_loss: 0.4268745813080737 test_loss: 0.4602430199295939\n",
      "iteration 4820train_loss: 0.426874580943058 test_loss: 0.46024316567688406\n",
      "iteration 4821train_loss: 0.4268745805784794 test_loss: 0.4602433113372875\n",
      "iteration 4822train_loss: 0.42687458021433744 test_loss: 0.460243456910856\n",
      "iteration 4823train_loss: 0.4268745798506313 test_loss: 0.460243602397641\n",
      "iteration 4824train_loss: 0.42687457948736074 test_loss: 0.4602437477976942\n",
      "iteration 4825train_loss: 0.4268745791245252 test_loss: 0.4602438931110675\n",
      "iteration 4826train_loss: 0.42687457876212404 test_loss: 0.46024403833781197\n",
      "iteration 4827train_loss: 0.4268745784001567 test_loss: 0.4602441834779796\n",
      "iteration 4828train_loss: 0.4268745780386229 test_loss: 0.4602443285316214\n",
      "iteration 4829train_loss: 0.42687457767752196 test_loss: 0.4602444734987892\n",
      "iteration 4830train_loss: 0.42687457731685324 test_loss: 0.46024461837953434\n",
      "iteration 4831train_loss: 0.42687457695661657 test_loss: 0.46024476317390817\n",
      "iteration 4832train_loss: 0.4268745765968111 test_loss: 0.46024490788196204\n",
      "iteration 4833train_loss: 0.4268745762374364 test_loss: 0.4602450525037475\n",
      "iteration 4834train_loss: 0.42687457587849204 test_loss: 0.4602451970393156\n",
      "iteration 4835train_loss: 0.4268745755199775 test_loss: 0.46024534148871776\n",
      "iteration 4836train_loss: 0.42687457516189214 test_loss: 0.46024548585200525\n",
      "iteration 4837train_loss: 0.4268745748042356 test_loss: 0.4602456301292292\n",
      "iteration 4838train_loss: 0.42687457444700716 test_loss: 0.4602457743204409\n",
      "iteration 4839train_loss: 0.4268745740902065 test_loss: 0.4602459184256914\n",
      "iteration 4840train_loss: 0.4268745737338331 test_loss: 0.4602460624450319\n",
      "iteration 4841train_loss: 0.42687457337788637 test_loss: 0.46024620637851354\n",
      "iteration 4842train_loss: 0.42687457302236576 test_loss: 0.4602463502261873\n",
      "iteration 4843train_loss: 0.4268745726672708 test_loss: 0.46024649398810447\n",
      "iteration 4844train_loss: 0.426874572312601 test_loss: 0.4602466376643156\n",
      "iteration 4845train_loss: 0.42687457195835593 test_loss: 0.4602467812548721\n",
      "iteration 4846train_loss: 0.42687457160453496 test_loss: 0.46024692475982476\n",
      "iteration 4847train_loss: 0.4268745712511376 test_loss: 0.46024706817922445\n",
      "iteration 4848train_loss: 0.4268745708981634 test_loss: 0.4602472115131221\n",
      "iteration 4849train_loss: 0.42687457054561173 test_loss: 0.46024735476156853\n",
      "iteration 4850train_loss: 0.4268745701934822 test_loss: 0.4602474979246147\n",
      "iteration 4851train_loss: 0.42687456984177435 test_loss: 0.4602476410023113\n",
      "iteration 4852train_loss: 0.42687456949048747 test_loss: 0.4602477839947091\n",
      "iteration 4853train_loss: 0.42687456913962124 test_loss: 0.46024792690185895\n",
      "iteration 4854train_loss: 0.4268745687891751 test_loss: 0.4602480697238113\n",
      "iteration 4855train_loss: 0.4268745684391485 test_loss: 0.4602482124606172\n",
      "iteration 4856train_loss: 0.426874568089541 test_loss: 0.46024835511232703\n",
      "iteration 4857train_loss: 0.42687456774035215 test_loss: 0.46024849767899145\n",
      "iteration 4858train_loss: 0.4268745673915813 test_loss: 0.46024864016066125\n",
      "iteration 4859train_loss: 0.426874567043228 test_loss: 0.4602487825573866\n",
      "iteration 4860train_loss: 0.42687456669529183 test_loss: 0.4602489248692184\n",
      "iteration 4861train_loss: 0.42687456634777216 test_loss: 0.46024906709620705\n",
      "iteration 4862train_loss: 0.42687456600066875 test_loss: 0.46024920923840285\n",
      "iteration 4863train_loss: 0.42687456565398063 test_loss: 0.4602493512958565\n",
      "iteration 4864train_loss: 0.42687456530770784 test_loss: 0.4602494932686182\n",
      "iteration 4865train_loss: 0.4268745649618495 test_loss: 0.4602496351567385\n",
      "iteration 4866train_loss: 0.4268745646164052 test_loss: 0.4602497769602675\n",
      "iteration 4867train_loss: 0.4268745642713746 test_loss: 0.46024991867925585\n",
      "iteration 4868train_loss: 0.42687456392675704 test_loss: 0.46025006031375365\n",
      "iteration 4869train_loss: 0.42687456358255205 test_loss: 0.4602502018638111\n",
      "iteration 4870train_loss: 0.4268745632387592 test_loss: 0.4602503433294787\n",
      "iteration 4871train_loss: 0.4268745628953779 test_loss: 0.4602504847108064\n",
      "iteration 4872train_loss: 0.4268745625524077 test_loss: 0.46025062600784433\n",
      "iteration 4873train_loss: 0.42687456220984826 test_loss: 0.4602507672206429\n",
      "iteration 4874train_loss: 0.4268745618676988 test_loss: 0.46025090834925203\n",
      "iteration 4875train_loss: 0.426874561525959 test_loss: 0.4602510493937218\n",
      "iteration 4876train_loss: 0.4268745611846284 test_loss: 0.4602511903541024\n",
      "iteration 4877train_loss: 0.4268745608437064 test_loss: 0.46025133123044376\n",
      "iteration 4878train_loss: 0.4268745605031926 test_loss: 0.46025147202279587\n",
      "iteration 4879train_loss: 0.4268745601630863 test_loss: 0.46025161273120874\n",
      "iteration 4880train_loss: 0.42687455982338746 test_loss: 0.4602517533557323\n",
      "iteration 4881train_loss: 0.42687455948409514 test_loss: 0.4602518938964164\n",
      "iteration 4882train_loss: 0.42687455914520905 test_loss: 0.4602520343533109\n",
      "iteration 4883train_loss: 0.42687455880672875 test_loss: 0.4602521747264658\n",
      "iteration 4884train_loss: 0.4268745584686537 test_loss: 0.4602523150159308\n",
      "iteration 4885train_loss: 0.42687455813098324 test_loss: 0.4602524552217556\n",
      "iteration 4886train_loss: 0.4268745577937172 test_loss: 0.46025259534399027\n",
      "iteration 4887train_loss: 0.4268745574568549 test_loss: 0.4602527353826842\n",
      "iteration 4888train_loss: 0.42687455712039585 test_loss: 0.4602528753378872\n",
      "iteration 4889train_loss: 0.42687455678433955 test_loss: 0.46025301520964906\n",
      "iteration 4890train_loss: 0.42687455644868566 test_loss: 0.46025315499801933\n",
      "iteration 4891train_loss: 0.42687455611343356 test_loss: 0.4602532947030476\n",
      "iteration 4892train_loss: 0.426874555778583 test_loss: 0.46025343432478344\n",
      "iteration 4893train_loss: 0.4268745554441333 test_loss: 0.4602535738632765\n",
      "iteration 4894train_loss: 0.4268745551100838 test_loss: 0.46025371331857623\n",
      "iteration 4895train_loss: 0.4268745547764343 test_loss: 0.4602538526907322\n",
      "iteration 4896train_loss: 0.42687455444318434 test_loss: 0.4602539919797937\n",
      "iteration 4897train_loss: 0.42687455411033315 test_loss: 0.4602541311858103\n",
      "iteration 4898train_loss: 0.42687455377788064 test_loss: 0.46025427030883154\n",
      "iteration 4899train_loss: 0.42687455344582614 test_loss: 0.46025440934890643\n",
      "iteration 4900train_loss: 0.42687455311416916 test_loss: 0.46025454830608475\n",
      "iteration 4901train_loss: 0.42687455278290903 test_loss: 0.46025468718041557\n",
      "iteration 4902train_loss: 0.4268745524520457 test_loss: 0.4602548259719481\n",
      "iteration 4903train_loss: 0.4268745521215784 test_loss: 0.46025496468073185\n",
      "iteration 4904train_loss: 0.42687455179150674 test_loss: 0.46025510330681585\n",
      "iteration 4905train_loss: 0.42687455146183023 test_loss: 0.4602552418502494\n",
      "iteration 4906train_loss: 0.42687455113254835 test_loss: 0.4602553803110816\n",
      "iteration 4907train_loss: 0.4268745508036607 test_loss: 0.46025551868936165\n",
      "iteration 4908train_loss: 0.4268745504751668 test_loss: 0.46025565698513865\n",
      "iteration 4909train_loss: 0.4268745501470662 test_loss: 0.4602557951984618\n",
      "iteration 4910train_loss: 0.42687454981935835 test_loss: 0.4602559333293801\n",
      "iteration 4911train_loss: 0.4268745494920428 test_loss: 0.4602560713779424\n",
      "iteration 4912train_loss: 0.42687454916511913 test_loss: 0.46025620934419775\n",
      "iteration 4913train_loss: 0.4268745488385867 test_loss: 0.4602563472281954\n",
      "iteration 4914train_loss: 0.4268745485124454 test_loss: 0.46025648502998406\n",
      "iteration 4915train_loss: 0.42687454818669446 test_loss: 0.46025662274961265\n",
      "iteration 4916train_loss: 0.4268745478613335 test_loss: 0.4602567603871301\n",
      "iteration 4917train_loss: 0.42687454753636206 test_loss: 0.46025689794258523\n",
      "iteration 4918train_loss: 0.42687454721177953 test_loss: 0.4602570354160269\n",
      "iteration 4919train_loss: 0.42687454688758575 test_loss: 0.4602571728075038\n",
      "iteration 4920train_loss: 0.4268745465637799 test_loss: 0.4602573101170649\n",
      "iteration 4921train_loss: 0.4268745462403618 test_loss: 0.4602574473447587\n",
      "iteration 4922train_loss: 0.4268745459173309 test_loss: 0.46025758449063425\n",
      "iteration 4923train_loss: 0.42687454559468674 test_loss: 0.46025772155473993\n",
      "iteration 4924train_loss: 0.42687454527242874 test_loss: 0.4602578585371245\n",
      "iteration 4925train_loss: 0.42687454495055654 test_loss: 0.46025799543783646\n",
      "iteration 4926train_loss: 0.4268745446290697 test_loss: 0.46025813225692463\n",
      "iteration 4927train_loss: 0.42687454430796773 test_loss: 0.4602582689944375\n",
      "iteration 4928train_loss: 0.4268745439872501 test_loss: 0.4602584056504234\n",
      "iteration 4929train_loss: 0.42687454366691646 test_loss: 0.4602585422249313\n",
      "iteration 4930train_loss: 0.42687454334696623 test_loss: 0.4602586787180092\n",
      "iteration 4931train_loss: 0.4268745430273991 test_loss: 0.4602588151297057\n",
      "iteration 4932train_loss: 0.42687454270821446 test_loss: 0.46025895146006934\n",
      "iteration 4933train_loss: 0.42687454238941197 test_loss: 0.4602590877091484\n",
      "iteration 4934train_loss: 0.4268745420709911 test_loss: 0.46025922387699136\n",
      "iteration 4935train_loss: 0.42687454175295136 test_loss: 0.4602593599636464\n",
      "iteration 4936train_loss: 0.42687454143529246 test_loss: 0.460259495969162\n",
      "iteration 4937train_loss: 0.42687454111801376 test_loss: 0.46025963189358626\n",
      "iteration 4938train_loss: 0.4268745408011149 test_loss: 0.4602597677369676\n",
      "iteration 4939train_loss: 0.42687454048459544 test_loss: 0.4602599034993541\n",
      "iteration 4940train_loss: 0.42687454016845483 test_loss: 0.4602600391807941\n",
      "iteration 4941train_loss: 0.42687453985269275 test_loss: 0.4602601747813357\n",
      "iteration 4942train_loss: 0.4268745395373087 test_loss: 0.46026031030102693\n",
      "iteration 4943train_loss: 0.42687453922230206 test_loss: 0.46026044573991604\n",
      "iteration 4944train_loss: 0.4268745389076726 test_loss: 0.46026058109805107\n",
      "iteration 4945train_loss: 0.4268745385934196 test_loss: 0.4602607163754801\n",
      "iteration 4946train_loss: 0.426874538279543 test_loss: 0.4602608515722511\n",
      "iteration 4947train_loss: 0.4268745379660421 test_loss: 0.46026098668841214\n",
      "iteration 4948train_loss: 0.42687453765291644 test_loss: 0.46026112172401107\n",
      "iteration 4949train_loss: 0.42687453734016567 test_loss: 0.4602612566790959\n",
      "iteration 4950train_loss: 0.42687453702778927 test_loss: 0.46026139155371454\n",
      "iteration 4951train_loss: 0.4268745367157868 test_loss: 0.4602615263479149\n",
      "iteration 4952train_loss: 0.42687453640415784 test_loss: 0.46026166106174476\n",
      "iteration 4953train_loss: 0.4268745360929019 test_loss: 0.4602617956952519\n",
      "iteration 4954train_loss: 0.4268745357820185 test_loss: 0.4602619302484844\n",
      "iteration 4955train_loss: 0.42687453547150733 test_loss: 0.4602620647214896\n",
      "iteration 4956train_loss: 0.4268745351613678 test_loss: 0.4602621991143156\n",
      "iteration 4957train_loss: 0.42687453485159965 test_loss: 0.46026233342701\n",
      "iteration 4958train_loss: 0.4268745345422023 test_loss: 0.4602624676596204\n",
      "iteration 4959train_loss: 0.4268745342331752 test_loss: 0.46026260181219464\n",
      "iteration 4960train_loss: 0.42687453392451796 test_loss: 0.46026273588478\n",
      "iteration 4961train_loss: 0.4268745336162303 test_loss: 0.46026286987742454\n",
      "iteration 4962train_loss: 0.42687453330831177 test_loss: 0.4602630037901755\n",
      "iteration 4963train_loss: 0.4268745330007618 test_loss: 0.4602631376230805\n",
      "iteration 4964train_loss: 0.42687453269358006 test_loss: 0.46026327137618717\n",
      "iteration 4965train_loss: 0.4268745323867658 test_loss: 0.4602634050495428\n",
      "iteration 4966train_loss: 0.42687453208031895 test_loss: 0.46026353864319497\n",
      "iteration 4967train_loss: 0.42687453177423895 test_loss: 0.4602636721571911\n",
      "iteration 4968train_loss: 0.42687453146852533 test_loss: 0.46026380559157865\n",
      "iteration 4969train_loss: 0.42687453116317764 test_loss: 0.46026393894640477\n",
      "iteration 4970train_loss: 0.4268745308581955 test_loss: 0.46026407222171706\n",
      "iteration 4971train_loss: 0.4268745305535783 test_loss: 0.4602642054175628\n",
      "iteration 4972train_loss: 0.42687453024932587 test_loss: 0.4602643385339892\n",
      "iteration 4973train_loss: 0.4268745299454376 test_loss: 0.4602644715710434\n",
      "iteration 4974train_loss: 0.4268745296419132 test_loss: 0.46026460452877294\n",
      "iteration 4975train_loss: 0.426874529338752 test_loss: 0.46026473740722484\n",
      "iteration 4976train_loss: 0.42687452903595374 test_loss: 0.4602648702064462\n",
      "iteration 4977train_loss: 0.42687452873351794 test_loss: 0.46026500292648437\n",
      "iteration 4978train_loss: 0.4268745284314442 test_loss: 0.4602651355673864\n",
      "iteration 4979train_loss: 0.426874528129732 test_loss: 0.4602652681291992\n",
      "iteration 4980train_loss: 0.4268745278283809 test_loss: 0.46026540061197024\n",
      "iteration 4981train_loss: 0.42687452752739063 test_loss: 0.46026553301574613\n",
      "iteration 4982train_loss: 0.42687452722676067 test_loss: 0.4602656653405741\n",
      "iteration 4983train_loss: 0.42687452692649047 test_loss: 0.4602657975865011\n",
      "iteration 4984train_loss: 0.42687452662657976 test_loss: 0.46026592975357417\n",
      "iteration 4985train_loss: 0.4268745263270281 test_loss: 0.4602660618418401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4986train_loss: 0.42687452602783477 test_loss: 0.4602661938513458\n",
      "iteration 4987train_loss: 0.4268745257289998 test_loss: 0.4602663257821382\n",
      "iteration 4988train_loss: 0.4268745254305225 test_loss: 0.4602664576342642\n",
      "iteration 4989train_loss: 0.4268745251324025 test_loss: 0.4602665894077705\n",
      "iteration 4990train_loss: 0.4268745248346392 test_loss: 0.4602667211027039\n",
      "iteration 4991train_loss: 0.42687452453723257 test_loss: 0.4602668527191112\n",
      "iteration 4992train_loss: 0.4268745242401818 test_loss: 0.46026698425703916\n",
      "iteration 4993train_loss: 0.42687452394348663 test_loss: 0.4602671157165345\n",
      "iteration 4994train_loss: 0.42687452364714645 test_loss: 0.4602672470976437\n",
      "iteration 4995train_loss: 0.4268745233511612 test_loss: 0.46026737840041376\n",
      "iteration 4996train_loss: 0.4268745230555302 test_loss: 0.460267509624891\n",
      "iteration 4997train_loss: 0.426874522760253 test_loss: 0.4602676407711222\n",
      "iteration 4998train_loss: 0.42687452246532925 test_loss: 0.46026777183915374\n",
      "iteration 4999train_loss: 0.42687452217075855 test_loss: 0.46026790282903224\n",
      "iteration 5000train_loss: 0.4268745218765404 test_loss: 0.4602680337408044\n",
      "iteration 5001train_loss: 0.4268745215826745 test_loss: 0.4602681645745164\n",
      "iteration 5002train_loss: 0.42687452128916026 test_loss: 0.4602682953302149\n",
      "iteration 5003train_loss: 0.42687452099599743 test_loss: 0.46026842600794626\n",
      "iteration 5004train_loss: 0.4268745207031855 test_loss: 0.46026855660775695\n",
      "iteration 5005train_loss: 0.426874520410724 test_loss: 0.46026868712969327\n",
      "iteration 5006train_loss: 0.4268745201186126 test_loss: 0.4602688175738016\n",
      "iteration 5007train_loss: 0.42687451982685093 test_loss: 0.4602689479401281\n",
      "iteration 5008train_loss: 0.4268745195354384 test_loss: 0.46026907822871943\n",
      "iteration 5009train_loss: 0.4268745192443747 test_loss: 0.46026920843962155\n",
      "iteration 5010train_loss: 0.42687451895365947 test_loss: 0.46026933857288077\n",
      "iteration 5011train_loss: 0.42687451866329207 test_loss: 0.46026946862854323\n",
      "iteration 5012train_loss: 0.4268745183732723 test_loss: 0.4602695986066554\n",
      "iteration 5013train_loss: 0.42687451808359966 test_loss: 0.4602697285072631\n",
      "iteration 5014train_loss: 0.42687451779427377 test_loss: 0.4602698583304127\n",
      "iteration 5015train_loss: 0.42687451750529415 test_loss: 0.46026998807615005\n",
      "iteration 5016train_loss: 0.4268745172166604 test_loss: 0.46027011774452153\n",
      "iteration 5017train_loss: 0.42687451692837225 test_loss: 0.4602702473355728\n",
      "iteration 5018train_loss: 0.426874516640429 test_loss: 0.4602703768493503\n",
      "iteration 5019train_loss: 0.4268745163528307 test_loss: 0.4602705062858998\n",
      "iteration 5020train_loss: 0.42687451606557636 test_loss: 0.46027063564526727\n",
      "iteration 5021train_loss: 0.4268745157786659 test_loss: 0.46027076492749863\n",
      "iteration 5022train_loss: 0.42687451549209887 test_loss: 0.4602708941326397\n",
      "iteration 5023train_loss: 0.4268745152058749 test_loss: 0.4602710232607367\n",
      "iteration 5024train_loss: 0.4268745149199935 test_loss: 0.46027115231183524\n",
      "iteration 5025train_loss: 0.42687451463445425 test_loss: 0.46027128128598116\n",
      "iteration 5026train_loss: 0.4268745143492568 test_loss: 0.4602714101832204\n",
      "iteration 5027train_loss: 0.4268745140644007 test_loss: 0.46027153900359846\n",
      "iteration 5028train_loss: 0.4268745137798855 test_loss: 0.46027166774716133\n",
      "iteration 5029train_loss: 0.4268745134957109 test_loss: 0.46027179641395466\n",
      "iteration 5030train_loss: 0.4268745132118765 test_loss: 0.46027192500402414\n",
      "iteration 5031train_loss: 0.4268745129283818 test_loss: 0.46027205351741535\n",
      "iteration 5032train_loss: 0.4268745126452263 test_loss: 0.4602721819541741\n",
      "iteration 5033train_loss: 0.4268745123624099 test_loss: 0.46027231031434584\n",
      "iteration 5034train_loss: 0.4268745120799319 test_loss: 0.4602724385979762\n",
      "iteration 5035train_loss: 0.426874511797792 test_loss: 0.4602725668051108\n",
      "iteration 5036train_loss: 0.4268745115159898 test_loss: 0.46027269493579526\n",
      "iteration 5037train_loss: 0.426874511234525 test_loss: 0.46027282299007477\n",
      "iteration 5038train_loss: 0.42687451095339696 test_loss: 0.46027295096799514\n",
      "iteration 5039train_loss: 0.42687451067260546 test_loss: 0.46027307886960167\n",
      "iteration 5040train_loss: 0.42687451039214996 test_loss: 0.4602732066949397\n",
      "iteration 5041train_loss: 0.42687451011203026 test_loss: 0.46027333444405477\n",
      "iteration 5042train_loss: 0.4268745098322457 test_loss: 0.46027346211699227\n",
      "iteration 5043train_loss: 0.4268745095527961 test_loss: 0.4602735897137974\n",
      "iteration 5044train_loss: 0.4268745092736811 test_loss: 0.46027371723451554\n",
      "iteration 5045train_loss: 0.4268745089949 test_loss: 0.460273844679192\n",
      "iteration 5046train_loss: 0.4268745087164526 test_loss: 0.46027397204787207\n",
      "iteration 5047train_loss: 0.42687450843833846 test_loss: 0.4602740993406009\n",
      "iteration 5048train_loss: 0.42687450816055716 test_loss: 0.4602742265574237\n",
      "iteration 5049train_loss: 0.4268745078831085 test_loss: 0.46027435369838576\n",
      "iteration 5050train_loss: 0.42687450760599177 test_loss: 0.46027448076353217\n",
      "iteration 5051train_loss: 0.42687450732920673 test_loss: 0.46027460775290807\n",
      "iteration 5052train_loss: 0.42687450705275304 test_loss: 0.4602747346665585\n",
      "iteration 5053train_loss: 0.42687450677663014 test_loss: 0.46027486150452857\n",
      "iteration 5054train_loss: 0.42687450650083786 test_loss: 0.46027498826686336\n",
      "iteration 5055train_loss: 0.42687450622537554 test_loss: 0.46027511495360796\n",
      "iteration 5056train_loss: 0.426874505950243 test_loss: 0.4602752415648072\n",
      "iteration 5057train_loss: 0.42687450567543966 test_loss: 0.46027536810050607\n",
      "iteration 5058train_loss: 0.42687450540096533 test_loss: 0.46027549456074973\n",
      "iteration 5059train_loss: 0.42687450512681946 test_loss: 0.4602756209455828\n",
      "iteration 5060train_loss: 0.42687450485300166 test_loss: 0.46027574725505044\n",
      "iteration 5061train_loss: 0.42687450457951176 test_loss: 0.4602758734891973\n",
      "iteration 5062train_loss: 0.4268745043063491 test_loss: 0.46027599964806815\n",
      "iteration 5063train_loss: 0.4268745040335133 test_loss: 0.4602761257317081\n",
      "iteration 5064train_loss: 0.4268745037610041 test_loss: 0.46027625174016185\n",
      "iteration 5065train_loss: 0.42687450348882106 test_loss: 0.4602763776734739\n",
      "iteration 5066train_loss: 0.4268745032169638 test_loss: 0.46027650353168925\n",
      "iteration 5067train_loss: 0.4268745029454319 test_loss: 0.46027662931485247\n",
      "iteration 5068train_loss: 0.42687450267422494 test_loss: 0.4602767550230083\n",
      "iteration 5069train_loss: 0.42687450240334257 test_loss: 0.46027688065620126\n",
      "iteration 5070train_loss: 0.4268745021327846 test_loss: 0.46027700621447626\n",
      "iteration 5071train_loss: 0.42687450186255016 test_loss: 0.4602771316978776\n",
      "iteration 5072train_loss: 0.42687450159263934 test_loss: 0.46027725710644996\n",
      "iteration 5073train_loss: 0.42687450132305155 test_loss: 0.46027738244023797\n",
      "iteration 5074train_loss: 0.4268745010537864 test_loss: 0.460277507699286\n",
      "iteration 5075train_loss: 0.42687450078484346 test_loss: 0.46027763288363854\n",
      "iteration 5076train_loss: 0.42687450051622244 test_loss: 0.4602777579933402\n",
      "iteration 5077train_loss: 0.426874500247923 test_loss: 0.4602778830284353\n",
      "iteration 5078train_loss: 0.4268744999799445 test_loss: 0.46027800798896834\n",
      "iteration 5079train_loss: 0.4268744997122869 test_loss: 0.4602781328749837\n",
      "iteration 5080train_loss: 0.4268744994449495 test_loss: 0.46027825768652564\n",
      "iteration 5081train_loss: 0.42687449917793213 test_loss: 0.4602783824236385\n",
      "iteration 5082train_loss: 0.4268744989112344 test_loss: 0.46027850708636675\n",
      "iteration 5083train_loss: 0.4268744986448557 test_loss: 0.4602786316747545\n",
      "iteration 5084train_loss: 0.4268744983787959 test_loss: 0.46027875618884606\n",
      "iteration 5085train_loss: 0.4268744981130545 test_loss: 0.4602788806286856\n",
      "iteration 5086train_loss: 0.42687449784763126 test_loss: 0.4602790049943175\n",
      "iteration 5087train_loss: 0.4268744975825256 test_loss: 0.4602791292857857\n",
      "iteration 5088train_loss: 0.42687449731773724 test_loss: 0.46027925350313453\n",
      "iteration 5089train_loss: 0.42687449705326563 test_loss: 0.46027937764640814\n",
      "iteration 5090train_loss: 0.4268744967891107 test_loss: 0.4602795017156504\n",
      "iteration 5091train_loss: 0.42687449652527193 test_loss: 0.4602796257109056\n",
      "iteration 5092train_loss: 0.42687449626174884 test_loss: 0.4602797496322176\n",
      "iteration 5093train_loss: 0.4268744959985412 test_loss: 0.46027987347963073\n",
      "iteration 5094train_loss: 0.4268744957356485 test_loss: 0.46027999725318863\n",
      "iteration 5095train_loss: 0.4268744954730705 test_loss: 0.4602801209529354\n",
      "iteration 5096train_loss: 0.42687449521080684 test_loss: 0.4602802445789151\n",
      "iteration 5097train_loss: 0.4268744949488568 test_loss: 0.4602803681311714\n",
      "iteration 5098train_loss: 0.42687449468722055 test_loss: 0.4602804916097484\n",
      "iteration 5099train_loss: 0.4268744944258972 test_loss: 0.4602806150146899\n",
      "iteration 5100train_loss: 0.4268744941648868 test_loss: 0.4602807383460398\n",
      "iteration 5101train_loss: 0.4268744939041886 test_loss: 0.46028086160384174\n",
      "iteration 5102train_loss: 0.4268744936438026 test_loss: 0.46028098478813967\n",
      "iteration 5103train_loss: 0.42687449338372796 test_loss: 0.46028110789897714\n",
      "iteration 5104train_loss: 0.42687449312396475 test_loss: 0.46028123093639817\n",
      "iteration 5105train_loss: 0.4268744928645124 test_loss: 0.4602813539004463\n",
      "iteration 5106train_loss: 0.4268744926053706 test_loss: 0.46028147679116516\n",
      "iteration 5107train_loss: 0.42687449234653885 test_loss: 0.46028159960859855\n",
      "iteration 5108train_loss: 0.4268744920880168 test_loss: 0.46028172235279\n",
      "iteration 5109train_loss: 0.4268744918298044 test_loss: 0.4602818450237832\n",
      "iteration 5110train_loss: 0.4268744915719008 test_loss: 0.4602819676216216\n",
      "iteration 5111train_loss: 0.426874491314306 test_loss: 0.46028209014634885\n",
      "iteration 5112train_loss: 0.42687449105701936 test_loss: 0.46028221259800844\n",
      "iteration 5113train_loss: 0.42687449080004064 test_loss: 0.46028233497664384\n",
      "iteration 5114train_loss: 0.4268744905433695 test_loss: 0.46028245728229866\n",
      "iteration 5115train_loss: 0.4268744902870055 test_loss: 0.46028257951501617\n",
      "iteration 5116train_loss: 0.42687449003094835 test_loss: 0.4602827016748399\n",
      "iteration 5117train_loss: 0.4268744897751977 test_loss: 0.4602828237618133\n",
      "iteration 5118train_loss: 0.42687448951975304 test_loss: 0.4602829457759796\n",
      "iteration 5119train_loss: 0.42687448926461413 test_loss: 0.46028306771738214\n",
      "iteration 5120train_loss: 0.42687448900978053 test_loss: 0.46028318958606435\n",
      "iteration 5121train_loss: 0.426874488755252 test_loss: 0.46028331138206957\n",
      "iteration 5122train_loss: 0.42687448850102794 test_loss: 0.46028343310544095\n",
      "iteration 5123train_loss: 0.4268744882471083 test_loss: 0.46028355475622174\n",
      "iteration 5124train_loss: 0.42687448799349226 test_loss: 0.4602836763344552\n",
      "iteration 5125train_loss: 0.42687448774018005 test_loss: 0.4602837978401844\n",
      "iteration 5126train_loss: 0.4268744874871707 test_loss: 0.46028391927345264\n",
      "iteration 5127train_loss: 0.4268744872344643 test_loss: 0.4602840406343032\n",
      "iteration 5128train_loss: 0.4268744869820604 test_loss: 0.4602841619227788\n",
      "iteration 5129train_loss: 0.4268744867299584 test_loss: 0.460284283138923\n",
      "iteration 5130train_loss: 0.4268744864781581 test_loss: 0.4602844042827784\n",
      "iteration 5131train_loss: 0.4268744862266593 test_loss: 0.4602845253543884\n",
      "iteration 5132train_loss: 0.4268744859754614 test_loss: 0.4602846463537958\n",
      "iteration 5133train_loss: 0.42687448572456405 test_loss: 0.4602847672810437\n",
      "iteration 5134train_loss: 0.42687448547396706 test_loss: 0.4602848881361751\n",
      "iteration 5135train_loss: 0.42687448522366994 test_loss: 0.46028500891923274\n",
      "iteration 5136train_loss: 0.4268744849736723 test_loss: 0.4602851296302596\n",
      "iteration 5137train_loss: 0.426874484723974 test_loss: 0.4602852502692988\n",
      "iteration 5138train_loss: 0.4268744844745743 test_loss: 0.46028537083639287\n",
      "iteration 5139train_loss: 0.42687448422547314 test_loss: 0.4602854913315848\n",
      "iteration 5140train_loss: 0.42687448397667016 test_loss: 0.46028561175491745\n",
      "iteration 5141train_loss: 0.42687448372816494 test_loss: 0.46028573210643353\n",
      "iteration 5142train_loss: 0.42687448347995705 test_loss: 0.4602858523861758\n",
      "iteration 5143train_loss: 0.4268744832320462 test_loss: 0.460285972594187\n",
      "iteration 5144train_loss: 0.42687448298443204 test_loss: 0.46028609273050985\n",
      "iteration 5145train_loss: 0.4268744827371142 test_loss: 0.4602862127951871\n",
      "iteration 5146train_loss: 0.4268744824900923 test_loss: 0.46028633278826125\n",
      "iteration 5147train_loss: 0.42687448224336605 test_loss: 0.46028645270977503\n",
      "iteration 5148train_loss: 0.42687448199693506 test_loss: 0.460286572559771\n",
      "iteration 5149train_loss: 0.42687448175079895 test_loss: 0.46028669233829184\n",
      "iteration 5150train_loss: 0.42687448150495744 test_loss: 0.46028681204538\n",
      "iteration 5151train_loss: 0.42687448125941013 test_loss: 0.46028693168107815\n",
      "iteration 5152train_loss: 0.4268744810141566 test_loss: 0.4602870512454285\n",
      "iteration 5153train_loss: 0.42687448076919665 test_loss: 0.4602871707384739\n",
      "iteration 5154train_loss: 0.4268744805245298 test_loss: 0.4602872901602566\n",
      "iteration 5155train_loss: 0.4268744802801558 test_loss: 0.46028740951081903\n",
      "iteration 5156train_loss: 0.42687448003607403 test_loss: 0.46028752879020374\n",
      "iteration 5157train_loss: 0.4268744797922846 test_loss: 0.46028764799845295\n",
      "iteration 5158train_loss: 0.4268744795487867 test_loss: 0.460287767135609\n",
      "iteration 5159train_loss: 0.4268744793055805 test_loss: 0.46028788620171435\n",
      "iteration 5160train_loss: 0.42687447906266507 test_loss: 0.4602880051968111\n",
      "iteration 5161train_loss: 0.42687447882004037 test_loss: 0.46028812412094183\n",
      "iteration 5162train_loss: 0.42687447857770616 test_loss: 0.4602882429741486\n",
      "iteration 5163train_loss: 0.42687447833566183 test_loss: 0.46028836175647375\n",
      "iteration 5164train_loss: 0.42687447809390716 test_loss: 0.46028848046795934\n",
      "iteration 5165train_loss: 0.42687447785244176 test_loss: 0.46028859910864767\n",
      "iteration 5166train_loss: 0.42687447761126535 test_loss: 0.46028871767858065\n",
      "iteration 5167train_loss: 0.42687447737037754 test_loss: 0.4602888361778008\n",
      "iteration 5168train_loss: 0.42687447712977805 test_loss: 0.4602889546063499\n",
      "iteration 5169train_loss: 0.42687447688946656 test_loss: 0.4602890729642703\n",
      "iteration 5170train_loss: 0.42687447664944245 test_loss: 0.4602891912516038\n",
      "iteration 5171train_loss: 0.4268744764097057 test_loss: 0.46028930946839264\n",
      "iteration 5172train_loss: 0.42687447617025576 test_loss: 0.46028942761467856\n",
      "iteration 5173train_loss: 0.42687447593109235 test_loss: 0.46028954569050384\n",
      "iteration 5174train_loss: 0.4268744756922152 test_loss: 0.4602896636959103\n",
      "iteration 5175train_loss: 0.42687447545362395 test_loss: 0.46028978163093986\n",
      "iteration 5176train_loss: 0.4268744752153182 test_loss: 0.46028989949563437\n",
      "iteration 5177train_loss: 0.4268744749772975 test_loss: 0.4602900172900359\n",
      "iteration 5178train_loss: 0.42687447473956175 test_loss: 0.46029013501418614\n",
      "iteration 5179train_loss: 0.42687447450211036 test_loss: 0.46029025266812695\n",
      "iteration 5180train_loss: 0.42687447426494324 test_loss: 0.46029037025190017\n",
      "iteration 5181train_loss: 0.4268744740280599 test_loss: 0.46029048776554754\n",
      "iteration 5182train_loss: 0.42687447379146004 test_loss: 0.46029060520911086\n",
      "iteration 5183train_loss: 0.4268744735551433 test_loss: 0.4602907225826317\n",
      "iteration 5184train_loss: 0.4268744733191093 test_loss: 0.4602908398861521\n",
      "iteration 5185train_loss: 0.42687447308335774 test_loss: 0.46029095711971346\n",
      "iteration 5186train_loss: 0.4268744728478883 test_loss: 0.4602910742833576\n",
      "iteration 5187train_loss: 0.42687447261270073 test_loss: 0.46029119137712593\n",
      "iteration 5188train_loss: 0.4268744723777945 test_loss: 0.4602913084010603\n",
      "iteration 5189train_loss: 0.4268744721431694 test_loss: 0.46029142535520223\n",
      "iteration 5190train_loss: 0.4268744719088252 test_loss: 0.46029154223959323\n",
      "iteration 5191train_loss: 0.42687447167476117 test_loss: 0.4602916590542748\n",
      "iteration 5192train_loss: 0.4268744714409774 test_loss: 0.46029177579928854\n",
      "iteration 5193train_loss: 0.4268744712074733 test_loss: 0.4602918924746759\n",
      "iteration 5194train_loss: 0.4268744709742487 test_loss: 0.4602920090804784\n",
      "iteration 5195train_loss: 0.42687447074130314 test_loss: 0.4602921256167374\n",
      "iteration 5196train_loss: 0.4268744705086362 test_loss: 0.46029224208349423\n",
      "iteration 5197train_loss: 0.4268744702762479 test_loss: 0.4602923584807904\n",
      "iteration 5198train_loss: 0.42687447004413753 test_loss: 0.46029247480866725\n",
      "iteration 5199train_loss: 0.426874469812305 test_loss: 0.46029259106716613\n",
      "iteration 5200train_loss: 0.42687446958074976 test_loss: 0.46029270725632837\n",
      "iteration 5201train_loss: 0.4268744693494717 test_loss: 0.46029282337619515\n",
      "iteration 5202train_loss: 0.4268744691184704 test_loss: 0.4602929394268079\n",
      "iteration 5203train_loss: 0.4268744688877455 test_loss: 0.4602930554082077\n",
      "iteration 5204train_loss: 0.42687446865729667 test_loss: 0.46029317132043585\n",
      "iteration 5205train_loss: 0.4268744684271236 test_loss: 0.46029328716353346\n",
      "iteration 5206train_loss: 0.426874468197226 test_loss: 0.46029340293754184\n",
      "iteration 5207train_loss: 0.4268744679676034 test_loss: 0.460293518642502\n",
      "iteration 5208train_loss: 0.42687446773825566 test_loss: 0.4602936342784551\n",
      "iteration 5209train_loss: 0.42687446750918234 test_loss: 0.46029374984544225\n",
      "iteration 5210train_loss: 0.4268744672803832 test_loss: 0.4602938653435046\n",
      "iteration 5211train_loss: 0.4268744670518578 test_loss: 0.4602939807726831\n",
      "iteration 5212train_loss: 0.42687446682360575 test_loss: 0.4602940961330185\n",
      "iteration 5213train_loss: 0.42687446659562694 test_loss: 0.46029421142455235\n",
      "iteration 5214train_loss: 0.42687446636792087 test_loss: 0.4602943266473253\n",
      "iteration 5215train_loss: 0.42687446614048735 test_loss: 0.46029444180137835\n",
      "iteration 5216train_loss: 0.4268744659133259 test_loss: 0.4602945568867523\n",
      "iteration 5217train_loss: 0.4268744656864363 test_loss: 0.4602946719034883\n",
      "iteration 5218train_loss: 0.42687446545981816 test_loss: 0.4602947868516269\n",
      "iteration 5219train_loss: 0.4268744652334712 test_loss: 0.4602949017312093\n",
      "iteration 5220train_loss: 0.42687446500739507 test_loss: 0.460295016542276\n",
      "iteration 5221train_loss: 0.4268744647815896 test_loss: 0.460295131284868\n",
      "iteration 5222train_loss: 0.42687446455605416 test_loss: 0.4602952459590261\n",
      "iteration 5223train_loss: 0.4268744643307887 test_loss: 0.46029536056479103\n",
      "iteration 5224train_loss: 0.42687446410579266 test_loss: 0.4602954751022034\n",
      "iteration 5225train_loss: 0.4268744638810659 test_loss: 0.4602955895713041\n",
      "iteration 5226train_loss: 0.42687446365660814 test_loss: 0.46029570397213354\n",
      "iteration 5227train_loss: 0.42687446343241886 test_loss: 0.46029581830473265\n",
      "iteration 5228train_loss: 0.42687446320849787 test_loss: 0.4602959325691419\n",
      "iteration 5229train_loss: 0.4268744629848448 test_loss: 0.460296046765402\n",
      "iteration 5230train_loss: 0.4268744627614594 test_loss: 0.46029616089355346\n",
      "iteration 5231train_loss: 0.42687446253834127 test_loss: 0.4602962749536369\n",
      "iteration 5232train_loss: 0.42687446231549003 test_loss: 0.4602963889456927\n",
      "iteration 5233train_loss: 0.4268744620929055 test_loss: 0.46029650286976165\n",
      "iteration 5234train_loss: 0.42687446187058736 test_loss: 0.46029661672588407\n",
      "iteration 5235train_loss: 0.42687446164853526 test_loss: 0.46029673051410036\n",
      "iteration 5236train_loss: 0.42687446142674873 test_loss: 0.46029684423445105\n",
      "iteration 5237train_loss: 0.42687446120522765 test_loss: 0.46029695788697655\n",
      "iteration 5238train_loss: 0.42687446098397164 test_loss: 0.4602970714717173\n",
      "iteration 5239train_loss: 0.4268744607629804 test_loss: 0.4602971849887135\n",
      "iteration 5240train_loss: 0.4268744605422536 test_loss: 0.46029729843800576\n",
      "iteration 5241train_loss: 0.42687446032179077 test_loss: 0.46029741181963413\n",
      "iteration 5242train_loss: 0.4268744601015919 test_loss: 0.460297525133639\n",
      "iteration 5243train_loss: 0.4268744598816565 test_loss: 0.4602976383800607\n",
      "iteration 5244train_loss: 0.4268744596619841 test_loss: 0.46029775155893954\n",
      "iteration 5245train_loss: 0.42687445944257474 test_loss: 0.4602978646703156\n",
      "iteration 5246train_loss: 0.42687445922342776 test_loss: 0.46029797771422903\n",
      "iteration 5247train_loss: 0.4268744590045431 test_loss: 0.46029809069072014\n",
      "iteration 5248train_loss: 0.4268744587859203 test_loss: 0.4602982035998291\n",
      "iteration 5249train_loss: 0.4268744585675592 test_loss: 0.46029831644159597\n",
      "iteration 5250train_loss: 0.4268744583494592 test_loss: 0.46029842921606096\n",
      "iteration 5251train_loss: 0.4268744581316203 test_loss: 0.4602985419232639\n",
      "iteration 5252train_loss: 0.426874457914042 test_loss: 0.46029865456324515\n",
      "iteration 5253train_loss: 0.42687445769672394 test_loss: 0.4602987671360445\n",
      "iteration 5254train_loss: 0.426874457479666 test_loss: 0.4602988796417022\n",
      "iteration 5255train_loss: 0.42687445726286777 test_loss: 0.460298992080258\n",
      "iteration 5256train_loss: 0.42687445704632904 test_loss: 0.46029910445175204\n",
      "iteration 5257train_loss: 0.4268744568300493 test_loss: 0.46029921675622404\n",
      "iteration 5258train_loss: 0.42687445661402834 test_loss: 0.4602993289937141\n",
      "iteration 5259train_loss: 0.426874456398266 test_loss: 0.46029944116426225\n",
      "iteration 5260train_loss: 0.42687445618276154 test_loss: 0.460299553267908\n",
      "iteration 5261train_loss: 0.4268744559675152 test_loss: 0.4602996653046915\n",
      "iteration 5262train_loss: 0.42687445575252636 test_loss: 0.4602997772746524\n",
      "iteration 5263train_loss: 0.42687445553779463 test_loss: 0.4602998891778305\n",
      "iteration 5264train_loss: 0.42687445532331997 test_loss: 0.46030000101426566\n",
      "iteration 5265train_loss: 0.4268744551091018 test_loss: 0.4603001127839976\n",
      "iteration 5266train_loss: 0.4268744548951399 test_loss: 0.46030022448706603\n",
      "iteration 5267train_loss: 0.4268744546814342 test_loss: 0.4603003361235107\n",
      "iteration 5268train_loss: 0.42687445446798417 test_loss: 0.46030044769337114\n",
      "iteration 5269train_loss: 0.4268744542547895 test_loss: 0.46030055919668716\n",
      "iteration 5270train_loss: 0.42687445404184987 test_loss: 0.46030067063349833\n",
      "iteration 5271train_loss: 0.42687445382916506 test_loss: 0.46030078200384433\n",
      "iteration 5272train_loss: 0.4268744536167347 test_loss: 0.46030089330776464\n",
      "iteration 5273train_loss: 0.4268744534045585 test_loss: 0.4603010045452989\n",
      "iteration 5274train_loss: 0.4268744531926363 test_loss: 0.46030111571648663\n",
      "iteration 5275train_loss: 0.42687445298096754 test_loss: 0.46030122682136737\n",
      "iteration 5276train_loss: 0.426874452769552 test_loss: 0.46030133785998056\n",
      "iteration 5277train_loss: 0.42687445255838957 test_loss: 0.46030144883236557\n",
      "iteration 5278train_loss: 0.42687445234747967 test_loss: 0.46030155973856207\n",
      "iteration 5279train_loss: 0.4268744521368221 test_loss: 0.4603016705786093\n",
      "iteration 5280train_loss: 0.42687445192641676 test_loss: 0.46030178135254673\n",
      "iteration 5281train_loss: 0.42687445171626304 test_loss: 0.4603018920604137\n",
      "iteration 5282train_loss: 0.4268744515063608 test_loss: 0.46030200270224975\n",
      "iteration 5283train_loss: 0.42687445129670964 test_loss: 0.4603021132780939\n",
      "iteration 5284train_loss: 0.4268744510873093 test_loss: 0.46030222378798574\n",
      "iteration 5285train_loss: 0.42687445087815956 test_loss: 0.4603023342319644\n",
      "iteration 5286train_loss: 0.4268744506692601 test_loss: 0.460302444610069\n",
      "iteration 5287train_loss: 0.42687445046061057 test_loss: 0.460302554922339\n",
      "iteration 5288train_loss: 0.4268744502522107 test_loss: 0.46030266516881363\n",
      "iteration 5289train_loss: 0.42687445004406005 test_loss: 0.46030277534953196\n",
      "iteration 5290train_loss: 0.4268744498361587 test_loss: 0.460302885464533\n",
      "iteration 5291train_loss: 0.4268744496285059 test_loss: 0.4603029955138563\n",
      "iteration 5292train_loss: 0.42687444942110153 test_loss: 0.4603031054975406\n",
      "iteration 5293train_loss: 0.4268744492139454 test_loss: 0.46030321541562524\n",
      "iteration 5294train_loss: 0.4268744490070371 test_loss: 0.46030332526814915\n",
      "iteration 5295train_loss: 0.42687444880037634 test_loss: 0.4603034350551514\n",
      "iteration 5296train_loss: 0.42687444859396284 test_loss: 0.4603035447766711\n",
      "iteration 5297train_loss: 0.42687444838779637 test_loss: 0.46030365443274723\n",
      "iteration 5298train_loss: 0.4268744481818765 test_loss: 0.4603037640234187\n",
      "iteration 5299train_loss: 0.42687444797620294 test_loss: 0.4603038735487245\n",
      "iteration 5300train_loss: 0.4268744477707756 test_loss: 0.4603039830087035\n",
      "iteration 5301train_loss: 0.426874447565594 test_loss: 0.4603040924033947\n",
      "iteration 5302train_loss: 0.4268744473606579 test_loss: 0.4603042017328371\n",
      "iteration 5303train_loss: 0.42687444715596695 test_loss: 0.46030431099706925\n",
      "iteration 5304train_loss: 0.42687444695152094 test_loss: 0.46030442019613016\n",
      "iteration 5305train_loss: 0.4268744467473196 test_loss: 0.4603045293300587\n",
      "iteration 5306train_loss: 0.4268744465433625 test_loss: 0.4603046383988936\n",
      "iteration 5307train_loss: 0.4268744463396494 test_loss: 0.4603047474026737\n",
      "iteration 5308train_loss: 0.42687444613618003 test_loss: 0.4603048563414376\n",
      "iteration 5309train_loss: 0.42687444593295415 test_loss: 0.4603049652152241\n",
      "iteration 5310train_loss: 0.4268744457299714 test_loss: 0.4603050740240721\n",
      "iteration 5311train_loss: 0.4268744455272315 test_loss: 0.46030518276802\n",
      "iteration 5312train_loss: 0.42687444532473423 test_loss: 0.4603052914471065\n",
      "iteration 5313train_loss: 0.42687444512247913 test_loss: 0.4603054000613704\n",
      "iteration 5314train_loss: 0.4268744449204661 test_loss: 0.4603055086108501\n",
      "iteration 5315train_loss: 0.4268744447186948 test_loss: 0.4603056170955844\n",
      "iteration 5316train_loss: 0.42687444451716466 test_loss: 0.4603057255156117\n",
      "iteration 5317train_loss: 0.42687444431587596 test_loss: 0.4603058338709705\n",
      "iteration 5318train_loss: 0.4268744441148278 test_loss: 0.4603059421616995\n",
      "iteration 5319train_loss: 0.42687444391402035 test_loss: 0.4603060503878371\n",
      "iteration 5320train_loss: 0.4268744437134531 test_loss: 0.46030615854942186\n",
      "iteration 5321train_loss: 0.42687444351312587 test_loss: 0.4603062666464921\n",
      "iteration 5322train_loss: 0.4268744433130382 test_loss: 0.4603063746790863\n",
      "iteration 5323train_loss: 0.42687444311319 test_loss: 0.46030648264724283\n",
      "iteration 5324train_loss: 0.426874442913581 test_loss: 0.46030659055100015\n",
      "iteration 5325train_loss: 0.4268744427142107 test_loss: 0.4603066983903966\n",
      "iteration 5326train_loss: 0.4268744425150789 test_loss: 0.46030680616547043\n",
      "iteration 5327train_loss: 0.42687444231618543 test_loss: 0.46030691387626016\n",
      "iteration 5328train_loss: 0.4268744421175299 test_loss: 0.4603070215228038\n",
      "iteration 5329train_loss: 0.42687444191911195 test_loss: 0.4603071291051398\n",
      "iteration 5330train_loss: 0.4268744417209314 test_loss: 0.4603072366233063\n",
      "iteration 5331train_loss: 0.42687444152298804 test_loss: 0.46030734407734175\n",
      "iteration 5332train_loss: 0.42687444132528146 test_loss: 0.460307451467284\n",
      "iteration 5333train_loss: 0.42687444112781153 test_loss: 0.4603075587931716\n",
      "iteration 5334train_loss: 0.4268744409305777 test_loss: 0.4603076660550425\n",
      "iteration 5335train_loss: 0.4268744407335799 test_loss: 0.46030777325293476\n",
      "iteration 5336train_loss: 0.42687444053681767 test_loss: 0.4603078803868865\n",
      "iteration 5337train_loss: 0.4268744403402911 test_loss: 0.460307987456936\n",
      "iteration 5338train_loss: 0.42687444014399945 test_loss: 0.46030809446312126\n",
      "iteration 5339train_loss: 0.42687443994794266 test_loss: 0.46030820140548023\n",
      "iteration 5340train_loss: 0.42687443975212047 test_loss: 0.460308308284051\n",
      "iteration 5341train_loss: 0.4268744395565326 test_loss: 0.4603084150988715\n",
      "iteration 5342train_loss: 0.42687443936117875 test_loss: 0.4603085218499798\n",
      "iteration 5343train_loss: 0.42687443916605855 test_loss: 0.4603086285374137\n",
      "iteration 5344train_loss: 0.42687443897117183 test_loss: 0.4603087351612113\n",
      "iteration 5345train_loss: 0.42687443877651826 test_loss: 0.4603088417214104\n",
      "iteration 5346train_loss: 0.4268744385820975 test_loss: 0.4603089482180489\n",
      "iteration 5347train_loss: 0.42687443838790945 test_loss: 0.46030905465116473\n",
      "iteration 5348train_loss: 0.4268744381939537 test_loss: 0.46030916102079567\n",
      "iteration 5349train_loss: 0.42687443800022984 test_loss: 0.4603092673269796\n",
      "iteration 5350train_loss: 0.42687443780673784 test_loss: 0.46030937356975415\n",
      "iteration 5351train_loss: 0.42687443761347743 test_loss: 0.46030947974915715\n",
      "iteration 5352train_loss: 0.42687443742044817 test_loss: 0.4603095858652266\n",
      "iteration 5353train_loss: 0.4268744372276499 test_loss: 0.4603096919179998\n",
      "iteration 5354train_loss: 0.42687443703508204 test_loss: 0.4603097979075148\n",
      "iteration 5355train_loss: 0.42687443684274484 test_loss: 0.46030990383380926\n",
      "iteration 5356train_loss: 0.42687443665063773 test_loss: 0.46031000969692065\n",
      "iteration 5357train_loss: 0.42687443645876033 test_loss: 0.4603101154968866\n",
      "iteration 5358train_loss: 0.4268744362671125 test_loss: 0.460310221233745\n",
      "iteration 5359train_loss: 0.426874436075694 test_loss: 0.4603103269075332\n",
      "iteration 5360train_loss: 0.42687443588450447 test_loss: 0.4603104325182888\n",
      "iteration 5361train_loss: 0.4268744356935436 test_loss: 0.4603105380660494\n",
      "iteration 5362train_loss: 0.42687443550281134 test_loss: 0.4603106435508525\n",
      "iteration 5363train_loss: 0.42687443531230723 test_loss: 0.46031074897273566\n",
      "iteration 5364train_loss: 0.42687443512203094 test_loss: 0.4603108543317363\n",
      "iteration 5365train_loss: 0.4268744349319824 test_loss: 0.4603109596278918\n",
      "iteration 5366train_loss: 0.42687443474216114 test_loss: 0.46031106486123996\n",
      "iteration 5367train_loss: 0.426874434552567 test_loss: 0.46031117003181765\n",
      "iteration 5368train_loss: 0.42687443436319966 test_loss: 0.4603112751396626\n",
      "iteration 5369train_loss: 0.426874434174059 test_loss: 0.4603113801848122\n",
      "iteration 5370train_loss: 0.4268744339851445 test_loss: 0.46031148516730375\n",
      "iteration 5371train_loss: 0.4268744337964559 test_loss: 0.4603115900871744\n",
      "iteration 5372train_loss: 0.42687443360799315 test_loss: 0.4603116949444617\n",
      "iteration 5373train_loss: 0.42687443341975595 test_loss: 0.4603117997392028\n",
      "iteration 5374train_loss: 0.42687443323174384 test_loss: 0.46031190447143494\n",
      "iteration 5375train_loss: 0.4268744330439566 test_loss: 0.4603120091411954\n",
      "iteration 5376train_loss: 0.42687443285639415 test_loss: 0.4603121137485215\n",
      "iteration 5377train_loss: 0.42687443266905606 test_loss: 0.46031221829345037\n",
      "iteration 5378train_loss: 0.4268744324819421 test_loss: 0.4603123227760191\n",
      "iteration 5379train_loss: 0.4268744322950519 test_loss: 0.4603124271962648\n",
      "iteration 5380train_loss: 0.4268744321083854 test_loss: 0.4603125315542247\n",
      "iteration 5381train_loss: 0.4268744319219421 test_loss: 0.46031263584993587\n",
      "iteration 5382train_loss: 0.4268744317357219 test_loss: 0.4603127400834355\n",
      "iteration 5383train_loss: 0.42687443154972454 test_loss: 0.46031284425476043\n",
      "iteration 5384train_loss: 0.4268744313639496 test_loss: 0.46031294836394787\n",
      "iteration 5385train_loss: 0.42687443117839696 test_loss: 0.46031305241103493\n",
      "iteration 5386train_loss: 0.42687443099306627 test_loss: 0.46031315639605835\n",
      "iteration 5387train_loss: 0.42687443080795723 test_loss: 0.46031326031905523\n",
      "iteration 5388train_loss: 0.4268744306230698 test_loss: 0.4603133641800626\n",
      "iteration 5389train_loss: 0.4268744304384034 test_loss: 0.46031346797911715\n",
      "iteration 5390train_loss: 0.426874430253958 test_loss: 0.46031357171625603\n",
      "iteration 5391train_loss: 0.4268744300697333 test_loss: 0.460313675391516\n",
      "iteration 5392train_loss: 0.4268744298857289 test_loss: 0.4603137790049339\n",
      "iteration 5393train_loss: 0.4268744297019447 test_loss: 0.46031388255654676\n",
      "iteration 5394train_loss: 0.4268744295183803 test_loss: 0.4603139860463912\n",
      "iteration 5395train_loss: 0.4268744293350355 test_loss: 0.460314089474504\n",
      "iteration 5396train_loss: 0.4268744291519101 test_loss: 0.4603141928409222\n",
      "iteration 5397train_loss: 0.42687442896900374 test_loss: 0.46031429614568226\n",
      "iteration 5398train_loss: 0.4268744287863162 test_loss: 0.46031439938882107\n",
      "iteration 5399train_loss: 0.4268744286038472 test_loss: 0.46031450257037526\n",
      "iteration 5400train_loss: 0.42687442842159645 test_loss: 0.4603146056903816\n",
      "iteration 5401train_loss: 0.42687442823956384 test_loss: 0.46031470874887676\n",
      "iteration 5402train_loss: 0.42687442805774883 test_loss: 0.46031481174589717\n",
      "iteration 5403train_loss: 0.42687442787615143 test_loss: 0.4603149146814797\n",
      "iteration 5404train_loss: 0.4268744276947713 test_loss: 0.4603150175556609\n",
      "iteration 5405train_loss: 0.4268744275136081 test_loss: 0.46031512036847727\n",
      "iteration 5406train_loss: 0.42687442733266157 test_loss: 0.46031522311996553\n",
      "iteration 5407train_loss: 0.42687442715193163 test_loss: 0.460315325810162\n",
      "iteration 5408train_loss: 0.4268744269714178 test_loss: 0.4603154284391033\n",
      "iteration 5409train_loss: 0.42687442679111987 test_loss: 0.46031553100682593\n",
      "iteration 5410train_loss: 0.42687442661103775 test_loss: 0.46031563351336646\n",
      "iteration 5411train_loss: 0.426874426431171 test_loss: 0.4603157359587612\n",
      "iteration 5412train_loss: 0.42687442625151945 test_loss: 0.46031583834304646\n",
      "iteration 5413train_loss: 0.4268744260720828 test_loss: 0.46031594066625886\n",
      "iteration 5414train_loss: 0.42687442589286084 test_loss: 0.46031604292843475\n",
      "iteration 5415train_loss: 0.42687442571385326 test_loss: 0.4603161451296106\n",
      "iteration 5416train_loss: 0.4268744255350597 test_loss: 0.4603162472698224\n",
      "iteration 5417train_loss: 0.42687442535648024 test_loss: 0.4603163493491068\n",
      "iteration 5418train_loss: 0.4268744251781143 test_loss: 0.46031645136749993\n",
      "iteration 5419train_loss: 0.42687442499996175 test_loss: 0.4603165533250381\n",
      "iteration 5420train_loss: 0.42687442482202226 test_loss: 0.4603166552217577\n",
      "iteration 5421train_loss: 0.4268744246442957 test_loss: 0.46031675705769487\n",
      "iteration 5422train_loss: 0.4268744244667817 test_loss: 0.46031685883288564\n",
      "iteration 5423train_loss: 0.4268744242894801 test_loss: 0.46031696054736654\n",
      "iteration 5424train_loss: 0.4268744241123906 test_loss: 0.46031706220117335\n",
      "iteration 5425train_loss: 0.4268744239355129 test_loss: 0.4603171637943427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5426train_loss: 0.4268744237588468 test_loss: 0.46031726532691014\n",
      "iteration 5427train_loss: 0.4268744235823921 test_loss: 0.46031736679891233\n",
      "iteration 5428train_loss: 0.4268744234061485 test_loss: 0.460317468210385\n",
      "iteration 5429train_loss: 0.42687442323011565 test_loss: 0.4603175695613643\n",
      "iteration 5430train_loss: 0.4268744230542934 test_loss: 0.46031767085188635\n",
      "iteration 5431train_loss: 0.42687442287868155 test_loss: 0.46031777208198715\n",
      "iteration 5432train_loss: 0.42687442270327963 test_loss: 0.4603178732517025\n",
      "iteration 5433train_loss: 0.42687442252808755 test_loss: 0.46031797436106864\n",
      "iteration 5434train_loss: 0.42687442235310524 test_loss: 0.46031807541012126\n",
      "iteration 5435train_loss: 0.4268744221783321 test_loss: 0.46031817639889666\n",
      "iteration 5436train_loss: 0.4268744220037681 test_loss: 0.4603182773274305\n",
      "iteration 5437train_loss: 0.42687442182941276 test_loss: 0.4603183781957586\n",
      "iteration 5438train_loss: 0.4268744216552662 test_loss: 0.460318479003917\n",
      "iteration 5439train_loss: 0.42687442148132787 test_loss: 0.46031857975194146\n",
      "iteration 5440train_loss: 0.42687442130759756 test_loss: 0.46031868043986796\n",
      "iteration 5441train_loss: 0.4268744211340751 test_loss: 0.46031878106773205\n",
      "iteration 5442train_loss: 0.4268744209607603 test_loss: 0.46031888163556967\n",
      "iteration 5443train_loss: 0.42687442078765275 test_loss: 0.4603189821434166\n",
      "iteration 5444train_loss: 0.42687442061475234 test_loss: 0.46031908259130855\n",
      "iteration 5445train_loss: 0.42687442044205864 test_loss: 0.4603191829792812\n",
      "iteration 5446train_loss: 0.4268744202695716 test_loss: 0.46031928330737026\n",
      "iteration 5447train_loss: 0.4268744200972909 test_loss: 0.4603193835756115\n",
      "iteration 5448train_loss: 0.4268744199252163 test_loss: 0.4603194837840404\n",
      "iteration 5449train_loss: 0.4268744197533475 test_loss: 0.46031958393269273\n",
      "iteration 5450train_loss: 0.4268744195816842 test_loss: 0.4603196840216041\n",
      "iteration 5451train_loss: 0.4268744194102264 test_loss: 0.46031978405081014\n",
      "iteration 5452train_loss: 0.4268744192389737 test_loss: 0.4603198840203463\n",
      "iteration 5453train_loss: 0.42687441906792584 test_loss: 0.46031998393024826\n",
      "iteration 5454train_loss: 0.4268744188970825 test_loss: 0.46032008378055134\n",
      "iteration 5455train_loss: 0.42687441872644366 test_loss: 0.46032018357129145\n",
      "iteration 5456train_loss: 0.4268744185560089 test_loss: 0.46032028330250363\n",
      "iteration 5457train_loss: 0.42687441838577794 test_loss: 0.46032038297422356\n",
      "iteration 5458train_loss: 0.42687441821575073 test_loss: 0.4603204825864867\n",
      "iteration 5459train_loss: 0.4268744180459268 test_loss: 0.4603205821393284\n",
      "iteration 5460train_loss: 0.4268744178763061 test_loss: 0.4603206816327841\n",
      "iteration 5461train_loss: 0.42687441770688805 test_loss: 0.46032078106688923\n",
      "iteration 5462train_loss: 0.42687441753767297 test_loss: 0.4603208804416791\n",
      "iteration 5463train_loss: 0.4268744173686602 test_loss: 0.46032097975718905\n",
      "iteration 5464train_loss: 0.4268744171998496 test_loss: 0.4603210790134545\n",
      "iteration 5465train_loss: 0.426874417031241 test_loss: 0.4603211782105105\n",
      "iteration 5466train_loss: 0.426874416862834 test_loss: 0.4603212773483926\n",
      "iteration 5467train_loss: 0.4268744166946285 test_loss: 0.4603213764271359\n",
      "iteration 5468train_loss: 0.42687441652662417 test_loss: 0.4603214754467757\n",
      "iteration 5469train_loss: 0.42687441635882084 test_loss: 0.4603215744073472\n",
      "iteration 5470train_loss: 0.4268744161912182 test_loss: 0.4603216733088856\n",
      "iteration 5471train_loss: 0.42687441602381604 test_loss: 0.46032177215142606\n",
      "iteration 5472train_loss: 0.4268744158566142 test_loss: 0.4603218709350037\n",
      "iteration 5473train_loss: 0.4268744156896122 test_loss: 0.4603219696596536\n",
      "iteration 5474train_loss: 0.4268744155228102 test_loss: 0.4603220683254111\n",
      "iteration 5475train_loss: 0.42687441535620757 test_loss: 0.46032216693231115\n",
      "iteration 5476train_loss: 0.4268744151898043 test_loss: 0.46032226548038874\n",
      "iteration 5477train_loss: 0.4268744150236001 test_loss: 0.460322363969679\n",
      "iteration 5478train_loss: 0.42687441485759453 test_loss: 0.460322462400217\n",
      "iteration 5479train_loss: 0.42687441469178766 test_loss: 0.46032256077203765\n",
      "iteration 5480train_loss: 0.4268744145261792 test_loss: 0.46032265908517606\n",
      "iteration 5481train_loss: 0.42687441436076873 test_loss: 0.46032275733966704\n",
      "iteration 5482train_loss: 0.4268744141955561 test_loss: 0.46032285553554564\n",
      "iteration 5483train_loss: 0.4268744140305412 test_loss: 0.46032295367284676\n",
      "iteration 5484train_loss: 0.4268744138657237 test_loss: 0.4603230517516052\n",
      "iteration 5485train_loss: 0.42687441370110313 test_loss: 0.46032314977185607\n",
      "iteration 5486train_loss: 0.4268744135366797 test_loss: 0.46032324773363403\n",
      "iteration 5487train_loss: 0.42687441337245285 test_loss: 0.46032334563697397\n",
      "iteration 5488train_loss: 0.4268744132084225 test_loss: 0.46032344348191084\n",
      "iteration 5489train_loss: 0.4268744130445883 test_loss: 0.4603235412684792\n",
      "iteration 5490train_loss: 0.4268744128809501 test_loss: 0.460323638996714\n",
      "iteration 5491train_loss: 0.42687441271750765 test_loss: 0.4603237366666501\n",
      "iteration 5492train_loss: 0.4268744125542608 test_loss: 0.46032383427832185\n",
      "iteration 5493train_loss: 0.42687441239120905 test_loss: 0.4603239318317644\n",
      "iteration 5494train_loss: 0.4268744122283524 test_loss: 0.4603240293270121\n",
      "iteration 5495train_loss: 0.4268744120656906 test_loss: 0.46032412676409995\n",
      "iteration 5496train_loss: 0.4268744119032233 test_loss: 0.4603242241430623\n",
      "iteration 5497train_loss: 0.42687441174095037 test_loss: 0.4603243214639339\n",
      "iteration 5498train_loss: 0.4268744115788715 test_loss: 0.46032441872674945\n",
      "iteration 5499train_loss: 0.42687441141698657 test_loss: 0.46032451593154355\n",
      "iteration 5500train_loss: 0.42687441125529524 test_loss: 0.46032461307835054\n",
      "iteration 5501train_loss: 0.42687441109379737 test_loss: 0.4603247101672053\n",
      "iteration 5502train_loss: 0.4268744109324925 test_loss: 0.460324807198142\n",
      "iteration 5503train_loss: 0.4268744107713808 test_loss: 0.4603249041711955\n",
      "iteration 5504train_loss: 0.42687441061046166 test_loss: 0.4603250010864001\n",
      "iteration 5505train_loss: 0.42687441044973506 test_loss: 0.4603250979437903\n",
      "iteration 5506train_loss: 0.42687441028920076 test_loss: 0.4603251947434004\n",
      "iteration 5507train_loss: 0.4268744101288584 test_loss: 0.4603252914852651\n",
      "iteration 5508train_loss: 0.4268744099687079 test_loss: 0.46032538816941876\n",
      "iteration 5509train_loss: 0.42687440980874886 test_loss: 0.46032548479589563\n",
      "iteration 5510train_loss: 0.42687440964898127 test_loss: 0.46032558136473\n",
      "iteration 5511train_loss: 0.4268744094894047 test_loss: 0.4603256778759566\n",
      "iteration 5512train_loss: 0.4268744093300191 test_loss: 0.4603257743296094\n",
      "iteration 5513train_loss: 0.4268744091708241 test_loss: 0.46032587072572284\n",
      "iteration 5514train_loss: 0.42687440901181944 test_loss: 0.46032596706433115\n",
      "iteration 5515train_loss: 0.42687440885300504 test_loss: 0.4603260633454686\n",
      "iteration 5516train_loss: 0.42687440869438065 test_loss: 0.4603261595691696\n",
      "iteration 5517train_loss: 0.4268744085359459 test_loss: 0.46032625573546815\n",
      "iteration 5518train_loss: 0.42687440837770074 test_loss: 0.4603263518443986\n",
      "iteration 5519train_loss: 0.42687440821964484 test_loss: 0.46032644789599503\n",
      "iteration 5520train_loss: 0.42687440806177795 test_loss: 0.46032654389029165\n",
      "iteration 5521train_loss: 0.42687440790409986 test_loss: 0.46032663982732264\n",
      "iteration 5522train_loss: 0.4268744077466105 test_loss: 0.4603267357071221\n",
      "iteration 5523train_loss: 0.4268744075893095 test_loss: 0.46032683152972403\n",
      "iteration 5524train_loss: 0.4268744074321965 test_loss: 0.46032692729516284\n",
      "iteration 5525train_loss: 0.4268744072752716 test_loss: 0.4603270230034721\n",
      "iteration 5526train_loss: 0.42687440711853436 test_loss: 0.4603271186546862\n",
      "iteration 5527train_loss: 0.4268744069619845 test_loss: 0.4603272142488392\n",
      "iteration 5528train_loss: 0.426874406805622 test_loss: 0.4603273097859649\n",
      "iteration 5529train_loss: 0.42687440664944637 test_loss: 0.46032740526609733\n",
      "iteration 5530train_loss: 0.4268744064934577 test_loss: 0.4603275006892705\n",
      "iteration 5531train_loss: 0.42687440633765555 test_loss: 0.46032759605551826\n",
      "iteration 5532train_loss: 0.42687440618203976 test_loss: 0.4603276913648747\n",
      "iteration 5533train_loss: 0.4268744060266102 test_loss: 0.46032778661737356\n",
      "iteration 5534train_loss: 0.4268744058713664 test_loss: 0.46032788181304873\n",
      "iteration 5535train_loss: 0.42687440571630836 test_loss: 0.4603279769519343\n",
      "iteration 5536train_loss: 0.4268744055614358 test_loss: 0.4603280720340638\n",
      "iteration 5537train_loss: 0.4268744054067484 test_loss: 0.46032816705947127\n",
      "iteration 5538train_loss: 0.42687440525224607 test_loss: 0.4603282620281904\n",
      "iteration 5539train_loss: 0.4268744050979285 test_loss: 0.460328356940255\n",
      "iteration 5540train_loss: 0.42687440494379564 test_loss: 0.4603284517956988\n",
      "iteration 5541train_loss: 0.42687440478984706 test_loss: 0.4603285465945555\n",
      "iteration 5542train_loss: 0.4268744046360826 test_loss: 0.46032864133685897\n",
      "iteration 5543train_loss: 0.4268744044825021 test_loss: 0.4603287360226428\n",
      "iteration 5544train_loss: 0.42687440432910523 test_loss: 0.46032883065194063\n",
      "iteration 5545train_loss: 0.42687440417589195 test_loss: 0.46032892522478636\n",
      "iteration 5546train_loss: 0.4268744040228618 test_loss: 0.4603290197412133\n",
      "iteration 5547train_loss: 0.4268744038700147 test_loss: 0.4603291142012553\n",
      "iteration 5548train_loss: 0.4268744037173505 test_loss: 0.46032920860494586\n",
      "iteration 5549train_loss: 0.42687440356486883 test_loss: 0.4603293029523186\n",
      "iteration 5550train_loss: 0.4268744034125696 test_loss: 0.46032939724340705\n",
      "iteration 5551train_loss: 0.4268744032604525 test_loss: 0.46032949147824476\n",
      "iteration 5552train_loss: 0.4268744031085174 test_loss: 0.46032958565686527\n",
      "iteration 5553train_loss: 0.42687440295676393 test_loss: 0.4603296797793021\n",
      "iteration 5554train_loss: 0.42687440280519195 test_loss: 0.4603297738455886\n",
      "iteration 5555train_loss: 0.4268744026538014 test_loss: 0.46032986785575847\n",
      "iteration 5556train_loss: 0.4268744025025918 test_loss: 0.4603299618098449\n",
      "iteration 5557train_loss: 0.4268744023515631 test_loss: 0.46033005570788144\n",
      "iteration 5558train_loss: 0.426874402200715 test_loss: 0.4603301495499015\n",
      "iteration 5559train_loss: 0.42687440205004734 test_loss: 0.4603302433359383\n",
      "iteration 5560train_loss: 0.42687440189956005 test_loss: 0.4603303370660256\n",
      "iteration 5561train_loss: 0.4268744017492525 test_loss: 0.46033043074019625\n",
      "iteration 5562train_loss: 0.4268744015991249 test_loss: 0.4603305243584838\n",
      "iteration 5563train_loss: 0.4268744014491768 test_loss: 0.46033061792092167\n",
      "iteration 5564train_loss: 0.426874401299408 test_loss: 0.46033071142754295\n",
      "iteration 5565train_loss: 0.4268744011498184 test_loss: 0.460330804878381\n",
      "iteration 5566train_loss: 0.4268744010004078 test_loss: 0.460330898273469\n",
      "iteration 5567train_loss: 0.4268744008511758 test_loss: 0.4603309916128402\n",
      "iteration 5568train_loss: 0.4268744007021223 test_loss: 0.46033108489652785\n",
      "iteration 5569train_loss: 0.4268744005532471 test_loss: 0.4603311781245651\n",
      "iteration 5570train_loss: 0.42687440040454994 test_loss: 0.460331271296985\n",
      "iteration 5571train_loss: 0.4268744002560307 test_loss: 0.4603313644138209\n",
      "iteration 5572train_loss: 0.42687440010768896 test_loss: 0.46033145747510584\n",
      "iteration 5573train_loss: 0.4268743999595247 test_loss: 0.46033155048087276\n",
      "iteration 5574train_loss: 0.42687439981153763 test_loss: 0.460331643431155\n",
      "iteration 5575train_loss: 0.4268743996637276 test_loss: 0.46033173632598545\n",
      "iteration 5576train_loss: 0.4268743995160944 test_loss: 0.46033182916539733\n",
      "iteration 5577train_loss: 0.42687439936863775 test_loss: 0.4603319219494235\n",
      "iteration 5578train_loss: 0.42687439922135745 test_loss: 0.46033201467809703\n",
      "iteration 5579train_loss: 0.42687439907425334 test_loss: 0.4603321073514509\n",
      "iteration 5580train_loss: 0.42687439892732504 test_loss: 0.4603321999695181\n",
      "iteration 5581train_loss: 0.4268743987805726 test_loss: 0.46033229253233154\n",
      "iteration 5582train_loss: 0.4268743986339957 test_loss: 0.46033238503992413\n",
      "iteration 5583train_loss: 0.426874398487594 test_loss: 0.46033247749232886\n",
      "iteration 5584train_loss: 0.42687439834136737 test_loss: 0.46033256988957855\n",
      "iteration 5585train_loss: 0.42687439819531586 test_loss: 0.4603326622317061\n",
      "iteration 5586train_loss: 0.42687439804943883 test_loss: 0.4603327545187443\n",
      "iteration 5587train_loss: 0.4268743979037364 test_loss: 0.4603328467507261\n",
      "iteration 5588train_loss: 0.4268743977582081 test_loss: 0.4603329389276842\n",
      "iteration 5589train_loss: 0.42687439761285395 test_loss: 0.4603330310496514\n",
      "iteration 5590train_loss: 0.42687439746767364 test_loss: 0.4603331231166605\n",
      "iteration 5591train_loss: 0.4268743973226669 test_loss: 0.4603332151287443\n",
      "iteration 5592train_loss: 0.4268743971778337 test_loss: 0.46033330708593545\n",
      "iteration 5593train_loss: 0.4268743970331737 test_loss: 0.46033339898826675\n",
      "iteration 5594train_loss: 0.42687439688868667 test_loss: 0.4603334908357708\n",
      "iteration 5595train_loss: 0.4268743967443725 test_loss: 0.4603335826284803\n",
      "iteration 5596train_loss: 0.42687439660023085 test_loss: 0.460333674366428\n",
      "iteration 5597train_loss: 0.42687439645626163 test_loss: 0.46033376604964643\n",
      "iteration 5598train_loss: 0.42687439631246465 test_loss: 0.4603338576781682\n",
      "iteration 5599train_loss: 0.42687439616883965 test_loss: 0.4603339492520259\n",
      "iteration 5600train_loss: 0.4268743960253863 test_loss: 0.4603340407712523\n",
      "iteration 5601train_loss: 0.4268743958821047 test_loss: 0.46033413223587966\n",
      "iteration 5602train_loss: 0.4268743957389944 test_loss: 0.4603342236459406\n",
      "iteration 5603train_loss: 0.4268743955960553 test_loss: 0.4603343150014679\n",
      "iteration 5604train_loss: 0.42687439545328704 test_loss: 0.4603344063024938\n",
      "iteration 5605train_loss: 0.4268743953106896 test_loss: 0.46033449754905076\n",
      "iteration 5606train_loss: 0.42687439516826275 test_loss: 0.46033458874117134\n",
      "iteration 5607train_loss: 0.4268743950260062 test_loss: 0.46033467987888804\n",
      "iteration 5608train_loss: 0.4268743948839198 test_loss: 0.4603347709622332\n",
      "iteration 5609train_loss: 0.4268743947420032 test_loss: 0.46033486199123913\n",
      "iteration 5610train_loss: 0.42687439460025656 test_loss: 0.4603349529659384\n",
      "iteration 5611train_loss: 0.4268743944586794 test_loss: 0.46033504388636326\n",
      "iteration 5612train_loss: 0.4268743943172714 test_loss: 0.4603351347525461\n",
      "iteration 5613train_loss: 0.4268743941760327 test_loss: 0.4603352255645193\n",
      "iteration 5614train_loss: 0.4268743940349628 test_loss: 0.460335316322315\n",
      "iteration 5615train_loss: 0.42687439389406173 test_loss: 0.46033540702596565\n",
      "iteration 5616train_loss: 0.42687439375332903 test_loss: 0.46033549767550336\n",
      "iteration 5617train_loss: 0.42687439361276486 test_loss: 0.46033558827096066\n",
      "iteration 5618train_loss: 0.4268743934723686 test_loss: 0.4603356788123694\n",
      "iteration 5619train_loss: 0.4268743933321404 test_loss: 0.4603357692997621\n",
      "iteration 5620train_loss: 0.4268743931920798 test_loss: 0.46033585973317076\n",
      "iteration 5621train_loss: 0.42687439305218683 test_loss: 0.46033595011262773\n",
      "iteration 5622train_loss: 0.42687439291246104 test_loss: 0.4603360404381651\n",
      "iteration 5623train_loss: 0.4268743927729025 test_loss: 0.46033613070981483\n",
      "iteration 5624train_loss: 0.42687439263351074 test_loss: 0.46033622092760934\n",
      "iteration 5625train_loss: 0.4268743924942857 test_loss: 0.4603363110915804\n",
      "iteration 5626train_loss: 0.4268743923552272 test_loss: 0.4603364012017603\n",
      "iteration 5627train_loss: 0.426874392216335 test_loss: 0.4603364912581811\n",
      "iteration 5628train_loss: 0.42687439207760897 test_loss: 0.4603365812608748\n",
      "iteration 5629train_loss: 0.4268743919390488 test_loss: 0.46033667120987337\n",
      "iteration 5630train_loss: 0.4268743918006545 test_loss: 0.46033676110520894\n",
      "iteration 5631train_loss: 0.4268743916624255 test_loss: 0.4603368509469133\n",
      "iteration 5632train_loss: 0.4268743915243619 test_loss: 0.4603369407350185\n",
      "iteration 5633train_loss: 0.4268743913864633 test_loss: 0.4603370304695565\n",
      "iteration 5634train_loss: 0.4268743912487297 test_loss: 0.46033712015055933\n",
      "iteration 5635train_loss: 0.426874391111161 test_loss: 0.4603372097780587\n",
      "iteration 5636train_loss: 0.4268743909737567 test_loss: 0.46033729935208656\n",
      "iteration 5637train_loss: 0.42687439083651674 test_loss: 0.46033738887267495\n",
      "iteration 5638train_loss: 0.4268743906994407 test_loss: 0.46033747833985544\n",
      "iteration 5639train_loss: 0.4268743905625289 test_loss: 0.4603375677536601\n",
      "iteration 5640train_loss: 0.4268743904257808 test_loss: 0.4603376571141206\n",
      "iteration 5641train_loss: 0.4268743902891961 test_loss: 0.4603377464212688\n",
      "iteration 5642train_loss: 0.42687439015277484 test_loss: 0.4603378356751364\n",
      "iteration 5643train_loss: 0.42687439001651667 test_loss: 0.4603379248757552\n",
      "iteration 5644train_loss: 0.4268743898804216 test_loss: 0.46033801402315694\n",
      "iteration 5645train_loss: 0.42687438974448916 test_loss: 0.46033810311737333\n",
      "iteration 5646train_loss: 0.4268743896087193 test_loss: 0.46033819215843613\n",
      "iteration 5647train_loss: 0.42687438947311185 test_loss: 0.4603382811463769\n",
      "iteration 5648train_loss: 0.42687438933766664 test_loss: 0.4603383700812275\n",
      "iteration 5649train_loss: 0.42687438920238335 test_loss: 0.46033845896301934\n",
      "iteration 5650train_loss: 0.4268743890672618 test_loss: 0.46033854779178407\n",
      "iteration 5651train_loss: 0.426874388932302 test_loss: 0.46033863656755347\n",
      "iteration 5652train_loss: 0.42687438879750356 test_loss: 0.4603387252903589\n",
      "iteration 5653train_loss: 0.4268743886628662 test_loss: 0.46033881396023213\n",
      "iteration 5654train_loss: 0.42687438852838994 test_loss: 0.46033890257720467\n",
      "iteration 5655train_loss: 0.4268743883940745 test_loss: 0.4603389911413079\n",
      "iteration 5656train_loss: 0.42687438825991963 test_loss: 0.4603390796525734\n",
      "iteration 5657train_loss: 0.42687438812592526 test_loss: 0.4603391681110328\n",
      "iteration 5658train_loss: 0.4268743879920912 test_loss: 0.4603392565167175\n",
      "iteration 5659train_loss: 0.426874387858417 test_loss: 0.46033934486965883\n",
      "iteration 5660train_loss: 0.426874387724903 test_loss: 0.46033943316988835\n",
      "iteration 5661train_loss: 0.4268743875915485 test_loss: 0.4603395214174375\n",
      "iteration 5662train_loss: 0.42687438745835343 test_loss: 0.4603396096123375\n",
      "iteration 5663train_loss: 0.42687438732531763 test_loss: 0.4603396977546199\n",
      "iteration 5664train_loss: 0.4268743871924411 test_loss: 0.460339785844316\n",
      "iteration 5665train_loss: 0.4268743870597233 test_loss: 0.4603398738814572\n",
      "iteration 5666train_loss: 0.42687438692716434 test_loss: 0.4603399618660748\n",
      "iteration 5667train_loss: 0.4268743867947639 test_loss: 0.4603400497981999\n",
      "iteration 5668train_loss: 0.42687438666252164 test_loss: 0.4603401376778642\n",
      "iteration 5669train_loss: 0.4268743865304377 test_loss: 0.4603402255050987\n",
      "iteration 5670train_loss: 0.4268743863985117 test_loss: 0.46034031327993447\n",
      "iteration 5671train_loss: 0.42687438626674346 test_loss: 0.46034040100240314\n",
      "iteration 5672train_loss: 0.4268743861351328 test_loss: 0.4603404886725357\n",
      "iteration 5673train_loss: 0.4268743860036796 test_loss: 0.4603405762903634\n",
      "iteration 5674train_loss: 0.4268743858723835 test_loss: 0.46034066385591743\n",
      "iteration 5675train_loss: 0.4268743857412444 test_loss: 0.46034075136922886\n",
      "iteration 5676train_loss: 0.4268743856102622 test_loss: 0.4603408388303289\n",
      "iteration 5677train_loss: 0.4268743854794367 test_loss: 0.46034092623924855\n",
      "iteration 5678train_loss: 0.4268743853487675 test_loss: 0.46034101359601914\n",
      "iteration 5679train_loss: 0.42687438521825466 test_loss: 0.46034110090067154\n",
      "iteration 5680train_loss: 0.42687438508789793 test_loss: 0.46034118815323705\n",
      "iteration 5681train_loss: 0.4268743849576971 test_loss: 0.46034127535374636\n",
      "iteration 5682train_loss: 0.42687438482765194 test_loss: 0.4603413625022308\n",
      "iteration 5683train_loss: 0.42687438469776234 test_loss: 0.4603414495987213\n",
      "iteration 5684train_loss: 0.426874384568028 test_loss: 0.46034153664324884\n",
      "iteration 5685train_loss: 0.4268743844384489 test_loss: 0.46034162363584424\n",
      "iteration 5686train_loss: 0.42687438430902463 test_loss: 0.4603417105765387\n",
      "iteration 5687train_loss: 0.4268743841797553 test_loss: 0.460341797465363\n",
      "iteration 5688train_loss: 0.4268743840506405 test_loss: 0.4603418843023482\n",
      "iteration 5689train_loss: 0.4268743839216801 test_loss: 0.46034197108752495\n",
      "iteration 5690train_loss: 0.4268743837928739 test_loss: 0.4603420578209243\n",
      "iteration 5691train_loss: 0.42687438366422176 test_loss: 0.4603421445025773\n",
      "iteration 5692train_loss: 0.4268743835357234 test_loss: 0.4603422311325144\n",
      "iteration 5693train_loss: 0.4268743834073789 test_loss: 0.46034231771076667\n",
      "iteration 5694train_loss: 0.4268743832791877 test_loss: 0.4603424042373648\n",
      "iteration 5695train_loss: 0.42687438315114984 test_loss: 0.4603424907123397\n",
      "iteration 5696train_loss: 0.4268743830232651 test_loss: 0.460342577135722\n",
      "iteration 5697train_loss: 0.4268743828955335 test_loss: 0.46034266350754255\n",
      "iteration 5698train_loss: 0.4268743827679544 test_loss: 0.46034274982783213\n",
      "iteration 5699train_loss: 0.42687438264052796 test_loss: 0.4603428360966213\n",
      "iteration 5700train_loss: 0.4268743825132539 test_loss: 0.4603429223139409\n",
      "iteration 5701train_loss: 0.42687438238613207 test_loss: 0.4603430084798215\n",
      "iteration 5702train_loss: 0.4268743822591622 test_loss: 0.4603430945942938\n",
      "iteration 5703train_loss: 0.42687438213234424 test_loss: 0.4603431806573885\n",
      "iteration 5704train_loss: 0.42687438200567795 test_loss: 0.4603432666691361\n",
      "iteration 5705train_loss: 0.426874381879163 test_loss: 0.46034335262956727\n",
      "iteration 5706train_loss: 0.42687438175279946 test_loss: 0.46034343853871273\n",
      "iteration 5707train_loss: 0.426874381626587 test_loss: 0.46034352439660287\n",
      "iteration 5708train_loss: 0.42687438150052553 test_loss: 0.46034361020326825\n",
      "iteration 5709train_loss: 0.4268743813746147 test_loss: 0.4603436959587395\n",
      "iteration 5710train_loss: 0.4268743812488545 test_loss: 0.46034378166304707\n",
      "iteration 5711train_loss: 0.42687438112324483 test_loss: 0.4603438673162215\n",
      "iteration 5712train_loss: 0.4268743809977852 test_loss: 0.4603439529182931\n",
      "iteration 5713train_loss: 0.42687438087247576 test_loss: 0.4603440384692926\n",
      "iteration 5714train_loss: 0.42687438074731604 test_loss: 0.4603441239692503\n",
      "iteration 5715train_loss: 0.42687438062230604 test_loss: 0.4603442094181965\n",
      "iteration 5716train_loss: 0.4268743804974454 test_loss: 0.46034429481616185\n",
      "iteration 5717train_loss: 0.42687438037273423 test_loss: 0.46034438016317664\n",
      "iteration 5718train_loss: 0.42687438024817215 test_loss: 0.46034446545927116\n",
      "iteration 5719train_loss: 0.42687438012375906 test_loss: 0.46034455070447594\n",
      "iteration 5720train_loss: 0.4268743799994947 test_loss: 0.46034463589882113\n",
      "iteration 5721train_loss: 0.426874379875379 test_loss: 0.4603447210423371\n",
      "iteration 5722train_loss: 0.42687437975141174 test_loss: 0.46034480613505413\n",
      "iteration 5723train_loss: 0.4268743796275926 test_loss: 0.46034489117700267\n",
      "iteration 5724train_loss: 0.4268743795039217 test_loss: 0.46034497616821274\n",
      "iteration 5725train_loss: 0.4268743793803985 test_loss: 0.4603450611087146\n",
      "iteration 5726train_loss: 0.4268743792570232 test_loss: 0.4603451459985387\n",
      "iteration 5727train_loss: 0.4268743791337954 test_loss: 0.46034523083771506\n",
      "iteration 5728train_loss: 0.4268743790107149 test_loss: 0.460345315626274\n",
      "iteration 5729train_loss: 0.42687437888778157 test_loss: 0.4603454003642455\n",
      "iteration 5730train_loss: 0.4268743787649953 test_loss: 0.46034548505165984\n",
      "iteration 5731train_loss: 0.42687437864235594 test_loss: 0.46034556968854723\n",
      "iteration 5732train_loss: 0.42687437851986304 test_loss: 0.4603456542749377\n",
      "iteration 5733train_loss: 0.42687437839751685 test_loss: 0.4603457388108612\n",
      "iteration 5734train_loss: 0.4268743782753168 test_loss: 0.46034582329634816\n",
      "iteration 5735train_loss: 0.42687437815326296 test_loss: 0.46034590773142825\n",
      "iteration 5736train_loss: 0.4268743780313551 test_loss: 0.4603459921161318\n",
      "iteration 5737train_loss: 0.4268743779095928 test_loss: 0.46034607645048875\n",
      "iteration 5738train_loss: 0.4268743777879764 test_loss: 0.46034616073452916\n",
      "iteration 5739train_loss: 0.42687437766650527 test_loss: 0.4603462449682829\n",
      "iteration 5740train_loss: 0.42687437754517943 test_loss: 0.46034632915178\n",
      "iteration 5741train_loss: 0.42687437742399875 test_loss: 0.4603464132850505\n",
      "iteration 5742train_loss: 0.42687437730296285 test_loss: 0.4603464973681242\n",
      "iteration 5743train_loss: 0.4268743771820718 test_loss: 0.46034658140103124\n",
      "iteration 5744train_loss: 0.42687437706132525 test_loss: 0.46034666538380126\n",
      "iteration 5745train_loss: 0.4268743769407231 test_loss: 0.46034674931646424\n",
      "iteration 5746train_loss: 0.42687437682026524 test_loss: 0.4603468331990501\n",
      "iteration 5747train_loss: 0.4268743766999514 test_loss: 0.4603469170315887\n",
      "iteration 5748train_loss: 0.4268743765797814 test_loss: 0.46034700081410984\n",
      "iteration 5749train_loss: 0.42687437645975507 test_loss: 0.46034708454664336\n",
      "iteration 5750train_loss: 0.4268743763398723 test_loss: 0.46034716822921895\n",
      "iteration 5751train_loss: 0.42687437622013286 test_loss: 0.46034725186186654\n",
      "iteration 5752train_loss: 0.4268743761005367 test_loss: 0.46034733544461565\n",
      "iteration 5753train_loss: 0.42687437598108346 test_loss: 0.46034741897749637\n",
      "iteration 5754train_loss: 0.4268743758617732 test_loss: 0.4603475024605381\n",
      "iteration 5755train_loss: 0.42687437574260545 test_loss: 0.4603475858937707\n",
      "iteration 5756train_loss: 0.42687437562358027 test_loss: 0.46034766927722415\n",
      "iteration 5757train_loss: 0.4268743755046974 test_loss: 0.4603477526109276\n",
      "iteration 5758train_loss: 0.4268743753859567 test_loss: 0.46034783589491085\n",
      "iteration 5759train_loss: 0.426874375267358 test_loss: 0.46034791912920375\n",
      "iteration 5760train_loss: 0.4268743751489012 test_loss: 0.46034800231383566\n",
      "iteration 5761train_loss: 0.4268743750305859 test_loss: 0.46034808544883654\n",
      "iteration 5762train_loss: 0.42687437491241215 test_loss: 0.4603481685342356\n",
      "iteration 5763train_loss: 0.4268743747943797 test_loss: 0.46034825157006254\n",
      "iteration 5764train_loss: 0.4268743746764884 test_loss: 0.460348334556347\n",
      "iteration 5765train_loss: 0.4268743745587382 test_loss: 0.46034841749311833\n",
      "iteration 5766train_loss: 0.4268743744411287 test_loss: 0.4603485003804064\n",
      "iteration 5767train_loss: 0.42687437432365977 test_loss: 0.4603485832182403\n",
      "iteration 5768train_loss: 0.4268743742063314 test_loss: 0.4603486660066497\n",
      "iteration 5769train_loss: 0.42687437408914336 test_loss: 0.46034874874566395\n",
      "iteration 5770train_loss: 0.4268743739720954 test_loss: 0.46034883143531263\n",
      "iteration 5771train_loss: 0.42687437385518734 test_loss: 0.46034891407562517\n",
      "iteration 5772train_loss: 0.4268743737384192 test_loss: 0.46034899666663087\n",
      "iteration 5773train_loss: 0.42687437362179054 test_loss: 0.46034907920835916\n",
      "iteration 5774train_loss: 0.4268743735053015 test_loss: 0.4603491617008395\n",
      "iteration 5775train_loss: 0.4268743733889517 test_loss: 0.4603492441441011\n",
      "iteration 5776train_loss: 0.426874373272741 test_loss: 0.46034932653817345\n",
      "iteration 5777train_loss: 0.4268743731566693 test_loss: 0.46034940888308573\n",
      "iteration 5778train_loss: 0.4268743730407364 test_loss: 0.46034949117886736\n",
      "iteration 5779train_loss: 0.4268743729249421 test_loss: 0.46034957342554766\n",
      "iteration 5780train_loss: 0.4268743728092863 test_loss: 0.4603496556231557\n",
      "iteration 5781train_loss: 0.4268743726937688 test_loss: 0.4603497377717209\n",
      "iteration 5782train_loss: 0.42687437257838934 test_loss: 0.4603498198712725\n",
      "iteration 5783train_loss: 0.426874372463148 test_loss: 0.46034990192183955\n",
      "iteration 5784train_loss: 0.4268743723480444 test_loss: 0.4603499839234515\n",
      "iteration 5785train_loss: 0.42687437223307834 test_loss: 0.4603500658761373\n",
      "iteration 5786train_loss: 0.4268743721182499 test_loss: 0.4603501477799262\n",
      "iteration 5787train_loss: 0.42687437200355866 test_loss: 0.46035022963484745\n",
      "iteration 5788train_loss: 0.4268743718890047 test_loss: 0.46035031144093014\n",
      "iteration 5789train_loss: 0.42687437177458754 test_loss: 0.46035039319820314\n",
      "iteration 5790train_loss: 0.4268743716603073 test_loss: 0.460350474906696\n",
      "iteration 5791train_loss: 0.42687437154616376 test_loss: 0.4603505565664374\n",
      "iteration 5792train_loss: 0.4268743714321567 test_loss: 0.4603506381774566\n",
      "iteration 5793train_loss: 0.42687437131828593 test_loss: 0.46035071973978253\n",
      "iteration 5794train_loss: 0.42687437120455124 test_loss: 0.4603508012534443\n",
      "iteration 5795train_loss: 0.42687437109095266 test_loss: 0.4603508827184709\n",
      "iteration 5796train_loss: 0.4268743709774899 test_loss: 0.4603509641348913\n",
      "iteration 5797train_loss: 0.42687437086416274 test_loss: 0.46035104550273453\n",
      "iteration 5798train_loss: 0.4268743707509712 test_loss: 0.4603511268220296\n",
      "iteration 5799train_loss: 0.426874370637915 test_loss: 0.46035120809280516\n",
      "iteration 5800train_loss: 0.42687437052499383 test_loss: 0.46035128931509045\n",
      "iteration 5801train_loss: 0.42687437041220794 test_loss: 0.46035137048891434\n",
      "iteration 5802train_loss: 0.42687437029955666 test_loss: 0.4603514516143055\n",
      "iteration 5803train_loss: 0.4268743701870403 test_loss: 0.46035153269129303\n",
      "iteration 5804train_loss: 0.4268743700746584 test_loss: 0.4603516137199058\n",
      "iteration 5805train_loss: 0.42687436996241085 test_loss: 0.46035169470017234\n",
      "iteration 5806train_loss: 0.4268743698502975 test_loss: 0.4603517756321218\n",
      "iteration 5807train_loss: 0.4268743697383182 test_loss: 0.4603518565157829\n",
      "iteration 5808train_loss: 0.4268743696264729 test_loss: 0.46035193735118435\n",
      "iteration 5809train_loss: 0.42687436951476126 test_loss: 0.4603520181383549\n",
      "iteration 5810train_loss: 0.42687436940318324 test_loss: 0.4603520988773234\n",
      "iteration 5811train_loss: 0.42687436929173866 test_loss: 0.4603521795681186\n",
      "iteration 5812train_loss: 0.42687436918042726 test_loss: 0.46035226021076914\n",
      "iteration 5813train_loss: 0.42687436906924914 test_loss: 0.46035234080530374\n",
      "iteration 5814train_loss: 0.4268743689582038 test_loss: 0.46035242135175103\n",
      "iteration 5815train_loss: 0.4268743688472912 test_loss: 0.46035250185013976\n",
      "iteration 5816train_loss: 0.4268743687365113 test_loss: 0.4603525823004986\n",
      "iteration 5817train_loss: 0.426874368625864 test_loss: 0.46035266270285613\n",
      "iteration 5818train_loss: 0.4268743685153487 test_loss: 0.46035274305724094\n",
      "iteration 5819train_loss: 0.4268743684049658 test_loss: 0.4603528233636816\n",
      "iteration 5820train_loss: 0.42687436829471476 test_loss: 0.4603529036222068\n",
      "iteration 5821train_loss: 0.42687436818459557 test_loss: 0.46035298383284523\n",
      "iteration 5822train_loss: 0.4268743680746081 test_loss: 0.46035306399562503\n",
      "iteration 5823train_loss: 0.4268743679647521 test_loss: 0.46035314411057504\n",
      "iteration 5824train_loss: 0.4268743678550274 test_loss: 0.46035322417772373\n",
      "iteration 5825train_loss: 0.426874367745434 test_loss: 0.4603533041970995\n",
      "iteration 5826train_loss: 0.4268743676359717 test_loss: 0.460353384168731\n",
      "iteration 5827train_loss: 0.42687436752664026 test_loss: 0.46035346409264655\n",
      "iteration 5828train_loss: 0.4268743674174394 test_loss: 0.46035354396887457\n",
      "iteration 5829train_loss: 0.4268743673083693 test_loss: 0.46035362379744366\n",
      "iteration 5830train_loss: 0.42687436719942945 test_loss: 0.4603537035783821\n",
      "iteration 5831train_loss: 0.42687436709061993 test_loss: 0.4603537833117182\n",
      "iteration 5832train_loss: 0.42687436698194064 test_loss: 0.4603538629974806\n",
      "iteration 5833train_loss: 0.42687436687339114 test_loss: 0.4603539426356974\n",
      "iteration 5834train_loss: 0.4268743667649716 test_loss: 0.4603540222263971\n",
      "iteration 5835train_loss: 0.4268743666566815 test_loss: 0.4603541017696079\n",
      "iteration 5836train_loss: 0.42687436654852096 test_loss: 0.46035418126535826\n",
      "iteration 5837train_loss: 0.4268743664404897 test_loss: 0.4603542607136764\n",
      "iteration 5838train_loss: 0.4268743663325878 test_loss: 0.4603543401145907\n",
      "iteration 5839train_loss: 0.4268743662248147 test_loss: 0.4603544194681293\n",
      "iteration 5840train_loss: 0.42687436611717067 test_loss: 0.4603544987743203\n",
      "iteration 5841train_loss: 0.4268743660096552 test_loss: 0.4603545780331923\n",
      "iteration 5842train_loss: 0.42687436590226824 test_loss: 0.46035465724477304\n",
      "iteration 5843train_loss: 0.42687436579500987 test_loss: 0.4603547364090912\n",
      "iteration 5844train_loss: 0.4268743656878797 test_loss: 0.46035481552617463\n",
      "iteration 5845train_loss: 0.42687436558087755 test_loss: 0.4603548945960516\n",
      "iteration 5846train_loss: 0.4268743654740033 test_loss: 0.4603549736187503\n",
      "iteration 5847train_loss: 0.426874365367257 test_loss: 0.46035505259429876\n",
      "iteration 5848train_loss: 0.4268743652606382 test_loss: 0.4603551315227252\n",
      "iteration 5849train_loss: 0.42687436515414695 test_loss: 0.4603552104040576\n",
      "iteration 5850train_loss: 0.426874365047783 test_loss: 0.46035528923832403\n",
      "iteration 5851train_loss: 0.4268743649415464 test_loss: 0.46035536802555255\n",
      "iteration 5852train_loss: 0.4268743648354366 test_loss: 0.4603554467657714\n",
      "iteration 5853train_loss: 0.4268743647294537 test_loss: 0.4603555254590084\n",
      "iteration 5854train_loss: 0.4268743646235975 test_loss: 0.4603556041052916\n",
      "iteration 5855train_loss: 0.42687436451786803 test_loss: 0.46035568270464894\n",
      "iteration 5856train_loss: 0.4268743644122649 test_loss: 0.4603557612571086\n",
      "iteration 5857train_loss: 0.42687436430678805 test_loss: 0.4603558397626982\n",
      "iteration 5858train_loss: 0.4268743642014373 test_loss: 0.460355918221446\n",
      "iteration 5859train_loss: 0.42687436409621254 test_loss: 0.46035599663338\n",
      "iteration 5860train_loss: 0.42687436399111356 test_loss: 0.4603560749985277\n",
      "iteration 5861train_loss: 0.42687436388614036 test_loss: 0.46035615331691726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5862train_loss: 0.42687436378129245 test_loss: 0.4603562315885764\n",
      "iteration 5863train_loss: 0.4268743636765701 test_loss: 0.46035630981353326\n",
      "iteration 5864train_loss: 0.4268743635719729 test_loss: 0.4603563879918154\n",
      "iteration 5865train_loss: 0.4268743634675008 test_loss: 0.46035646612345077\n",
      "iteration 5866train_loss: 0.42687436336315354 test_loss: 0.4603565442084672\n",
      "iteration 5867train_loss: 0.4268743632589311 test_loss: 0.4603566222468925\n",
      "iteration 5868train_loss: 0.4268743631548333 test_loss: 0.46035670023875436\n",
      "iteration 5869train_loss: 0.4268743630508598 test_loss: 0.4603567781840805\n",
      "iteration 5870train_loss: 0.4268743629470108 test_loss: 0.4603568560828988\n",
      "iteration 5871train_loss: 0.42687436284328606 test_loss: 0.4603569339352369\n",
      "iteration 5872train_loss: 0.42687436273968515 test_loss: 0.4603570117411226\n",
      "iteration 5873train_loss: 0.42687436263620815 test_loss: 0.4603570895005834\n",
      "iteration 5874train_loss: 0.42687436253285477 test_loss: 0.46035716721364717\n",
      "iteration 5875train_loss: 0.42687436242962523 test_loss: 0.46035724488034147\n",
      "iteration 5876train_loss: 0.426874362326519 test_loss: 0.4603573225006941\n",
      "iteration 5877train_loss: 0.4268743622235361 test_loss: 0.4603574000747326\n",
      "iteration 5878train_loss: 0.4268743621206761 test_loss: 0.4603574776024844\n",
      "iteration 5879train_loss: 0.4268743620179393 test_loss: 0.46035755508397735\n",
      "iteration 5880train_loss: 0.4268743619153253 test_loss: 0.4603576325192389\n",
      "iteration 5881train_loss: 0.42687436181283395 test_loss: 0.46035770990829666\n",
      "iteration 5882train_loss: 0.42687436171046517 test_loss: 0.46035778725117815\n",
      "iteration 5883train_loss: 0.4268743616082189 test_loss: 0.46035786454791083\n",
      "iteration 5884train_loss: 0.42687436150609476 test_loss: 0.4603579417985224\n",
      "iteration 5885train_loss: 0.42687436140409263 test_loss: 0.4603580190030403\n",
      "iteration 5886train_loss: 0.42687436130221273 test_loss: 0.4603580961614918\n",
      "iteration 5887train_loss: 0.4268743612004544 test_loss: 0.4603581732739047\n",
      "iteration 5888train_loss: 0.4268743610988178 test_loss: 0.4603582503403061\n",
      "iteration 5889train_loss: 0.4268743609973028 test_loss: 0.4603583273607237\n",
      "iteration 5890train_loss: 0.4268743608959092 test_loss: 0.4603584043351847\n",
      "iteration 5891train_loss: 0.42687436079463664 test_loss: 0.4603584812637167\n",
      "iteration 5892train_loss: 0.42687436069348533 test_loss: 0.4603585581463471\n",
      "iteration 5893train_loss: 0.4268743605924549 test_loss: 0.46035863498310303\n",
      "iteration 5894train_loss: 0.42687436049154537 test_loss: 0.46035871177401205\n",
      "iteration 5895train_loss: 0.42687436039075644 test_loss: 0.4603587885191014\n",
      "iteration 5896train_loss: 0.42687436029008785 test_loss: 0.46035886521839836\n",
      "iteration 5897train_loss: 0.42687436018953984 test_loss: 0.4603589418719303\n",
      "iteration 5898train_loss: 0.42687436008911195 test_loss: 0.46035901847972466\n",
      "iteration 5899train_loss: 0.42687435998880413 test_loss: 0.46035909504180844\n",
      "iteration 5900train_loss: 0.42687435988861633 test_loss: 0.46035917155820893\n",
      "iteration 5901train_loss: 0.4268743597885483 test_loss: 0.4603592480289535\n",
      "iteration 5902train_loss: 0.42687435968859977 test_loss: 0.4603593244540693\n",
      "iteration 5903train_loss: 0.4268743595887709 test_loss: 0.46035940083358357\n",
      "iteration 5904train_loss: 0.4268743594890614 test_loss: 0.4603594771675234\n",
      "iteration 5905train_loss: 0.426874359389471 test_loss: 0.460359553455916\n",
      "iteration 5906train_loss: 0.4268743592899998 test_loss: 0.46035962969878863\n",
      "iteration 5907train_loss: 0.4268743591906474 test_loss: 0.4603597058961683\n",
      "iteration 5908train_loss: 0.42687435909141386 test_loss: 0.4603597820480822\n",
      "iteration 5909train_loss: 0.4268743589922989 test_loss: 0.46035985815455743\n",
      "iteration 5910train_loss: 0.4268743588933025 test_loss: 0.4603599342156211\n",
      "iteration 5911train_loss: 0.42687435879442454 test_loss: 0.4603600102313002\n",
      "iteration 5912train_loss: 0.42687435869566476 test_loss: 0.46036008620162183\n",
      "iteration 5913train_loss: 0.426874358597023 test_loss: 0.46036016212661307\n",
      "iteration 5914train_loss: 0.42687435849849925 test_loss: 0.460360238006301\n",
      "iteration 5915train_loss: 0.42687435840009325 test_loss: 0.46036031384071247\n",
      "iteration 5916train_loss: 0.42687435830180487 test_loss: 0.4603603896298747\n",
      "iteration 5917train_loss: 0.42687435820363406 test_loss: 0.4603604653738143\n",
      "iteration 5918train_loss: 0.42687435810558066 test_loss: 0.46036054107255864\n",
      "iteration 5919train_loss: 0.42687435800764445 test_loss: 0.4603606167261344\n",
      "iteration 5920train_loss: 0.4268743579098253 test_loss: 0.4603606923345686\n",
      "iteration 5921train_loss: 0.4268743578121232 test_loss: 0.46036076789788827\n",
      "iteration 5922train_loss: 0.4268743577145378 test_loss: 0.4603608434161201\n",
      "iteration 5923train_loss: 0.4268743576170691 test_loss: 0.460360918889291\n",
      "iteration 5924train_loss: 0.42687435751971703 test_loss: 0.460360994317428\n",
      "iteration 5925train_loss: 0.42687435742248125 test_loss: 0.46036106970055773\n",
      "iteration 5926train_loss: 0.4268743573253617 test_loss: 0.4603611450387073\n",
      "iteration 5927train_loss: 0.4268743572283584 test_loss: 0.46036122033190324\n",
      "iteration 5928train_loss: 0.42687435713147104 test_loss: 0.4603612955801726\n",
      "iteration 5929train_loss: 0.42687435703469945 test_loss: 0.46036137078354195\n",
      "iteration 5930train_loss: 0.4268743569380437 test_loss: 0.46036144594203815\n",
      "iteration 5931train_loss: 0.4268743568415034 test_loss: 0.46036152105568795\n",
      "iteration 5932train_loss: 0.42687435674507845 test_loss: 0.46036159612451816\n",
      "iteration 5933train_loss: 0.42687435664876894 test_loss: 0.4603616711485555\n",
      "iteration 5934train_loss: 0.42687435655257455 test_loss: 0.4603617461278265\n",
      "iteration 5935train_loss: 0.42687435645649513 test_loss: 0.4603618210623581\n",
      "iteration 5936train_loss: 0.42687435636053067 test_loss: 0.46036189595217675\n",
      "iteration 5937train_loss: 0.4268743562646808 test_loss: 0.46036197079730923\n",
      "iteration 5938train_loss: 0.4268743561689456 test_loss: 0.4603620455977822\n",
      "iteration 5939train_loss: 0.42687435607332486 test_loss: 0.4603621203536222\n",
      "iteration 5940train_loss: 0.4268743559778184 test_loss: 0.46036219506485593\n",
      "iteration 5941train_loss: 0.4268743558824262 test_loss: 0.4603622697315099\n",
      "iteration 5942train_loss: 0.42687435578714805 test_loss: 0.46036234435361073\n",
      "iteration 5943train_loss: 0.4268743556919838 test_loss: 0.4603624189311849\n",
      "iteration 5944train_loss: 0.4268743555969333 test_loss: 0.4603624934642592\n",
      "iteration 5945train_loss: 0.4268743555019965 test_loss: 0.4603625679528599\n",
      "iteration 5946train_loss: 0.42687435540717317 test_loss: 0.4603626423970138\n",
      "iteration 5947train_loss: 0.4268743553124632 test_loss: 0.46036271679674706\n",
      "iteration 5948train_loss: 0.42687435521786643 test_loss: 0.4603627911520864\n",
      "iteration 5949train_loss: 0.42687435512338284 test_loss: 0.4603628654630582\n",
      "iteration 5950train_loss: 0.42687435502901216 test_loss: 0.4603629397296889\n",
      "iteration 5951train_loss: 0.4268743549347543 test_loss: 0.4603630139520049\n",
      "iteration 5952train_loss: 0.42687435484060915 test_loss: 0.4603630881300329\n",
      "iteration 5953train_loss: 0.42687435474657653 test_loss: 0.4603631622637989\n",
      "iteration 5954train_loss: 0.42687435465265644 test_loss: 0.46036323635332965\n",
      "iteration 5955train_loss: 0.4268743545588485 test_loss: 0.4603633103986513\n",
      "iteration 5956train_loss: 0.4268743544651528 test_loss: 0.46036338439979024\n",
      "iteration 5957train_loss: 0.4268743543715691 test_loss: 0.46036345835677284\n",
      "iteration 5958train_loss: 0.4268743542780973 test_loss: 0.4603635322696255\n",
      "iteration 5959train_loss: 0.4268743541847373 test_loss: 0.46036360613837446\n",
      "iteration 5960train_loss: 0.42687435409148883 test_loss: 0.460363679963046\n",
      "iteration 5961train_loss: 0.42687435399835183 test_loss: 0.46036375374366634\n",
      "iteration 5962train_loss: 0.42687435390532624 test_loss: 0.46036382748026194\n",
      "iteration 5963train_loss: 0.4268743538124119 test_loss: 0.4603639011728589\n",
      "iteration 5964train_loss: 0.4268743537196086 test_loss: 0.46036397482148356\n",
      "iteration 5965train_loss: 0.4268743536269162 test_loss: 0.46036404842616196\n",
      "iteration 5966train_loss: 0.4268743535343348 test_loss: 0.46036412198692056\n",
      "iteration 5967train_loss: 0.42687435344186386 test_loss: 0.46036419550378527\n",
      "iteration 5968train_loss: 0.42687435334950363 test_loss: 0.4603642689767825\n",
      "iteration 5969train_loss: 0.4268743532572538 test_loss: 0.46036434240593815\n",
      "iteration 5970train_loss: 0.4268743531651143 test_loss: 0.4603644157912787\n",
      "iteration 5971train_loss: 0.42687435307308486 test_loss: 0.46036448913282996\n",
      "iteration 5972train_loss: 0.4268743529811655 test_loss: 0.46036456243061813\n",
      "iteration 5973train_loss: 0.426874352889356 test_loss: 0.46036463568466945\n",
      "iteration 5974train_loss: 0.42687435279765623 test_loss: 0.46036470889500986\n",
      "iteration 5975train_loss: 0.42687435270606616 test_loss: 0.4603647820616653\n",
      "iteration 5976train_loss: 0.42687435261458556 test_loss: 0.460364855184662\n",
      "iteration 5977train_loss: 0.4268743525232144 test_loss: 0.4603649282640262\n",
      "iteration 5978train_loss: 0.42687435243195243 test_loss: 0.4603650012997835\n",
      "iteration 5979train_loss: 0.42687435234079957 test_loss: 0.46036507429196005\n",
      "iteration 5980train_loss: 0.4268743522497557 test_loss: 0.46036514724058186\n",
      "iteration 5981train_loss: 0.4268743521588206 test_loss: 0.460365220145675\n",
      "iteration 5982train_loss: 0.4268743520679943 test_loss: 0.4603652930072653\n",
      "iteration 5983train_loss: 0.4268743519772766 test_loss: 0.46036536582537874\n",
      "iteration 5984train_loss: 0.4268743518866674 test_loss: 0.4603654386000411\n",
      "iteration 5985train_loss: 0.42687435179616645 test_loss: 0.4603655113312785\n",
      "iteration 5986train_loss: 0.4268743517057737 test_loss: 0.46036558401911676\n",
      "iteration 5987train_loss: 0.4268743516154889 test_loss: 0.4603656566635817\n",
      "iteration 5988train_loss: 0.4268743515253123 test_loss: 0.4603657292646993\n",
      "iteration 5989train_loss: 0.4268743514352434 test_loss: 0.4603658018224952\n",
      "iteration 5990train_loss: 0.4268743513452822 test_loss: 0.46036587433699544\n",
      "iteration 5991train_loss: 0.4268743512554285 test_loss: 0.46036594680822585\n",
      "iteration 5992train_loss: 0.4268743511656821 test_loss: 0.46036601923621207\n",
      "iteration 5993train_loss: 0.42687435107604327 test_loss: 0.4603660916209799\n",
      "iteration 5994train_loss: 0.4268743509865114 test_loss: 0.46036616396255514\n",
      "iteration 5995train_loss: 0.4268743508970867 test_loss: 0.4603662362609638\n",
      "iteration 5996train_loss: 0.42687435080776875 test_loss: 0.4603663085162311\n",
      "iteration 5997train_loss: 0.4268743507185577 test_loss: 0.4603663807283832\n",
      "iteration 5998train_loss: 0.4268743506294533 test_loss: 0.46036645289744565\n",
      "iteration 5999train_loss: 0.4268743505404554 test_loss: 0.4603665250234441\n",
      "iteration 6000train_loss: 0.4268743504515639 test_loss: 0.4603665971064043\n",
      "iteration 6001train_loss: 0.42687435036277876 test_loss: 0.4603666691463519\n",
      "iteration 6002train_loss: 0.4268743502740997 test_loss: 0.46036674114331255\n",
      "iteration 6003train_loss: 0.4268743501855266 test_loss: 0.46036681309731187\n",
      "iteration 6004train_loss: 0.42687435009705954 test_loss: 0.46036688500837547\n",
      "iteration 6005train_loss: 0.4268743500086981 test_loss: 0.46036695687652895\n",
      "iteration 6006train_loss: 0.4268743499204424 test_loss: 0.46036702870179796\n",
      "iteration 6007train_loss: 0.4268743498322921 test_loss: 0.4603671004842079\n",
      "iteration 6008train_loss: 0.4268743497442473 test_loss: 0.4603671722237845\n",
      "iteration 6009train_loss: 0.4268743496563077 test_loss: 0.4603672439205533\n",
      "iteration 6010train_loss: 0.4268743495684732 test_loss: 0.4603673155745397\n",
      "iteration 6011train_loss: 0.42687434948074376 test_loss: 0.4603673871857693\n",
      "iteration 6012train_loss: 0.4268743493931192 test_loss: 0.46036745875426754\n",
      "iteration 6013train_loss: 0.4268743493055994 test_loss: 0.46036753028006006\n",
      "iteration 6014train_loss: 0.4268743492181843 test_loss: 0.46036760176317204\n",
      "iteration 6015train_loss: 0.4268743491308736 test_loss: 0.4603676732036292\n",
      "iteration 6016train_loss: 0.42687434904366733 test_loss: 0.4603677446014568\n",
      "iteration 6017train_loss: 0.4268743489565653 test_loss: 0.46036781595668036\n",
      "iteration 6018train_loss: 0.4268743488695673 test_loss: 0.46036788726932515\n",
      "iteration 6019train_loss: 0.42687434878267355 test_loss: 0.4603679585394168\n",
      "iteration 6020train_loss: 0.4268743486958835 test_loss: 0.46036802976698044\n",
      "iteration 6021train_loss: 0.42687434860919726 test_loss: 0.46036810095204156\n",
      "iteration 6022train_loss: 0.42687434852261463 test_loss: 0.46036817209462544\n",
      "iteration 6023train_loss: 0.42687434843613553 test_loss: 0.46036824319475744\n",
      "iteration 6024train_loss: 0.4268743483497599 test_loss: 0.46036831425246283\n",
      "iteration 6025train_loss: 0.4268743482634873 test_loss: 0.46036838526776697\n",
      "iteration 6026train_loss: 0.42687434817731806 test_loss: 0.46036845624069517\n",
      "iteration 6027train_loss: 0.42687434809125174 test_loss: 0.4603685271712727\n",
      "iteration 6028train_loss: 0.4268743480052884 test_loss: 0.4603685980595246\n",
      "iteration 6029train_loss: 0.42687434791942774 test_loss: 0.46036866890547645\n",
      "iteration 6030train_loss: 0.4268743478336696 test_loss: 0.46036873970915315\n",
      "iteration 6031train_loss: 0.4268743477480142 test_loss: 0.46036881047058004\n",
      "iteration 6032train_loss: 0.4268743476624613 test_loss: 0.46036888118978253\n",
      "iteration 6033train_loss: 0.4268743475770105 test_loss: 0.46036895186678545\n",
      "iteration 6034train_loss: 0.42687434749166187 test_loss: 0.460369022501614\n",
      "iteration 6035train_loss: 0.4268743474064153 test_loss: 0.46036909309429364\n",
      "iteration 6036train_loss: 0.42687434732127055 test_loss: 0.4603691636448492\n",
      "iteration 6037train_loss: 0.42687434723622764 test_loss: 0.4603692341533059\n",
      "iteration 6038train_loss: 0.42687434715128647 test_loss: 0.4603693046196889\n",
      "iteration 6039train_loss: 0.4268743470664467 test_loss: 0.46036937504402314\n",
      "iteration 6040train_loss: 0.4268743469817085 test_loss: 0.4603694454263338\n",
      "iteration 6041train_loss: 0.4268743468970716 test_loss: 0.4603695157666459\n",
      "iteration 6042train_loss: 0.42687434681253583 test_loss: 0.46036958606498457\n",
      "iteration 6043train_loss: 0.4268743467281011 test_loss: 0.4603696563213748\n",
      "iteration 6044train_loss: 0.4268743466437674 test_loss: 0.46036972653584146\n",
      "iteration 6045train_loss: 0.42687434655953443 test_loss: 0.4603697967084097\n",
      "iteration 6046train_loss: 0.4268743464754023 test_loss: 0.4603698668391045\n",
      "iteration 6047train_loss: 0.42687434639137056 test_loss: 0.46036993692795086\n",
      "iteration 6048train_loss: 0.42687434630743937 test_loss: 0.46037000697497343\n",
      "iteration 6049train_loss: 0.4268743462236085 test_loss: 0.4603700769801977\n",
      "iteration 6050train_loss: 0.4268743461398779 test_loss: 0.4603701469436482\n",
      "iteration 6051train_loss: 0.42687434605624736 test_loss: 0.46037021686534985\n",
      "iteration 6052train_loss: 0.42687434597271673 test_loss: 0.4603702867453276\n",
      "iteration 6053train_loss: 0.426874345889286 test_loss: 0.46037035658360653\n",
      "iteration 6054train_loss: 0.4268743458059551 test_loss: 0.46037042638021114\n",
      "iteration 6055train_loss: 0.42687434572272387 test_loss: 0.4603704961351665\n",
      "iteration 6056train_loss: 0.426874345639592 test_loss: 0.46037056584849745\n",
      "iteration 6057train_loss: 0.4268743455565594 test_loss: 0.46037063552022867\n",
      "iteration 6058train_loss: 0.42687434547362635 test_loss: 0.4603707051503853\n",
      "iteration 6059train_loss: 0.4268743453907922 test_loss: 0.46037077473899163\n",
      "iteration 6060train_loss: 0.42687434530805723 test_loss: 0.4603708442860729\n",
      "iteration 6061train_loss: 0.42687434522542117 test_loss: 0.4603709137916536\n",
      "iteration 6062train_loss: 0.42687434514288375 test_loss: 0.4603709832557586\n",
      "iteration 6063train_loss: 0.42687434506044514 test_loss: 0.46037105267841255\n",
      "iteration 6064train_loss: 0.426874344978105 test_loss: 0.4603711220596402\n",
      "iteration 6065train_loss: 0.42687434489586334 test_loss: 0.46037119139946625\n",
      "iteration 6066train_loss: 0.42687434481372005 test_loss: 0.4603712606979154\n",
      "iteration 6067train_loss: 0.42687434473167485 test_loss: 0.46037132995501234\n",
      "iteration 6068train_loss: 0.4268743446497278 test_loss: 0.4603713991707817\n",
      "iteration 6069train_loss: 0.42687434456787865 test_loss: 0.46037146834524817\n",
      "iteration 6070train_loss: 0.42687434448612743 test_loss: 0.46037153747843623\n",
      "iteration 6071train_loss: 0.4268743444044739 test_loss: 0.46037160657037074\n",
      "iteration 6072train_loss: 0.42687434432291793 test_loss: 0.46037167562107606\n",
      "iteration 6073train_loss: 0.42687434424145954 test_loss: 0.460371744630577\n",
      "iteration 6074train_loss: 0.42687434416009856 test_loss: 0.4603718135988979\n",
      "iteration 6075train_loss: 0.42687434407883484 test_loss: 0.4603718825260634\n",
      "iteration 6076train_loss: 0.4268743439976681 test_loss: 0.4603719514120982\n",
      "iteration 6077train_loss: 0.4268743439165986 test_loss: 0.4603720202570268\n",
      "iteration 6078train_loss: 0.42687434383562584 test_loss: 0.4603720890608734\n",
      "iteration 6079train_loss: 0.42687434375475 test_loss: 0.46037215782366275\n",
      "iteration 6080train_loss: 0.4268743436739708 test_loss: 0.4603722265454193\n",
      "iteration 6081train_loss: 0.4268743435932882 test_loss: 0.4603722952261675\n",
      "iteration 6082train_loss: 0.4268743435127021 test_loss: 0.46037236386593183\n",
      "iteration 6083train_loss: 0.4268743434322122 test_loss: 0.4603724324647368\n",
      "iteration 6084train_loss: 0.4268743433518185 test_loss: 0.4603725010226067\n",
      "iteration 6085train_loss: 0.4268743432715211 test_loss: 0.4603725695395659\n",
      "iteration 6086train_loss: 0.42687434319131945 test_loss: 0.460372638015639\n",
      "iteration 6087train_loss: 0.4268743431112139 test_loss: 0.4603727064508502\n",
      "iteration 6088train_loss: 0.42687434303120403 test_loss: 0.460372774845224\n",
      "iteration 6089train_loss: 0.4268743429512898 test_loss: 0.46037284319878463\n",
      "iteration 6090train_loss: 0.42687434287147114 test_loss: 0.4603729115115564\n",
      "iteration 6091train_loss: 0.4268743427917478 test_loss: 0.4603729797835638\n",
      "iteration 6092train_loss: 0.42687434271211977 test_loss: 0.46037304801483103\n",
      "iteration 6093train_loss: 0.42687434263258706 test_loss: 0.4603731162053825\n",
      "iteration 6094train_loss: 0.4268743425531493 test_loss: 0.46037318435524244\n",
      "iteration 6095train_loss: 0.4268743424738066 test_loss: 0.46037325246443495\n",
      "iteration 6096train_loss: 0.4268743423945587 test_loss: 0.4603733205329844\n",
      "iteration 6097train_loss: 0.42687434231540555 test_loss: 0.46037338856091503\n",
      "iteration 6098train_loss: 0.42687434223634707 test_loss: 0.4603734565482511\n",
      "iteration 6099train_loss: 0.426874342157383 test_loss: 0.4603735244950169\n",
      "iteration 6100train_loss: 0.4268743420785134 test_loss: 0.4603735924012364\n",
      "iteration 6101train_loss: 0.42687434199973806 test_loss: 0.4603736602669338\n",
      "iteration 6102train_loss: 0.4268743419210569 test_loss: 0.46037372809213356\n",
      "iteration 6103train_loss: 0.42687434184246975 test_loss: 0.46037379587685945\n",
      "iteration 6104train_loss: 0.42687434176397654 test_loss: 0.4603738636211358\n",
      "iteration 6105train_loss: 0.42687434168557725 test_loss: 0.4603739313249867\n",
      "iteration 6106train_loss: 0.42687434160727167 test_loss: 0.4603739989884363\n",
      "iteration 6107train_loss: 0.4268743415290597 test_loss: 0.46037406661150865\n",
      "iteration 6108train_loss: 0.4268743414509411 test_loss: 0.4603741341942279\n",
      "iteration 6109train_loss: 0.426874341372916 test_loss: 0.460374201736618\n",
      "iteration 6110train_loss: 0.42687434129498414 test_loss: 0.46037426923870306\n",
      "iteration 6111train_loss: 0.4268743412171455 test_loss: 0.4603743367005071\n",
      "iteration 6112train_loss: 0.42687434113939976 test_loss: 0.46037440412205405\n",
      "iteration 6113train_loss: 0.4268743410617471 test_loss: 0.46037447150336813\n",
      "iteration 6114train_loss: 0.4268743409841873 test_loss: 0.46037453884447327\n",
      "iteration 6115train_loss: 0.4268743409067201 test_loss: 0.4603746061453933\n",
      "iteration 6116train_loss: 0.4268743408293455 test_loss: 0.46037467340615223\n",
      "iteration 6117train_loss: 0.42687434075206343 test_loss: 0.460374740626774\n",
      "iteration 6118train_loss: 0.4268743406748738 test_loss: 0.46037480780728285\n",
      "iteration 6119train_loss: 0.4268743405977764 test_loss: 0.46037487494770224\n",
      "iteration 6120train_loss: 0.4268743405207712 test_loss: 0.46037494204805646\n",
      "iteration 6121train_loss: 0.426874340443858 test_loss: 0.46037500910836904\n",
      "iteration 6122train_loss: 0.42687434036703675 test_loss: 0.4603750761286642\n",
      "iteration 6123train_loss: 0.42687434029030735 test_loss: 0.4603751431089656\n",
      "iteration 6124train_loss: 0.42687434021366977 test_loss: 0.46037521004929716\n",
      "iteration 6125train_loss: 0.42687434013712366 test_loss: 0.4603752769496828\n",
      "iteration 6126train_loss: 0.4268743400606691 test_loss: 0.46037534381014605\n",
      "iteration 6127train_loss: 0.4268743399843059 test_loss: 0.4603754106307111\n",
      "iteration 6128train_loss: 0.426874339908034 test_loss: 0.4603754774114014\n",
      "iteration 6129train_loss: 0.4268743398318533 test_loss: 0.46037554415224086\n",
      "iteration 6130train_loss: 0.4268743397557636 test_loss: 0.46037561085325346\n",
      "iteration 6131train_loss: 0.42687433967976496 test_loss: 0.46037567751446257\n",
      "iteration 6132train_loss: 0.4268743396038572 test_loss: 0.4603757441358922\n",
      "iteration 6133train_loss: 0.42687433952804005 test_loss: 0.460375810717566\n",
      "iteration 6134train_loss: 0.42687433945231357 test_loss: 0.4603758772595076\n",
      "iteration 6135train_loss: 0.4268743393766777 test_loss: 0.4603759437617408\n",
      "iteration 6136train_loss: 0.42687433930113217 test_loss: 0.46037601022428926\n",
      "iteration 6137train_loss: 0.4268743392256769 test_loss: 0.4603760766471766\n",
      "iteration 6138train_loss: 0.42687433915031187 test_loss: 0.4603761430304267\n",
      "iteration 6139train_loss: 0.42687433907503697 test_loss: 0.4603762093740627\n",
      "iteration 6140train_loss: 0.426874338999852 test_loss: 0.4603762756781086\n",
      "iteration 6141train_loss: 0.426874338924757 test_loss: 0.46037634194258803\n",
      "iteration 6142train_loss: 0.4268743388497516 test_loss: 0.4603764081675245\n",
      "iteration 6143train_loss: 0.426874338774836 test_loss: 0.4603764743529415\n",
      "iteration 6144train_loss: 0.42687433870000996 test_loss: 0.4603765404988626\n",
      "iteration 6145train_loss: 0.4268743386252733 test_loss: 0.4603766066053115\n",
      "iteration 6146train_loss: 0.426874338550626 test_loss: 0.46037667267231175\n",
      "iteration 6147train_loss: 0.426874338476068 test_loss: 0.4603767386998866\n",
      "iteration 6148train_loss: 0.426874338401599 test_loss: 0.4603768046880599\n",
      "iteration 6149train_loss: 0.42687433832721905 test_loss: 0.460376870636855\n",
      "iteration 6150train_loss: 0.42687433825292803 test_loss: 0.46037693654629536\n",
      "iteration 6151train_loss: 0.4268743381787259 test_loss: 0.4603770024164045\n",
      "iteration 6152train_loss: 0.4268743381046123 test_loss: 0.46037706824720587\n",
      "iteration 6153train_loss: 0.4268743380305874 test_loss: 0.4603771340387229\n",
      "iteration 6154train_loss: 0.426874337956651 test_loss: 0.4603771997909789\n",
      "iteration 6155train_loss: 0.426874337882803 test_loss: 0.46037726550399743\n",
      "iteration 6156train_loss: 0.4268743378090433 test_loss: 0.4603773311778019\n",
      "iteration 6157train_loss: 0.4268743377353717 test_loss: 0.4603773968124157\n",
      "iteration 6158train_loss: 0.4268743376617881 test_loss: 0.46037746240786204\n",
      "iteration 6159train_loss: 0.4268743375882926 test_loss: 0.4603775279641645\n",
      "iteration 6160train_loss: 0.4268743375148849 test_loss: 0.4603775934813462\n",
      "iteration 6161train_loss: 0.42687433744156494 test_loss: 0.46037765895943067\n",
      "iteration 6162train_loss: 0.4268743373683325 test_loss: 0.4603777243984411\n",
      "iteration 6163train_loss: 0.42687433729518787 test_loss: 0.4603777897984009\n",
      "iteration 6164train_loss: 0.4268743372221305 test_loss: 0.4603778551593332\n",
      "iteration 6165train_loss: 0.42687433714916045 test_loss: 0.46037792048126136\n",
      "iteration 6166train_loss: 0.4268743370762777 test_loss: 0.46037798576420874\n",
      "iteration 6167train_loss: 0.426874337003482 test_loss: 0.4603780510081984\n",
      "iteration 6168train_loss: 0.4268743369307734 test_loss: 0.4603781162132537\n",
      "iteration 6169train_loss: 0.4268743368581516 test_loss: 0.4603781813793979\n",
      "iteration 6170train_loss: 0.4268743367856167 test_loss: 0.4603782465066541\n",
      "iteration 6171train_loss: 0.42687433671316855 test_loss: 0.46037831159504544\n",
      "iteration 6172train_loss: 0.42687433664080693 test_loss: 0.46037837664459524\n",
      "iteration 6173train_loss: 0.4268743365685318 test_loss: 0.46037844165532665\n",
      "iteration 6174train_loss: 0.4268743364963431 test_loss: 0.46037850662726276\n",
      "iteration 6175train_loss: 0.42687433642424066 test_loss: 0.4603785715604266\n",
      "iteration 6176train_loss: 0.4268743363522244 test_loss: 0.46037863645484156\n",
      "iteration 6177train_loss: 0.4268743362802942 test_loss: 0.46037870131053055\n",
      "iteration 6178train_loss: 0.42687433620845017 test_loss: 0.4603787661275168\n",
      "iteration 6179train_loss: 0.42687433613669185 test_loss: 0.4603788309058231\n",
      "iteration 6180train_loss: 0.42687433606501923 test_loss: 0.4603788956454729\n",
      "iteration 6181train_loss: 0.4268743359934325 test_loss: 0.46037896034648906\n",
      "iteration 6182train_loss: 0.42687433592193125 test_loss: 0.46037902500889466\n",
      "iteration 6183train_loss: 0.42687433585051543 test_loss: 0.4603790896327127\n",
      "iteration 6184train_loss: 0.426874335779185 test_loss: 0.4603791542179661\n",
      "iteration 6185train_loss: 0.4268743357079398 test_loss: 0.460379218764678\n",
      "iteration 6186train_loss: 0.4268743356367798 test_loss: 0.4603792832728714\n",
      "iteration 6187train_loss: 0.4268743355657048 test_loss: 0.46037934774256917\n",
      "iteration 6188train_loss: 0.4268743354947149 test_loss: 0.4603794121737944\n",
      "iteration 6189train_loss: 0.4268743354238097 test_loss: 0.46037947656656986\n",
      "iteration 6190train_loss: 0.42687433535298935 test_loss: 0.46037954092091865\n",
      "iteration 6191train_loss: 0.4268743352822537 test_loss: 0.46037960523686355\n",
      "iteration 6192train_loss: 0.42687433521160256 test_loss: 0.46037966951442755\n",
      "iteration 6193train_loss: 0.42687433514103584 test_loss: 0.4603797337536335\n",
      "iteration 6194train_loss: 0.4268743350705535 test_loss: 0.46037979795450423\n",
      "iteration 6195train_loss: 0.4268743350001553 test_loss: 0.4603798621170627\n",
      "iteration 6196train_loss: 0.4268743349298414 test_loss: 0.4603799262413318\n",
      "iteration 6197train_loss: 0.42687433485961146 test_loss: 0.4603799903273342\n",
      "iteration 6198train_loss: 0.4268743347894655 test_loss: 0.46038005437509283\n",
      "iteration 6199train_loss: 0.4268743347194035 test_loss: 0.4603801183846305\n",
      "iteration 6200train_loss: 0.42687433464942515 test_loss: 0.46038018235596984\n",
      "iteration 6201train_loss: 0.4268743345795304 test_loss: 0.46038024628913393\n",
      "iteration 6202train_loss: 0.4268743345097192 test_loss: 0.46038031018414527\n",
      "iteration 6203train_loss: 0.42687433443999157 test_loss: 0.46038037404102683\n",
      "iteration 6204train_loss: 0.4268743343703471 test_loss: 0.46038043785980104\n",
      "iteration 6205train_loss: 0.42687433430078603 test_loss: 0.460380501640491\n",
      "iteration 6206train_loss: 0.42687433423130805 test_loss: 0.46038056538311917\n",
      "iteration 6207train_loss: 0.42687433416191317 test_loss: 0.4603806290877083\n",
      "iteration 6208train_loss: 0.42687433409260106 test_loss: 0.46038069275428123\n",
      "iteration 6209train_loss: 0.426874334023372 test_loss: 0.46038075638286047\n",
      "iteration 6210train_loss: 0.42687433395422575 test_loss: 0.4603808199734687\n",
      "iteration 6211train_loss: 0.4268743338851619 test_loss: 0.46038088352612855\n",
      "iteration 6212train_loss: 0.42687433381618073 test_loss: 0.4603809470408627\n",
      "iteration 6213train_loss: 0.4268743337472821 test_loss: 0.4603810105176938\n",
      "iteration 6214train_loss: 0.4268743336784657 test_loss: 0.4603810739566443\n",
      "iteration 6215train_loss: 0.42687433360973165 test_loss: 0.46038113735773695\n",
      "iteration 6216train_loss: 0.4268743335410797 test_loss: 0.46038120072099437\n",
      "iteration 6217train_loss: 0.4268743334725098 test_loss: 0.4603812640464389\n",
      "iteration 6218train_loss: 0.42687433340402187 test_loss: 0.46038132733409337\n",
      "iteration 6219train_loss: 0.42687433333561586 test_loss: 0.46038139058398003\n",
      "iteration 6220train_loss: 0.4268743332672916 test_loss: 0.4603814537961216\n",
      "iteration 6221train_loss: 0.426874333199049 test_loss: 0.4603815169705406\n",
      "iteration 6222train_loss: 0.42687433313088796 test_loss: 0.4603815801072594\n",
      "iteration 6223train_loss: 0.42687433306280836 test_loss: 0.46038164320630054\n",
      "iteration 6224train_loss: 0.4268743329948102 test_loss: 0.46038170626768654\n",
      "iteration 6225train_loss: 0.42687433292689336 test_loss: 0.46038176929143976\n",
      "iteration 6226train_loss: 0.4268743328590575 test_loss: 0.4603818322775827\n",
      "iteration 6227train_loss: 0.42687433279130294 test_loss: 0.46038189522613787\n",
      "iteration 6228train_loss: 0.42687433272362924 test_loss: 0.4603819581371275\n",
      "iteration 6229train_loss: 0.42687433265603647 test_loss: 0.46038202101057424\n",
      "iteration 6230train_loss: 0.4268743325885246 test_loss: 0.46038208384650026\n",
      "iteration 6231train_loss: 0.4268743325210933 test_loss: 0.46038214664492805\n",
      "iteration 6232train_loss: 0.4268743324537426 test_loss: 0.4603822094058799\n",
      "iteration 6233train_loss: 0.4268743323864724 test_loss: 0.4603822721293782\n",
      "iteration 6234train_loss: 0.42687433231928273 test_loss: 0.4603823348154455\n",
      "iteration 6235train_loss: 0.4268743322521733 test_loss: 0.4603823974641036\n",
      "iteration 6236train_loss: 0.42687433218514403 test_loss: 0.46038246007537537\n",
      "iteration 6237train_loss: 0.426874332118195 test_loss: 0.4603825226492828\n",
      "iteration 6238train_loss: 0.4268743320513258 test_loss: 0.46038258518584824\n",
      "iteration 6239train_loss: 0.42687433198453667 test_loss: 0.46038264768509385\n",
      "iteration 6240train_loss: 0.4268743319178273 test_loss: 0.4603827101470422\n",
      "iteration 6241train_loss: 0.4268743318511977 test_loss: 0.4603827725717152\n",
      "iteration 6242train_loss: 0.42687433178464795 test_loss: 0.4603828349591353\n",
      "iteration 6243train_loss: 0.42687433171817746 test_loss: 0.4603828973093245\n",
      "iteration 6244train_loss: 0.4268743316517865 test_loss: 0.4603829596223053\n",
      "iteration 6245train_loss: 0.426874331585475 test_loss: 0.46038302189809965\n",
      "iteration 6246train_loss: 0.42687433151924264 test_loss: 0.4603830841367298\n",
      "iteration 6247train_loss: 0.4268743314530895 test_loss: 0.46038314633821803\n",
      "iteration 6248train_loss: 0.42687433138701547 test_loss: 0.4603832085025862\n",
      "iteration 6249train_loss: 0.4268743313210203 test_loss: 0.4603832706298567\n",
      "iteration 6250train_loss: 0.42687433125510404 test_loss: 0.46038333272005166\n",
      "iteration 6251train_loss: 0.4268743311892666 test_loss: 0.46038339477319307\n",
      "iteration 6252train_loss: 0.426874331123508 test_loss: 0.46038345678930315\n",
      "iteration 6253train_loss: 0.42687433105782785 test_loss: 0.46038351876840383\n",
      "iteration 6254train_loss: 0.4268743309922262 test_loss: 0.4603835807105173\n",
      "iteration 6255train_loss: 0.4268743309267029 test_loss: 0.4603836426156656\n",
      "iteration 6256train_loss: 0.4268743308612581 test_loss: 0.4603837044838707\n",
      "iteration 6257train_loss: 0.4268743307958913 test_loss: 0.4603837663151548\n",
      "iteration 6258train_loss: 0.4268743307306028 test_loss: 0.4603838281095398\n",
      "iteration 6259train_loss: 0.4268743306653924 test_loss: 0.46038388986704776\n",
      "iteration 6260train_loss: 0.42687433060025987 test_loss: 0.46038395158770057\n",
      "iteration 6261train_loss: 0.4268743305352051 test_loss: 0.4603840132715203\n",
      "iteration 6262train_loss: 0.4268743304702282 test_loss: 0.4603840749185289\n",
      "iteration 6263train_loss: 0.4268743304053289 test_loss: 0.46038413652874827\n",
      "iteration 6264train_loss: 0.42687433034050715 test_loss: 0.4603841981022005\n",
      "iteration 6265train_loss: 0.42687433027576294 test_loss: 0.4603842596389074\n",
      "iteration 6266train_loss: 0.4268743302110961 test_loss: 0.4603843211388909\n",
      "iteration 6267train_loss: 0.42687433014650655 test_loss: 0.4603843826021729\n",
      "iteration 6268train_loss: 0.4268743300819942 test_loss: 0.46038444402877543\n",
      "iteration 6269train_loss: 0.42687433001755903 test_loss: 0.46038450541872006\n",
      "iteration 6270train_loss: 0.4268743299532007 test_loss: 0.46038456677202894\n",
      "iteration 6271train_loss: 0.42687432988891955 test_loss: 0.46038462808872377\n",
      "iteration 6272train_loss: 0.42687432982471507 test_loss: 0.46038468936882637\n",
      "iteration 6273train_loss: 0.42687432976058737 test_loss: 0.46038475061235873\n",
      "iteration 6274train_loss: 0.42687432969653627 test_loss: 0.46038481181934254\n",
      "iteration 6275train_loss: 0.4268743296325618 test_loss: 0.46038487298979963\n",
      "iteration 6276train_loss: 0.4268743295686637 test_loss: 0.46038493412375175\n",
      "iteration 6277train_loss: 0.4268743295048421 test_loss: 0.46038499522122067\n",
      "iteration 6278train_loss: 0.4268743294410967 test_loss: 0.46038505628222826\n",
      "iteration 6279train_loss: 0.42687432937742753 test_loss: 0.46038511730679615\n",
      "iteration 6280train_loss: 0.4268743293138344 test_loss: 0.46038517829494596\n",
      "iteration 6281train_loss: 0.4268743292503173 test_loss: 0.46038523924669966\n",
      "iteration 6282train_loss: 0.42687432918687623 test_loss: 0.4603853001620789\n",
      "iteration 6283train_loss: 0.426874329123511 test_loss: 0.46038536104110533\n",
      "iteration 6284train_loss: 0.4268743290602213 test_loss: 0.46038542188380055\n",
      "iteration 6285train_loss: 0.42687432899700734 test_loss: 0.46038548269018637\n",
      "iteration 6286train_loss: 0.426874328933869 test_loss: 0.4603855434602843\n",
      "iteration 6287train_loss: 0.4268743288708061 test_loss: 0.4603856041941162\n",
      "iteration 6288train_loss: 0.4268743288078186 test_loss: 0.46038566489170346\n",
      "iteration 6289train_loss: 0.4268743287449063 test_loss: 0.4603857255530678\n",
      "iteration 6290train_loss: 0.42687432868206926 test_loss: 0.4603857861782308\n",
      "iteration 6291train_loss: 0.4268743286193074 test_loss: 0.46038584676721417\n",
      "iteration 6292train_loss: 0.42687432855662033 test_loss: 0.46038590732003937\n",
      "iteration 6293train_loss: 0.42687432849400836 test_loss: 0.46038596783672814\n",
      "iteration 6294train_loss: 0.42687432843147116 test_loss: 0.46038602831730163\n",
      "iteration 6295train_loss: 0.42687432836900874 test_loss: 0.4603860887617817\n",
      "iteration 6296train_loss: 0.426874328306621 test_loss: 0.46038614917019\n",
      "iteration 6297train_loss: 0.42687432824430777 test_loss: 0.46038620954254766\n",
      "iteration 6298train_loss: 0.4268743281820691 test_loss: 0.4603862698788765\n",
      "iteration 6299train_loss: 0.42687432811990483 test_loss: 0.4603863301791979\n",
      "iteration 6300train_loss: 0.4268743280578148 test_loss: 0.4603863904435332\n",
      "iteration 6301train_loss: 0.42687432799579894 test_loss: 0.46038645067190404\n",
      "iteration 6302train_loss: 0.4268743279338573 test_loss: 0.46038651086433185\n",
      "iteration 6303train_loss: 0.4268743278719896 test_loss: 0.4603865710208381\n",
      "iteration 6304train_loss: 0.42687432781019596 test_loss: 0.4603866311414441\n",
      "iteration 6305train_loss: 0.42687432774847606 test_loss: 0.46038669122617126\n",
      "iteration 6306train_loss: 0.42687432768683004 test_loss: 0.46038675127504114\n",
      "iteration 6307train_loss: 0.4268743276252577 test_loss: 0.460386811288075\n",
      "iteration 6308train_loss: 0.42687432756375887 test_loss: 0.4603868712652942\n",
      "iteration 6309train_loss: 0.42687432750233356 test_loss: 0.46038693120672014\n",
      "iteration 6310train_loss: 0.42687432744098164 test_loss: 0.46038699111237413\n",
      "iteration 6311train_loss: 0.42687432737970316 test_loss: 0.46038705098227756\n",
      "iteration 6312train_loss: 0.42687432731849784 test_loss: 0.4603871108164518\n",
      "iteration 6313train_loss: 0.42687432725736574 test_loss: 0.4603871706149181\n",
      "iteration 6314train_loss: 0.4268743271963067 test_loss: 0.4603872303776977\n",
      "iteration 6315train_loss: 0.42687432713532053 test_loss: 0.46038729010481194\n",
      "iteration 6316train_loss: 0.42687432707440726 test_loss: 0.4603873497962821\n",
      "iteration 6317train_loss: 0.4268743270135669 test_loss: 0.4603874094521294\n",
      "iteration 6318train_loss: 0.4268743269527992 test_loss: 0.46038746907237527\n",
      "iteration 6319train_loss: 0.42687432689210414 test_loss: 0.46038752865704063\n",
      "iteration 6320train_loss: 0.4268743268314817 test_loss: 0.46038758820614695\n",
      "iteration 6321train_loss: 0.4268743267709317 test_loss: 0.4603876477197153\n",
      "iteration 6322train_loss: 0.42687432671045394 test_loss: 0.46038770719776695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6323train_loss: 0.4268743266500486 test_loss: 0.46038776664032305\n",
      "iteration 6324train_loss: 0.4268743265897154 test_loss: 0.4603878260474048\n",
      "iteration 6325train_loss: 0.42687432652945423 test_loss: 0.4603878854190333\n",
      "iteration 6326train_loss: 0.42687432646926526 test_loss: 0.4603879447552298\n",
      "iteration 6327train_loss: 0.42687432640914813 test_loss: 0.4603880040560154\n",
      "iteration 6328train_loss: 0.4268743263491029 test_loss: 0.4603880633214111\n",
      "iteration 6329train_loss: 0.4268743262891294 test_loss: 0.46038812255143824\n",
      "iteration 6330train_loss: 0.42687432622922755 test_loss: 0.4603881817461177\n",
      "iteration 6331train_loss: 0.42687432616939736 test_loss: 0.46038824090547054\n",
      "iteration 6332train_loss: 0.4268743261096387 test_loss: 0.4603883000295181\n",
      "iteration 6333train_loss: 0.42687432604995135 test_loss: 0.46038835911828124\n",
      "iteration 6334train_loss: 0.42687432599033537 test_loss: 0.46038841817178094\n",
      "iteration 6335train_loss: 0.4268743259307908 test_loss: 0.46038847719003845\n",
      "iteration 6336train_loss: 0.42687432587131724 test_loss: 0.4603885361730745\n",
      "iteration 6337train_loss: 0.42687432581191476 test_loss: 0.46038859512091035\n",
      "iteration 6338train_loss: 0.4268743257525834 test_loss: 0.460388654033567\n",
      "iteration 6339train_loss: 0.4268743256933229 test_loss: 0.46038871291106526\n",
      "iteration 6340train_loss: 0.4268743256341332 test_loss: 0.46038877175342624\n",
      "iteration 6341train_loss: 0.4268743255750142 test_loss: 0.46038883056067076\n",
      "iteration 6342train_loss: 0.42687432551596594 test_loss: 0.46038888933281996\n",
      "iteration 6343train_loss: 0.4268743254569881 test_loss: 0.4603889480698945\n",
      "iteration 6344train_loss: 0.426874325398081 test_loss: 0.4603890067719156\n",
      "iteration 6345train_loss: 0.42687432533924413 test_loss: 0.46038906543890395\n",
      "iteration 6346train_loss: 0.4268743252804776 test_loss: 0.4603891240708807\n",
      "iteration 6347train_loss: 0.42687432522178137 test_loss: 0.4603891826678664\n",
      "iteration 6348train_loss: 0.42687432516315527 test_loss: 0.4603892412298822\n",
      "iteration 6349train_loss: 0.4268743251045992 test_loss: 0.4603892997569487\n",
      "iteration 6350train_loss: 0.4268743250461132 test_loss: 0.46038935824908683\n",
      "iteration 6351train_loss: 0.42687432498769706 test_loss: 0.4603894167063176\n",
      "iteration 6352train_loss: 0.4268743249293507 test_loss: 0.4603894751286617\n",
      "iteration 6353train_loss: 0.42687432487107413 test_loss: 0.46038953351614\n",
      "iteration 6354train_loss: 0.42687432481286725 test_loss: 0.46038959186877315\n",
      "iteration 6355train_loss: 0.4268743247547298 test_loss: 0.46038965018658207\n",
      "iteration 6356train_loss: 0.4268743246966619 test_loss: 0.46038970846958743\n",
      "iteration 6357train_loss: 0.4268743246386635 test_loss: 0.46038976671781\n",
      "iteration 6358train_loss: 0.4268743245807343 test_loss: 0.46038982493127073\n",
      "iteration 6359train_loss: 0.4268743245228744 test_loss: 0.46038988310999013\n",
      "iteration 6360train_loss: 0.4268743244650836 test_loss: 0.46038994125398897\n",
      "iteration 6361train_loss: 0.4268743244073619 test_loss: 0.46038999936328806\n",
      "iteration 6362train_loss: 0.42687432434970923 test_loss: 0.46039005743790795\n",
      "iteration 6363train_loss: 0.4268743242921254 test_loss: 0.46039011547786945\n",
      "iteration 6364train_loss: 0.4268743242346104 test_loss: 0.46039017348319305\n",
      "iteration 6365train_loss: 0.4268743241771641 test_loss: 0.4603902314538996\n",
      "iteration 6366train_loss: 0.42687432411978654 test_loss: 0.46039028939000975\n",
      "iteration 6367train_loss: 0.4268743240624776 test_loss: 0.46039034729154393\n",
      "iteration 6368train_loss: 0.4268743240052371 test_loss: 0.4603904051585231\n",
      "iteration 6369train_loss: 0.42687432394806496 test_loss: 0.4603904629909675\n",
      "iteration 6370train_loss: 0.4268743238909612 test_loss: 0.4603905207888979\n",
      "iteration 6371train_loss: 0.42687432383392576 test_loss: 0.46039057855233495\n",
      "iteration 6372train_loss: 0.4268743237769584 test_loss: 0.4603906362812992\n",
      "iteration 6373train_loss: 0.42687432372005907 test_loss: 0.46039069397581106\n",
      "iteration 6374train_loss: 0.42687432366322786 test_loss: 0.4603907516358914\n",
      "iteration 6375train_loss: 0.42687432360646455 test_loss: 0.46039080926156045\n",
      "iteration 6376train_loss: 0.4268743235497691 test_loss: 0.4603908668528387\n",
      "iteration 6377train_loss: 0.4268743234931413 test_loss: 0.4603909244097469\n",
      "iteration 6378train_loss: 0.42687432343658127 test_loss: 0.46039098193230543\n",
      "iteration 6379train_loss: 0.4268743233800889 test_loss: 0.4603910394205349\n",
      "iteration 6380train_loss: 0.4268743233236639 test_loss: 0.4603910968744556\n",
      "iteration 6381train_loss: 0.42687432326730645 test_loss: 0.4603911542940881\n",
      "iteration 6382train_loss: 0.4268743232110163 test_loss: 0.46039121167945285\n",
      "iteration 6383train_loss: 0.4268743231547935 test_loss: 0.4603912690305702\n",
      "iteration 6384train_loss: 0.4268743230986379 test_loss: 0.46039132634746077\n",
      "iteration 6385train_loss: 0.4268743230425493 test_loss: 0.46039138363014476\n",
      "iteration 6386train_loss: 0.4268743229865278 test_loss: 0.46039144087864264\n",
      "iteration 6387train_loss: 0.4268743229305732 test_loss: 0.46039149809297497\n",
      "iteration 6388train_loss: 0.4268743228746856 test_loss: 0.4603915552731619\n",
      "iteration 6389train_loss: 0.42687432281886467 test_loss: 0.4603916124192239\n",
      "iteration 6390train_loss: 0.4268743227631105 test_loss: 0.46039166953118127\n",
      "iteration 6391train_loss: 0.4268743227074229 test_loss: 0.4603917266090544\n",
      "iteration 6392train_loss: 0.426874322651802 test_loss: 0.4603917836528637\n",
      "iteration 6393train_loss: 0.4268743225962475 test_loss: 0.4603918406626294\n",
      "iteration 6394train_loss: 0.4268743225407594 test_loss: 0.46039189763837174\n",
      "iteration 6395train_loss: 0.42687432248533763 test_loss: 0.46039195458011106\n",
      "iteration 6396train_loss: 0.4268743224299821 test_loss: 0.46039201148786774\n",
      "iteration 6397train_loss: 0.4268743223746927 test_loss: 0.46039206836166197\n",
      "iteration 6398train_loss: 0.4268743223194693 test_loss: 0.46039212520151396\n",
      "iteration 6399train_loss: 0.42687432226431204 test_loss: 0.4603921820074441\n",
      "iteration 6400train_loss: 0.42687432220922067 test_loss: 0.46039223877947244\n",
      "iteration 6401train_loss: 0.42687432215419513 test_loss: 0.4603922955176193\n",
      "iteration 6402train_loss: 0.42687432209923537 test_loss: 0.46039235222190494\n",
      "iteration 6403train_loss: 0.4268743220443413 test_loss: 0.46039240889234945\n",
      "iteration 6404train_loss: 0.4268743219895128 test_loss: 0.4603924655289731\n",
      "iteration 6405train_loss: 0.4268743219347498 test_loss: 0.4603925221317961\n",
      "iteration 6406train_loss: 0.42687432188005237 test_loss: 0.4603925787008385\n",
      "iteration 6407train_loss: 0.4268743218254202 test_loss: 0.4603926352361204\n",
      "iteration 6408train_loss: 0.4268743217708533 test_loss: 0.4603926917376622\n",
      "iteration 6409train_loss: 0.4268743217163517 test_loss: 0.4603927482054837\n",
      "iteration 6410train_loss: 0.42687432166191525 test_loss: 0.4603928046396052\n",
      "iteration 6411train_loss: 0.42687432160754385 test_loss: 0.46039286104004684\n",
      "iteration 6412train_loss: 0.4268743215532374 test_loss: 0.46039291740682853\n",
      "iteration 6413train_loss: 0.4268743214989958 test_loss: 0.46039297373997046\n",
      "iteration 6414train_loss: 0.4268743214448192 test_loss: 0.46039303003949283\n",
      "iteration 6415train_loss: 0.4268743213907072 test_loss: 0.46039308630541553\n",
      "iteration 6416train_loss: 0.4268743213366599 test_loss: 0.4603931425377586\n",
      "iteration 6417train_loss: 0.4268743212826772 test_loss: 0.4603931987365421\n",
      "iteration 6418train_loss: 0.426874321228759 test_loss: 0.460393254901786\n",
      "iteration 6419train_loss: 0.4268743211749053 test_loss: 0.4603933110335105\n",
      "iteration 6420train_loss: 0.4268743211211159 test_loss: 0.4603933671317353\n",
      "iteration 6421train_loss: 0.42687432106739087 test_loss: 0.4603934231964806\n",
      "iteration 6422train_loss: 0.42687432101373 test_loss: 0.4603934792277663\n",
      "iteration 6423train_loss: 0.42687432096013317 test_loss: 0.4603935352256124\n",
      "iteration 6424train_loss: 0.42687432090660055 test_loss: 0.4603935911900387\n",
      "iteration 6425train_loss: 0.42687432085313176 test_loss: 0.4603936471210653\n",
      "iteration 6426train_loss: 0.42687432079972704 test_loss: 0.4603937030187121\n",
      "iteration 6427train_loss: 0.42687432074638604 test_loss: 0.46039375888299894\n",
      "iteration 6428train_loss: 0.42687432069310877 test_loss: 0.4603938147139457\n",
      "iteration 6429train_loss: 0.42687432063989533 test_loss: 0.4603938705115723\n",
      "iteration 6430train_loss: 0.42687432058674524 test_loss: 0.46039392627589887\n",
      "iteration 6431train_loss: 0.42687432053365887 test_loss: 0.46039398200694476\n",
      "iteration 6432train_loss: 0.42687432048063584 test_loss: 0.46039403770473025\n",
      "iteration 6433train_loss: 0.4268743204276762 test_loss: 0.46039409336927495\n",
      "iteration 6434train_loss: 0.4268743203747799 test_loss: 0.4603941490005989\n",
      "iteration 6435train_loss: 0.4268743203219468 test_loss: 0.46039420459872166\n",
      "iteration 6436train_loss: 0.4268743202691769 test_loss: 0.4603942601636632\n",
      "iteration 6437train_loss: 0.42687432021646987 test_loss: 0.4603943156954433\n",
      "iteration 6438train_loss: 0.42687432016382604 test_loss: 0.4603943711940816\n",
      "iteration 6439train_loss: 0.42687432011124504 test_loss: 0.4603944266595981\n",
      "iteration 6440train_loss: 0.42687432005872683 test_loss: 0.46039448209201245\n",
      "iteration 6441train_loss: 0.42687432000627146 test_loss: 0.46039453749134435\n",
      "iteration 6442train_loss: 0.4268743199538788 test_loss: 0.46039459285761364\n",
      "iteration 6443train_loss: 0.42687431990154867 test_loss: 0.46039464819083986\n",
      "iteration 6444train_loss: 0.4268743198492811 test_loss: 0.4603947034910429\n",
      "iteration 6445train_loss: 0.42687431979707613 test_loss: 0.4603947587582424\n",
      "iteration 6446train_loss: 0.42687431974493334 test_loss: 0.46039481399245796\n",
      "iteration 6447train_loss: 0.4268743196928531 test_loss: 0.46039486919370937\n",
      "iteration 6448train_loss: 0.4268743196408349 test_loss: 0.4603949243620163\n",
      "iteration 6449train_loss: 0.426874319588879 test_loss: 0.4603949794973983\n",
      "iteration 6450train_loss: 0.42687431953698507 test_loss: 0.4603950345998751\n",
      "iteration 6451train_loss: 0.4268743194851533 test_loss: 0.46039508966946624\n",
      "iteration 6452train_loss: 0.42687431943338333 test_loss: 0.46039514470619136\n",
      "iteration 6453train_loss: 0.42687431938167536 test_loss: 0.4603951997100702\n",
      "iteration 6454train_loss: 0.4268743193300291 test_loss: 0.46039525468112225\n",
      "iteration 6455train_loss: 0.4268743192784446 test_loss: 0.46039530961936703\n",
      "iteration 6456train_loss: 0.4268743192269218 test_loss: 0.46039536452482416\n",
      "iteration 6457train_loss: 0.4268743191754605 test_loss: 0.46039541939751333\n",
      "iteration 6458train_loss: 0.4268743191240607 test_loss: 0.46039547423745386\n",
      "iteration 6459train_loss: 0.42687431907272233 test_loss: 0.46039552904466535\n",
      "iteration 6460train_loss: 0.4268743190214454 test_loss: 0.46039558381916745\n",
      "iteration 6461train_loss: 0.42687431897022976 test_loss: 0.4603956385609795\n",
      "iteration 6462train_loss: 0.4268743189190751 test_loss: 0.46039569327012114\n",
      "iteration 6463train_loss: 0.4268743188679818 test_loss: 0.46039574794661176\n",
      "iteration 6464train_loss: 0.4268743188169495 test_loss: 0.4603958025904709\n",
      "iteration 6465train_loss: 0.4268743187659782 test_loss: 0.4603958572017181\n",
      "iteration 6466train_loss: 0.4268743187150678 test_loss: 0.4603959117803726\n",
      "iteration 6467train_loss: 0.42687431866421827 test_loss: 0.460395966326454\n",
      "iteration 6468train_loss: 0.42687431861342945 test_loss: 0.4603960208399816\n",
      "iteration 6469train_loss: 0.4268743185627013 test_loss: 0.4603960753209751\n",
      "iteration 6470train_loss: 0.42687431851203383 test_loss: 0.4603961297694536\n",
      "iteration 6471train_loss: 0.4268743184614269 test_loss: 0.4603961841854366\n",
      "iteration 6472train_loss: 0.4268743184108805 test_loss: 0.46039623856894346\n",
      "iteration 6473train_loss: 0.4268743183603946 test_loss: 0.4603962929199937\n",
      "iteration 6474train_loss: 0.42687431830996886 test_loss: 0.4603963472386065\n",
      "iteration 6475train_loss: 0.4268743182596034 test_loss: 0.46039640152480127\n",
      "iteration 6476train_loss: 0.42687431820929816 test_loss: 0.4603964557785974\n",
      "iteration 6477train_loss: 0.4268743181590531 test_loss: 0.4603965100000141\n",
      "iteration 6478train_loss: 0.4268743181088681 test_loss: 0.4603965641890708\n",
      "iteration 6479train_loss: 0.42687431805874304 test_loss: 0.4603966183457868\n",
      "iteration 6480train_loss: 0.42687431800867776 test_loss: 0.46039667247018123\n",
      "iteration 6481train_loss: 0.42687431795867237 test_loss: 0.46039672656227365\n",
      "iteration 6482train_loss: 0.42687431790872676 test_loss: 0.46039678062208306\n",
      "iteration 6483train_loss: 0.426874317858841 test_loss: 0.4603968346496289\n",
      "iteration 6484train_loss: 0.4268743178090147 test_loss: 0.4603968886449303\n",
      "iteration 6485train_loss: 0.42687431775924795 test_loss: 0.46039694260800645\n",
      "iteration 6486train_loss: 0.4268743177095407 test_loss: 0.46039699653887683\n",
      "iteration 6487train_loss: 0.42687431765989287 test_loss: 0.4603970504375604\n",
      "iteration 6488train_loss: 0.4268743176103043 test_loss: 0.46039710430407643\n",
      "iteration 6489train_loss: 0.426874317560775 test_loss: 0.4603971581384442\n",
      "iteration 6490train_loss: 0.426874317511305 test_loss: 0.4603972119406827\n",
      "iteration 6491train_loss: 0.4268743174618941 test_loss: 0.4603972657108113\n",
      "iteration 6492train_loss: 0.4268743174125421 test_loss: 0.46039731944884904\n",
      "iteration 6493train_loss: 0.4268743173632493 test_loss: 0.46039737315481505\n",
      "iteration 6494train_loss: 0.42687431731401526 test_loss: 0.4603974268287285\n",
      "iteration 6495train_loss: 0.42687431726484004 test_loss: 0.4603974804706084\n",
      "iteration 6496train_loss: 0.42687431721572366 test_loss: 0.46039753408047407\n",
      "iteration 6497train_loss: 0.42687431716666596 test_loss: 0.46039758765834454\n",
      "iteration 6498train_loss: 0.42687431711766693 test_loss: 0.46039764120423887\n",
      "iteration 6499train_loss: 0.42687431706872647 test_loss: 0.46039769471817604\n",
      "iteration 6500train_loss: 0.42687431701984446 test_loss: 0.4603977482001752\n",
      "iteration 6501train_loss: 0.4268743169710208 test_loss: 0.4603978016502554\n",
      "iteration 6502train_loss: 0.4268743169222555 test_loss: 0.46039785506843567\n",
      "iteration 6503train_loss: 0.42687431687354854 test_loss: 0.46039790845473516\n",
      "iteration 6504train_loss: 0.4268743168248998 test_loss: 0.46039796180917264\n",
      "iteration 6505train_loss: 0.42687431677630916 test_loss: 0.4603980151317673\n",
      "iteration 6506train_loss: 0.4268743167277766 test_loss: 0.460398068422538\n",
      "iteration 6507train_loss: 0.426874316679302 test_loss: 0.46039812168150396\n",
      "iteration 6508train_loss: 0.42687431663088543 test_loss: 0.46039817490868384\n",
      "iteration 6509train_loss: 0.42687431658252656 test_loss: 0.46039822810409686\n",
      "iteration 6510train_loss: 0.42687431653422564 test_loss: 0.4603982812677618\n",
      "iteration 6511train_loss: 0.42687431648598245 test_loss: 0.46039833439969774\n",
      "iteration 6512train_loss: 0.42687431643779683 test_loss: 0.46039838749992346\n",
      "iteration 6513train_loss: 0.42687431638966883 test_loss: 0.460398440568458\n",
      "iteration 6514train_loss: 0.4268743163415983 test_loss: 0.46039849360532026\n",
      "iteration 6515train_loss: 0.42687431629358535 test_loss: 0.4603985466105291\n",
      "iteration 6516train_loss: 0.4268743162456296 test_loss: 0.4603985995841034\n",
      "iteration 6517train_loss: 0.4268743161977312 test_loss: 0.46039865252606194\n",
      "iteration 6518train_loss: 0.42687431614989013 test_loss: 0.46039870543642375\n",
      "iteration 6519train_loss: 0.42687431610210624 test_loss: 0.46039875831520755\n",
      "iteration 6520train_loss: 0.4268743160543793 test_loss: 0.4603988111624322\n",
      "iteration 6521train_loss: 0.4268743160067095 test_loss: 0.4603988639781165\n",
      "iteration 6522train_loss: 0.4268743159590967 test_loss: 0.4603989167622794\n",
      "iteration 6523train_loss: 0.42687431591154074 test_loss: 0.46039896951493964\n",
      "iteration 6524train_loss: 0.42687431586404173 test_loss: 0.46039902223611584\n",
      "iteration 6525train_loss: 0.42687431581659935 test_loss: 0.46039907492582716\n",
      "iteration 6526train_loss: 0.4268743157692137 test_loss: 0.46039912758409196\n",
      "iteration 6527train_loss: 0.4268743157218847 test_loss: 0.4603991802109293\n",
      "iteration 6528train_loss: 0.4268743156746122 test_loss: 0.4603992328063577\n",
      "iteration 6529train_loss: 0.42687431562739636 test_loss: 0.46039928537039604\n",
      "iteration 6530train_loss: 0.42687431558023675 test_loss: 0.46039933790306303\n",
      "iteration 6531train_loss: 0.42687431553313354 test_loss: 0.4603993904043774\n",
      "iteration 6532train_loss: 0.42687431548608673 test_loss: 0.4603994428743577\n",
      "iteration 6533train_loss: 0.42687431543909604 test_loss: 0.4603994953130228\n",
      "iteration 6534train_loss: 0.42687431539216153 test_loss: 0.4603995477203913\n",
      "iteration 6535train_loss: 0.42687431534528314 test_loss: 0.46039960009648195\n",
      "iteration 6536train_loss: 0.42687431529846076 test_loss: 0.4603996524413133\n",
      "iteration 6537train_loss: 0.42687431525169434 test_loss: 0.460399704754904\n",
      "iteration 6538train_loss: 0.4268743152049837 test_loss: 0.46039975703727287\n",
      "iteration 6539train_loss: 0.426874315158329 test_loss: 0.4603998092884382\n",
      "iteration 6540train_loss: 0.42687431511173 test_loss: 0.4603998615084189\n",
      "iteration 6541train_loss: 0.42687431506518675 test_loss: 0.4603999136972335\n",
      "iteration 6542train_loss: 0.42687431501869916 test_loss: 0.46039996585490045\n",
      "iteration 6543train_loss: 0.426874314972267 test_loss: 0.4604000179814386\n",
      "iteration 6544train_loss: 0.42687431492589045 test_loss: 0.46040007007686634\n",
      "iteration 6545train_loss: 0.42687431487956917 test_loss: 0.4604001221412022\n",
      "iteration 6546train_loss: 0.4268743148333034 test_loss: 0.46040017417446477\n",
      "iteration 6547train_loss: 0.42687431478709287 test_loss: 0.4604002261766726\n",
      "iteration 6548train_loss: 0.4268743147409375 test_loss: 0.46040027814784423\n",
      "iteration 6549train_loss: 0.4268743146948373 test_loss: 0.46040033008799813\n",
      "iteration 6550train_loss: 0.4268743146487923 test_loss: 0.4604003819971529\n",
      "iteration 6551train_loss: 0.42687431460280223 test_loss: 0.46040043387532686\n",
      "iteration 6552train_loss: 0.4268743145568671 test_loss: 0.4604004857225387\n",
      "iteration 6553train_loss: 0.4268743145109869 test_loss: 0.4604005375388067\n",
      "iteration 6554train_loss: 0.4268743144651616 test_loss: 0.4604005893241494\n",
      "iteration 6555train_loss: 0.42687431441939105 test_loss: 0.46040064107858536\n",
      "iteration 6556train_loss: 0.42687431437367507 test_loss: 0.46040069280213286\n",
      "iteration 6557train_loss: 0.42687431432801376 test_loss: 0.4604007444948104\n",
      "iteration 6558train_loss: 0.4268743142824071 test_loss: 0.4604007961566364\n",
      "iteration 6559train_loss: 0.4268743142368549 test_loss: 0.46040084778762913\n",
      "iteration 6560train_loss: 0.4268743141913572 test_loss: 0.46040089938780715\n",
      "iteration 6561train_loss: 0.4268743141459137 test_loss: 0.46040095095718875\n",
      "iteration 6562train_loss: 0.4268743141005247 test_loss: 0.46040100249579236\n",
      "iteration 6563train_loss: 0.4268743140551897 test_loss: 0.4604010540036363\n",
      "iteration 6564train_loss: 0.42687431400990916 test_loss: 0.460401105480739\n",
      "iteration 6565train_loss: 0.42687431396468256 test_loss: 0.4604011569271187\n",
      "iteration 6566train_loss: 0.42687431391951003 test_loss: 0.4604012083427937\n",
      "iteration 6567train_loss: 0.4268743138743915 test_loss: 0.4604012597277824\n",
      "iteration 6568train_loss: 0.42687431382932683 test_loss: 0.4604013110821032\n",
      "iteration 6569train_loss: 0.4268743137843161 test_loss: 0.46040136240577406\n",
      "iteration 6570train_loss: 0.4268743137393591 test_loss: 0.46040141369881366\n",
      "iteration 6571train_loss: 0.4268743136944559 test_loss: 0.46040146496124007\n",
      "iteration 6572train_loss: 0.4268743136496064 test_loss: 0.4604015161930715\n",
      "iteration 6573train_loss: 0.4268743136048104 test_loss: 0.4604015673943263\n",
      "iteration 6574train_loss: 0.42687431356006794 test_loss: 0.4604016185650228\n",
      "iteration 6575train_loss: 0.42687431351537886 test_loss: 0.4604016697051791\n",
      "iteration 6576train_loss: 0.4268743134707433 test_loss: 0.4604017208148134\n",
      "iteration 6577train_loss: 0.42687431342616106 test_loss: 0.46040177189394393\n",
      "iteration 6578train_loss: 0.4268743133816322 test_loss: 0.46040182294258897\n",
      "iteration 6579train_loss: 0.4268743133371564 test_loss: 0.4604018739607666\n",
      "iteration 6580train_loss: 0.4268743132927338 test_loss: 0.46040192494849497\n",
      "iteration 6581train_loss: 0.4268743132483643 test_loss: 0.46040197590579224\n",
      "iteration 6582train_loss: 0.42687431320404773 test_loss: 0.4604020268326769\n",
      "iteration 6583train_loss: 0.42687431315978425 test_loss: 0.4604020777291667\n",
      "iteration 6584train_loss: 0.42687431311557356 test_loss: 0.46040212859527985\n",
      "iteration 6585train_loss: 0.4268743130714158 test_loss: 0.4604021794310345\n",
      "iteration 6586train_loss: 0.4268743130273107 test_loss: 0.4604022302364489\n",
      "iteration 6587train_loss: 0.4268743129832584 test_loss: 0.460402281011541\n",
      "iteration 6588train_loss: 0.4268743129392587 test_loss: 0.4604023317563289\n",
      "iteration 6589train_loss: 0.4268743128953116 test_loss: 0.4604023824708307\n",
      "iteration 6590train_loss: 0.42687431285141697 test_loss: 0.4604024331550645\n",
      "iteration 6591train_loss: 0.4268743128075748 test_loss: 0.46040248380904836\n",
      "iteration 6592train_loss: 0.4268743127637852 test_loss: 0.46040253443280027\n",
      "iteration 6593train_loss: 0.42687431272004767 test_loss: 0.4604025850263382\n",
      "iteration 6594train_loss: 0.42687431267636256 test_loss: 0.4604026355896803\n",
      "iteration 6595train_loss: 0.42687431263272957 test_loss: 0.4604026861228446\n",
      "iteration 6596train_loss: 0.4268743125891489 test_loss: 0.4604027366258489\n",
      "iteration 6597train_loss: 0.42687431254562014 test_loss: 0.46040278709871146\n",
      "iteration 6598train_loss: 0.42687431250214336 test_loss: 0.4604028375414501\n",
      "iteration 6599train_loss: 0.42687431245871854 test_loss: 0.46040288795408296\n",
      "iteration 6600train_loss: 0.42687431241534574 test_loss: 0.4604029383366277\n",
      "iteration 6601train_loss: 0.42687431237202467 test_loss: 0.4604029886891025\n",
      "iteration 6602train_loss: 0.42687431232875545 test_loss: 0.46040303901152524\n",
      "iteration 6603train_loss: 0.4268743122855379 test_loss: 0.4604030893039139\n",
      "iteration 6604train_loss: 0.426874312242372 test_loss: 0.4604031395662863\n",
      "iteration 6605train_loss: 0.42687431219925764 test_loss: 0.46040318979866035\n",
      "iteration 6606train_loss: 0.42687431215619487 test_loss: 0.460403240001054\n",
      "iteration 6607train_loss: 0.42687431211318355 test_loss: 0.4604032901734852\n",
      "iteration 6608train_loss: 0.42687431207022364 test_loss: 0.4604033403159717\n",
      "iteration 6609train_loss: 0.4268743120273151 test_loss: 0.4604033904285314\n",
      "iteration 6610train_loss: 0.4268743119844577 test_loss: 0.46040344051118215\n",
      "iteration 6611train_loss: 0.4268743119416516 test_loss: 0.46040349056394186\n",
      "iteration 6612train_loss: 0.4268743118988966 test_loss: 0.46040354058682825\n",
      "iteration 6613train_loss: 0.4268743118561927 test_loss: 0.4604035905798593\n",
      "iteration 6614train_loss: 0.4268743118135399 test_loss: 0.4604036405430527\n",
      "iteration 6615train_loss: 0.42687431177093804 test_loss: 0.46040369047642626\n",
      "iteration 6616train_loss: 0.426874311728387 test_loss: 0.4604037403799977\n",
      "iteration 6617train_loss: 0.42687431168588696 test_loss: 0.46040379025378486\n",
      "iteration 6618train_loss: 0.4268743116434376 test_loss: 0.46040384009780566\n",
      "iteration 6619train_loss: 0.42687431160103906 test_loss: 0.4604038899120777\n",
      "iteration 6620train_loss: 0.4268743115586911 test_loss: 0.46040393969661875\n",
      "iteration 6621train_loss: 0.4268743115163937 test_loss: 0.4604039894514465\n",
      "iteration 6622train_loss: 0.426874311474147 test_loss: 0.4604040391765788\n",
      "iteration 6623train_loss: 0.4268743114319507 test_loss: 0.46040408887203316\n",
      "iteration 6624train_loss: 0.4268743113898048 test_loss: 0.4604041385378276\n",
      "iteration 6625train_loss: 0.4268743113477093 test_loss: 0.4604041881739795\n",
      "iteration 6626train_loss: 0.42687431130566406 test_loss: 0.46040423778050676\n",
      "iteration 6627train_loss: 0.4268743112636691 test_loss: 0.4604042873574269\n",
      "iteration 6628train_loss: 0.4268743112217243 test_loss: 0.4604043369047577\n",
      "iteration 6629train_loss: 0.4268743111798297 test_loss: 0.4604043864225168\n",
      "iteration 6630train_loss: 0.4268743111379851 test_loss: 0.4604044359107217\n",
      "iteration 6631train_loss: 0.4268743110961905 test_loss: 0.4604044853693902\n",
      "iteration 6632train_loss: 0.4268743110544459 test_loss: 0.4604045347985399\n",
      "iteration 6633train_loss: 0.4268743110127511 test_loss: 0.46040458419818825\n",
      "iteration 6634train_loss: 0.42687431097110623 test_loss: 0.460404633568353\n",
      "iteration 6635train_loss: 0.426874310929511 test_loss: 0.46040468290905184\n",
      "iteration 6636train_loss: 0.42687431088796546 test_loss: 0.4604047322203021\n",
      "iteration 6637train_loss: 0.42687431084646976 test_loss: 0.46040478150212144\n",
      "iteration 6638train_loss: 0.4268743108050235 test_loss: 0.4604048307545276\n",
      "iteration 6639train_loss: 0.42687431076362675 test_loss: 0.4604048799775378\n",
      "iteration 6640train_loss: 0.4268743107222796 test_loss: 0.4604049291711698\n",
      "iteration 6641train_loss: 0.4268743106809817 test_loss: 0.4604049783354412\n",
      "iteration 6642train_loss: 0.4268743106397332 test_loss: 0.4604050274703692\n",
      "iteration 6643train_loss: 0.42687431059853403 test_loss: 0.4604050765759716\n",
      "iteration 6644train_loss: 0.42687431055738406 test_loss: 0.46040512565226577\n",
      "iteration 6645train_loss: 0.42687431051628333 test_loss: 0.46040517469926917\n",
      "iteration 6646train_loss: 0.4268743104752316 test_loss: 0.4604052237169994\n",
      "iteration 6647train_loss: 0.4268743104342291 test_loss: 0.46040527270547377\n",
      "iteration 6648train_loss: 0.42687431039327534 test_loss: 0.46040532166470977\n",
      "iteration 6649train_loss: 0.4268743103523707 test_loss: 0.460405370594725\n",
      "iteration 6650train_loss: 0.426874310311515 test_loss: 0.4604054194955367\n",
      "iteration 6651train_loss: 0.4268743102707079 test_loss: 0.4604054683671624\n",
      "iteration 6652train_loss: 0.4268743102299497 test_loss: 0.4604055172096195\n",
      "iteration 6653train_loss: 0.42687431018924016 test_loss: 0.46040556602292537\n",
      "iteration 6654train_loss: 0.4268743101485793 test_loss: 0.46040561480709735\n",
      "iteration 6655train_loss: 0.4268743101079671 test_loss: 0.4604056635621529\n",
      "iteration 6656train_loss: 0.42687431006740334 test_loss: 0.4604057122881093\n",
      "iteration 6657train_loss: 0.42687431002688814 test_loss: 0.46040576098498415\n",
      "iteration 6658train_loss: 0.4268743099864212 test_loss: 0.46040580965279454\n",
      "iteration 6659train_loss: 0.4268743099460027 test_loss: 0.4604058582915579\n",
      "iteration 6660train_loss: 0.4268743099056325 test_loss: 0.4604059069012916\n",
      "iteration 6661train_loss: 0.42687430986531055 test_loss: 0.46040595548201274\n",
      "iteration 6662train_loss: 0.42687430982503677 test_loss: 0.460406004033739\n",
      "iteration 6663train_loss: 0.42687430978481106 test_loss: 0.46040605255648753\n",
      "iteration 6664train_loss: 0.42687430974463353 test_loss: 0.4604061010502755\n",
      "iteration 6665train_loss: 0.4268743097045039 test_loss: 0.4604061495151202\n",
      "iteration 6666train_loss: 0.4268743096644223 test_loss: 0.46040619795103904\n",
      "iteration 6667train_loss: 0.42687430962438866 test_loss: 0.46040624635804905\n",
      "iteration 6668train_loss: 0.4268743095844027 test_loss: 0.46040629473616773\n",
      "iteration 6669train_loss: 0.4268743095444646 test_loss: 0.4604063430854122\n",
      "iteration 6670train_loss: 0.4268743095045742 test_loss: 0.46040639140579964\n",
      "iteration 6671train_loss: 0.42687430946473154 test_loss: 0.46040643969734735\n",
      "iteration 6672train_loss: 0.4268743094249365 test_loss: 0.46040648796007255\n",
      "iteration 6673train_loss: 0.4268743093851888 test_loss: 0.46040653619399236\n",
      "iteration 6674train_loss: 0.4268743093454888 test_loss: 0.46040658439912396\n",
      "iteration 6675train_loss: 0.4268743093058363 test_loss: 0.4604066325754846\n",
      "iteration 6676train_loss: 0.42687430926623093 test_loss: 0.4604066807230914\n",
      "iteration 6677train_loss: 0.426874309226673 test_loss: 0.46040672884196154\n",
      "iteration 6678train_loss: 0.4268743091871624 test_loss: 0.4604067769321121\n",
      "iteration 6679train_loss: 0.426874309147699 test_loss: 0.4604068249935602\n",
      "iteration 6680train_loss: 0.4268743091082827 test_loss: 0.46040687302632316\n",
      "iteration 6681train_loss: 0.42687430906891355 test_loss: 0.46040692103041786\n",
      "iteration 6682train_loss: 0.4268743090295915 test_loss: 0.4604069690058615\n",
      "iteration 6683train_loss: 0.42687430899031625 test_loss: 0.4604070169526712\n",
      "iteration 6684train_loss: 0.42687430895108813 test_loss: 0.46040706487086397\n",
      "iteration 6685train_loss: 0.42687430891190686 test_loss: 0.46040711276045704\n",
      "iteration 6686train_loss: 0.4268743088727723 test_loss: 0.46040716062146725\n",
      "iteration 6687train_loss: 0.42687430883368455 test_loss: 0.46040720845391175\n",
      "iteration 6688train_loss: 0.4268743087946435 test_loss: 0.46040725625780765\n",
      "iteration 6689train_loss: 0.42687430875564913 test_loss: 0.4604073040331718\n",
      "iteration 6690train_loss: 0.42687430871670135 test_loss: 0.4604073517800215\n",
      "iteration 6691train_loss: 0.4268743086778001 test_loss: 0.46040739949837356\n",
      "iteration 6692train_loss: 0.42687430863894527 test_loss: 0.46040744718824494\n",
      "iteration 6693train_loss: 0.42687430860013703 test_loss: 0.4604074948496528\n",
      "iteration 6694train_loss: 0.4268743085613751 test_loss: 0.4604075424826141\n",
      "iteration 6695train_loss: 0.42687430852265945 test_loss: 0.46040759008714566\n",
      "iteration 6696train_loss: 0.42687430848398994 test_loss: 0.46040763766326454\n",
      "iteration 6697train_loss: 0.4268743084453669 test_loss: 0.46040768521098774\n",
      "iteration 6698train_loss: 0.42687430840678986 test_loss: 0.46040773273033214\n",
      "iteration 6699train_loss: 0.42687430836825896 test_loss: 0.4604077802213146\n",
      "iteration 6700train_loss: 0.426874308329774 test_loss: 0.4604078276839522\n",
      "iteration 6701train_loss: 0.4268743082913352 test_loss: 0.46040787511826176\n",
      "iteration 6702train_loss: 0.42687430825294215 test_loss: 0.4604079225242601\n",
      "iteration 6703train_loss: 0.4268743082145951 test_loss: 0.46040796990196436\n",
      "iteration 6704train_loss: 0.4268743081762939 test_loss: 0.46040801725139113\n",
      "iteration 6705train_loss: 0.42687430813803834 test_loss: 0.4604080645725575\n",
      "iteration 6706train_loss: 0.4268743080998285 test_loss: 0.4604081118654801\n",
      "iteration 6707train_loss: 0.42687430806166443 test_loss: 0.460408159130176\n",
      "iteration 6708train_loss: 0.4268743080235459 test_loss: 0.460408206366662\n",
      "iteration 6709train_loss: 0.426874307985473 test_loss: 0.4604082535749548\n",
      "iteration 6710train_loss: 0.42687430794744535 test_loss: 0.46040830075507144\n",
      "iteration 6711train_loss: 0.4268743079094634 test_loss: 0.4604083479070285\n",
      "iteration 6712train_loss: 0.42687430787152675 test_loss: 0.4604083950308429\n",
      "iteration 6713train_loss: 0.42687430783363545 test_loss: 0.4604084421265314\n",
      "iteration 6714train_loss: 0.4268743077957894 test_loss: 0.4604084891941108\n",
      "iteration 6715train_loss: 0.4268743077579885 test_loss: 0.46040853623359784\n",
      "iteration 6716train_loss: 0.42687430772023294 test_loss: 0.4604085832450094\n",
      "iteration 6717train_loss: 0.4268743076825225 test_loss: 0.46040863022836204\n",
      "iteration 6718train_loss: 0.4268743076448569 test_loss: 0.46040867718367257\n",
      "iteration 6719train_loss: 0.42687430760723644 test_loss: 0.4604087241109578\n",
      "iteration 6720train_loss: 0.426874307569661 test_loss: 0.4604087710102345\n",
      "iteration 6721train_loss: 0.4268743075321303 test_loss: 0.4604088178815191\n",
      "iteration 6722train_loss: 0.42687430749464456 test_loss: 0.4604088647248285\n",
      "iteration 6723train_loss: 0.4268743074572036 test_loss: 0.4604089115401794\n",
      "iteration 6724train_loss: 0.4268743074198074 test_loss: 0.46040895832758844\n",
      "iteration 6725train_loss: 0.42687430738245574 test_loss: 0.4604090050870722\n",
      "iteration 6726train_loss: 0.4268743073451489 test_loss: 0.4604090518186476\n",
      "iteration 6727train_loss: 0.42687430730788656 test_loss: 0.4604090985223311\n",
      "iteration 6728train_loss: 0.4268743072706687 test_loss: 0.46040914519813914\n",
      "iteration 6729train_loss: 0.4268743072334954 test_loss: 0.4604091918460888\n",
      "iteration 6730train_loss: 0.42687430719636643 test_loss: 0.46040923846619636\n",
      "iteration 6731train_loss: 0.4268743071592819 test_loss: 0.4604092850584786\n",
      "iteration 6732train_loss: 0.4268743071222417 test_loss: 0.4604093316229521\n",
      "iteration 6733train_loss: 0.42687430708524565 test_loss: 0.46040937815963334\n",
      "iteration 6734train_loss: 0.4268743070482939 test_loss: 0.460409424668539\n",
      "iteration 6735train_loss: 0.4268743070113863 test_loss: 0.46040947114968556\n",
      "iteration 6736train_loss: 0.4268743069745227 test_loss: 0.4604095176030898\n",
      "iteration 6737train_loss: 0.42687430693770334 test_loss: 0.4604095640287681\n",
      "iteration 6738train_loss: 0.42687430690092776 test_loss: 0.460409610426737\n",
      "iteration 6739train_loss: 0.4268743068641963 test_loss: 0.460409656797013\n",
      "iteration 6740train_loss: 0.42687430682750865 test_loss: 0.4604097031396127\n",
      "iteration 6741train_loss: 0.42687430679086485 test_loss: 0.4604097494545527\n",
      "iteration 6742train_loss: 0.4268743067542648 test_loss: 0.4604097957418493\n",
      "iteration 6743train_loss: 0.42687430671770854 test_loss: 0.4604098420015191\n",
      "iteration 6744train_loss: 0.4268743066811959 test_loss: 0.46040988823357853\n",
      "iteration 6745train_loss: 0.42687430664472686 test_loss: 0.4604099344380441\n",
      "iteration 6746train_loss: 0.42687430660830156 test_loss: 0.4604099806149324\n",
      "iteration 6747train_loss: 0.42687430657191966 test_loss: 0.4604100267642596\n",
      "iteration 6748train_loss: 0.42687430653558134 test_loss: 0.46041007288604235\n",
      "iteration 6749train_loss: 0.42687430649928626 test_loss: 0.46041011898029705\n",
      "iteration 6750train_loss: 0.4268743064630347 test_loss: 0.4604101650470401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6751train_loss: 0.42687430642682644 test_loss: 0.46041021108628793\n",
      "iteration 6752train_loss: 0.4268743063906613 test_loss: 0.46041025709805694\n",
      "iteration 6753train_loss: 0.4268743063545395 test_loss: 0.4604103030823635\n",
      "iteration 6754train_loss: 0.4268743063184608 test_loss: 0.460410349039224\n",
      "iteration 6755train_loss: 0.42687430628242534 test_loss: 0.4604103949686549\n",
      "iteration 6756train_loss: 0.4268743062464328 test_loss: 0.4604104408706724\n",
      "iteration 6757train_loss: 0.4268743062104833 test_loss: 0.46041048674529306\n",
      "iteration 6758train_loss: 0.4268743061745769 test_loss: 0.46041053259253295\n",
      "iteration 6759train_loss: 0.42687430613871324 test_loss: 0.46041057841240873\n",
      "iteration 6760train_loss: 0.4268743061028925 test_loss: 0.46041062420493656\n",
      "iteration 6761train_loss: 0.4268743060671145 test_loss: 0.46041066997013264\n",
      "iteration 6762train_loss: 0.42687430603137927 test_loss: 0.46041071570801345\n",
      "iteration 6763train_loss: 0.4268743059956868 test_loss: 0.46041076141859527\n",
      "iteration 6764train_loss: 0.42687430596003684 test_loss: 0.4604108071018944\n",
      "iteration 6765train_loss: 0.4268743059244296 test_loss: 0.460410852757927\n",
      "iteration 6766train_loss: 0.426874305888865 test_loss: 0.46041089838670934\n",
      "iteration 6767train_loss: 0.4268743058533427 test_loss: 0.4604109439882578\n",
      "iteration 6768train_loss: 0.426874305817863 test_loss: 0.4604109895625887\n",
      "iteration 6769train_loss: 0.4268743057824255 test_loss: 0.460411035109718\n",
      "iteration 6770train_loss: 0.42687430574703045 test_loss: 0.4604110806296622\n",
      "iteration 6771train_loss: 0.42687430571167784 test_loss: 0.46041112612243734\n",
      "iteration 6772train_loss: 0.4268743056763672 test_loss: 0.46041117158805966\n",
      "iteration 6773train_loss: 0.426874305641099 test_loss: 0.46041121702654547\n",
      "iteration 6774train_loss: 0.4268743056058728 test_loss: 0.46041126243791086\n",
      "iteration 6775train_loss: 0.4268743055706886 test_loss: 0.46041130782217193\n",
      "iteration 6776train_loss: 0.4268743055355466 test_loss: 0.46041135317934506\n",
      "iteration 6777train_loss: 0.4268743055004466 test_loss: 0.46041139850944623\n",
      "iteration 6778train_loss: 0.4268743054653885 test_loss: 0.46041144381249177\n",
      "iteration 6779train_loss: 0.42687430543037225 test_loss: 0.46041148908849766\n",
      "iteration 6780train_loss: 0.42687430539539784 test_loss: 0.4604115343374801\n",
      "iteration 6781train_loss: 0.42687430536046533 test_loss: 0.46041157955945533\n",
      "iteration 6782train_loss: 0.4268743053255744 test_loss: 0.46041162475443914\n",
      "iteration 6783train_loss: 0.4268743052907253 test_loss: 0.4604116699224479\n",
      "iteration 6784train_loss: 0.4268743052559178 test_loss: 0.46041171506349765\n",
      "iteration 6785train_loss: 0.4268743052211519 test_loss: 0.46041176017760455\n",
      "iteration 6786train_loss: 0.4268743051864275 test_loss: 0.4604118052647844\n",
      "iteration 6787train_loss: 0.4268743051517446 test_loss: 0.4604118503250536\n",
      "iteration 6788train_loss: 0.42687430511710317 test_loss: 0.46041189535842797\n",
      "iteration 6789train_loss: 0.4268743050825031 test_loss: 0.46041194036492367\n",
      "iteration 6790train_loss: 0.4268743050479443 test_loss: 0.46041198534455674\n",
      "iteration 6791train_loss: 0.42687430501342694 test_loss: 0.4604120302973431\n",
      "iteration 6792train_loss: 0.42687430497895074 test_loss: 0.4604120752232989\n",
      "iteration 6793train_loss: 0.4268743049445158 test_loss: 0.4604121201224401\n",
      "iteration 6794train_loss: 0.42687430491012196 test_loss: 0.4604121649947827\n",
      "iteration 6795train_loss: 0.42687430487576916 test_loss: 0.46041220984034276\n",
      "iteration 6796train_loss: 0.42687430484145744 test_loss: 0.460412254659136\n",
      "iteration 6797train_loss: 0.4268743048071868 test_loss: 0.4604122994511786\n",
      "iteration 6798train_loss: 0.426874304772957 test_loss: 0.4604123442164866\n",
      "iteration 6799train_loss: 0.42687430473876825 test_loss: 0.46041238895507564\n",
      "iteration 6800train_loss: 0.4268743047046201 test_loss: 0.46041243366696205\n",
      "iteration 6801train_loss: 0.42687430467051296 test_loss: 0.4604124783521615\n",
      "iteration 6802train_loss: 0.42687430463644654 test_loss: 0.46041252301069\n",
      "iteration 6803train_loss: 0.4268743046024209 test_loss: 0.46041256764256344\n",
      "iteration 6804train_loss: 0.4268743045684358 test_loss: 0.4604126122477978\n",
      "iteration 6805train_loss: 0.4268743045344913 test_loss: 0.46041265682640886\n",
      "iteration 6806train_loss: 0.4268743045005873 test_loss: 0.46041270137841245\n",
      "iteration 6807train_loss: 0.4268743044667239 test_loss: 0.4604127459038247\n",
      "iteration 6808train_loss: 0.426874304432901 test_loss: 0.4604127904026613\n",
      "iteration 6809train_loss: 0.4268743043991185 test_loss: 0.46041283487493806\n",
      "iteration 6810train_loss: 0.4268743043653763 test_loss: 0.4604128793206708\n",
      "iteration 6811train_loss: 0.42687430433167456 test_loss: 0.4604129237398756\n",
      "iteration 6812train_loss: 0.4268743042980129 test_loss: 0.46041296813256816\n",
      "iteration 6813train_loss: 0.42687430426439155 test_loss: 0.4604130124987642\n",
      "iteration 6814train_loss: 0.42687430423081035 test_loss: 0.4604130568384796\n",
      "iteration 6815train_loss: 0.42687430419726935 test_loss: 0.4604131011517302\n",
      "iteration 6816train_loss: 0.4268743041637684 test_loss: 0.46041314543853173\n",
      "iteration 6817train_loss: 0.42687430413030736 test_loss: 0.4604131896989\n",
      "iteration 6818train_loss: 0.4268743040968864 test_loss: 0.4604132339328508\n",
      "iteration 6819train_loss: 0.42687430406350535 test_loss: 0.46041327814039984\n",
      "iteration 6820train_loss: 0.4268743040301642 test_loss: 0.46041332232156285\n",
      "iteration 6821train_loss: 0.4268743039968629 test_loss: 0.46041336647635567\n",
      "iteration 6822train_loss: 0.42687430396360126 test_loss: 0.460413410604794\n",
      "iteration 6823train_loss: 0.42687430393037956 test_loss: 0.46041345470689343\n",
      "iteration 6824train_loss: 0.4268743038971974 test_loss: 0.46041349878266996\n",
      "iteration 6825train_loss: 0.42687430386405506 test_loss: 0.46041354283213903\n",
      "iteration 6826train_loss: 0.4268743038309522 test_loss: 0.4604135868553165\n",
      "iteration 6827train_loss: 0.42687430379788893 test_loss: 0.4604136308522179\n",
      "iteration 6828train_loss: 0.4268743037648652 test_loss: 0.46041367482285916\n",
      "iteration 6829train_loss: 0.42687430373188084 test_loss: 0.46041371876725573\n",
      "iteration 6830train_loss: 0.426874303698936 test_loss: 0.4604137626854233\n",
      "iteration 6831train_loss: 0.4268743036660303 test_loss: 0.46041380657737746\n",
      "iteration 6832train_loss: 0.4268743036331641 test_loss: 0.460413850443134\n",
      "iteration 6833train_loss: 0.4268743036003372 test_loss: 0.4604138942827085\n",
      "iteration 6834train_loss: 0.42687430356754946 test_loss: 0.4604139380961166\n",
      "iteration 6835train_loss: 0.4268743035348009 test_loss: 0.46041398188337385\n",
      "iteration 6836train_loss: 0.4268743035020915 test_loss: 0.46041402564449585\n",
      "iteration 6837train_loss: 0.426874303469421 test_loss: 0.46041406937949825\n",
      "iteration 6838train_loss: 0.4268743034367898 test_loss: 0.4604141130883966\n",
      "iteration 6839train_loss: 0.42687430340419746 test_loss: 0.4604141567712065\n",
      "iteration 6840train_loss: 0.42687430337164406 test_loss: 0.46041420042794345\n",
      "iteration 6841train_loss: 0.4268743033391296 test_loss: 0.4604142440586231\n",
      "iteration 6842train_loss: 0.42687430330665393 test_loss: 0.4604142876632609\n",
      "iteration 6843train_loss: 0.42687430327421705 test_loss: 0.46041433124187264\n",
      "iteration 6844train_loss: 0.4268743032418191 test_loss: 0.4604143747944735\n",
      "iteration 6845train_loss: 0.42687430320945974 test_loss: 0.4604144183210791\n",
      "iteration 6846train_loss: 0.426874303177139 test_loss: 0.46041446182170503\n",
      "iteration 6847train_loss: 0.42687430314485697 test_loss: 0.46041450529636685\n",
      "iteration 6848train_loss: 0.42687430311261343 test_loss: 0.4604145487450798\n",
      "iteration 6849train_loss: 0.42687430308040836 test_loss: 0.46041459216785974\n",
      "iteration 6850train_loss: 0.42687430304824187 test_loss: 0.46041463556472173\n",
      "iteration 6851train_loss: 0.42687430301611384 test_loss: 0.4604146789356816\n",
      "iteration 6852train_loss: 0.42687430298402423 test_loss: 0.4604147222807544\n",
      "iteration 6853train_loss: 0.4268743029519728 test_loss: 0.46041476559995603\n",
      "iteration 6854train_loss: 0.42687430291995976 test_loss: 0.4604148088933016\n",
      "iteration 6855train_loss: 0.42687430288798495 test_loss: 0.4604148521608067\n",
      "iteration 6856train_loss: 0.42687430285604844 test_loss: 0.4604148954024866\n",
      "iteration 6857train_loss: 0.42687430282414995 test_loss: 0.4604149386183569\n",
      "iteration 6858train_loss: 0.4268743027922896 test_loss: 0.4604149818084328\n",
      "iteration 6859train_loss: 0.42687430276046734 test_loss: 0.4604150249727299\n",
      "iteration 6860train_loss: 0.4268743027286831 test_loss: 0.4604150681112635\n",
      "iteration 6861train_loss: 0.42687430269693694 test_loss: 0.46041511122404866\n",
      "iteration 6862train_loss: 0.4268743026652285 test_loss: 0.46041515431110136\n",
      "iteration 6863train_loss: 0.4268743026335582 test_loss: 0.4604151973724365\n",
      "iteration 6864train_loss: 0.42687430260192555 test_loss: 0.46041524040806947\n",
      "iteration 6865train_loss: 0.42687430257033077 test_loss: 0.46041528341801585\n",
      "iteration 6866train_loss: 0.42687430253877373 test_loss: 0.4604153264022906\n",
      "iteration 6867train_loss: 0.4268743025072544 test_loss: 0.4604153693609094\n",
      "iteration 6868train_loss: 0.4268743024757727 test_loss: 0.4604154122938871\n",
      "iteration 6869train_loss: 0.4268743024443286 test_loss: 0.46041545520123955\n",
      "iteration 6870train_loss: 0.4268743024129221 test_loss: 0.46041549808298177\n",
      "iteration 6871train_loss: 0.4268743023815531 test_loss: 0.46041554093912895\n",
      "iteration 6872train_loss: 0.42687430235022156 test_loss: 0.4604155837696965\n",
      "iteration 6873train_loss: 0.42687430231892753 test_loss: 0.46041562657469964\n",
      "iteration 6874train_loss: 0.4268743022876708 test_loss: 0.4604156693541537\n",
      "iteration 6875train_loss: 0.4268743022564516 test_loss: 0.46041571210807375\n",
      "iteration 6876train_loss: 0.42687430222526956 test_loss: 0.46041575483647507\n",
      "iteration 6877train_loss: 0.42687430219412476 test_loss: 0.460415797539373\n",
      "iteration 6878train_loss: 0.4268743021630173 test_loss: 0.4604158402167828\n",
      "iteration 6879train_loss: 0.4268743021319468 test_loss: 0.4604158828687194\n",
      "iteration 6880train_loss: 0.4268743021009136 test_loss: 0.4604159254951983\n",
      "iteration 6881train_loss: 0.42687430206991744 test_loss: 0.4604159680962344\n",
      "iteration 6882train_loss: 0.4268743020389583 test_loss: 0.46041601067184323\n",
      "iteration 6883train_loss: 0.4268743020080361 test_loss: 0.4604160532220397\n",
      "iteration 6884train_loss: 0.426874301977151 test_loss: 0.4604160957468391\n",
      "iteration 6885train_loss: 0.4268743019463026 test_loss: 0.4604161382462564\n",
      "iteration 6886train_loss: 0.4268743019154911 test_loss: 0.46041618072030693\n",
      "iteration 6887train_loss: 0.42687430188471653 test_loss: 0.4604162231690059\n",
      "iteration 6888train_loss: 0.42687430185397857 test_loss: 0.4604162655923682\n",
      "iteration 6889train_loss: 0.4268743018232775 test_loss: 0.4604163079904089\n",
      "iteration 6890train_loss: 0.426874301792613 test_loss: 0.4604163503631435\n",
      "iteration 6891train_loss: 0.4268743017619852 test_loss: 0.4604163927105868\n",
      "iteration 6892train_loss: 0.42687430173139396 test_loss: 0.46041643503275387\n",
      "iteration 6893train_loss: 0.42687430170083934 test_loss: 0.4604164773296599\n",
      "iteration 6894train_loss: 0.42687430167032114 test_loss: 0.46041651960131996\n",
      "iteration 6895train_loss: 0.4268743016398394 test_loss: 0.4604165618477491\n",
      "iteration 6896train_loss: 0.42687430160939416 test_loss: 0.46041660406896234\n",
      "iteration 6897train_loss: 0.42687430157898537 test_loss: 0.4604166462649748\n",
      "iteration 6898train_loss: 0.4268743015486127 test_loss: 0.4604166884358014\n",
      "iteration 6899train_loss: 0.42687430151827643 test_loss: 0.46041673058145727\n",
      "iteration 6900train_loss: 0.4268743014879764 test_loss: 0.4604167727019573\n",
      "iteration 6901train_loss: 0.42687430145771255 test_loss: 0.4604168147973167\n",
      "iteration 6902train_loss: 0.4268743014274848 test_loss: 0.46041685686755024\n",
      "iteration 6903train_loss: 0.42687430139729327 test_loss: 0.4604168989126732\n",
      "iteration 6904train_loss: 0.4268743013671379 test_loss: 0.4604169409327003\n",
      "iteration 6905train_loss: 0.42687430133701837 test_loss: 0.4604169829276466\n",
      "iteration 6906train_loss: 0.4268743013069349 test_loss: 0.46041702489752706\n",
      "iteration 6907train_loss: 0.4268743012768873 test_loss: 0.46041706684235667\n",
      "iteration 6908train_loss: 0.42687430124687575 test_loss: 0.46041710876215036\n",
      "iteration 6909train_loss: 0.4268743012168999 test_loss: 0.4604171506569231\n",
      "iteration 6910train_loss: 0.42687430118695996 test_loss: 0.4604171925266897\n",
      "iteration 6911train_loss: 0.4268743011570558 test_loss: 0.46041723437146526\n",
      "iteration 6912train_loss: 0.4268743011271874 test_loss: 0.46041727619126455\n",
      "iteration 6913train_loss: 0.42687430109735464 test_loss: 0.46041731798610247\n",
      "iteration 6914train_loss: 0.42687430106755747 test_loss: 0.46041735975599407\n",
      "iteration 6915train_loss: 0.4268743010377959 test_loss: 0.4604174015009539\n",
      "iteration 6916train_loss: 0.42687430100806995 test_loss: 0.4604174432209973\n",
      "iteration 6917train_loss: 0.42687430097837953 test_loss: 0.4604174849161387\n",
      "iteration 6918train_loss: 0.4268743009487246 test_loss: 0.46041752658639334\n",
      "iteration 6919train_loss: 0.426874300919105 test_loss: 0.46041756823177576\n",
      "iteration 6920train_loss: 0.4268743008895209 test_loss: 0.46041760985230107\n",
      "iteration 6921train_loss: 0.4268743008599721 test_loss: 0.46041765144798386\n",
      "iteration 6922train_loss: 0.4268743008304586 test_loss: 0.460417693018839\n",
      "iteration 6923train_loss: 0.4268743008009804 test_loss: 0.46041773456488144\n",
      "iteration 6924train_loss: 0.42687430077153726 test_loss: 0.4604177760861259\n",
      "iteration 6925train_loss: 0.42687430074212945 test_loss: 0.46041781758258715\n",
      "iteration 6926train_loss: 0.4268743007127567 test_loss: 0.46041785905428\n",
      "iteration 6927train_loss: 0.42687430068341914 test_loss: 0.4604179005012192\n",
      "iteration 6928train_loss: 0.4268743006541166 test_loss: 0.46041794192341967\n",
      "iteration 6929train_loss: 0.42687430062484905 test_loss: 0.46041798332089584\n",
      "iteration 6930train_loss: 0.4268743005956165 test_loss: 0.4604180246936628\n",
      "iteration 6931train_loss: 0.42687430056641884 test_loss: 0.4604180660417352\n",
      "iteration 6932train_loss: 0.4268743005372561 test_loss: 0.4604181073651277\n",
      "iteration 6933train_loss: 0.4268743005081281 test_loss: 0.4604181486638551\n",
      "iteration 6934train_loss: 0.426874300479035 test_loss: 0.46041818993793204\n",
      "iteration 6935train_loss: 0.4268743004499767 test_loss: 0.46041823118737324\n",
      "iteration 6936train_loss: 0.426874300420953 test_loss: 0.4604182724121935\n",
      "iteration 6937train_loss: 0.42687430039196406 test_loss: 0.4604183136124074\n",
      "iteration 6938train_loss: 0.4268743003630098 test_loss: 0.4604183547880297\n",
      "iteration 6939train_loss: 0.42687430033409 test_loss: 0.4604183959390751\n",
      "iteration 6940train_loss: 0.42687430030520485 test_loss: 0.46041843706555813\n",
      "iteration 6941train_loss: 0.4268743002763541 test_loss: 0.46041847816749343\n",
      "iteration 6942train_loss: 0.42687430024753786 test_loss: 0.46041851924489585\n",
      "iteration 6943train_loss: 0.4268743002187561 test_loss: 0.4604185602977799\n",
      "iteration 6944train_loss: 0.4268743001900089 test_loss: 0.46041860132616025\n",
      "iteration 6945train_loss: 0.42687430016129574 test_loss: 0.4604186423300514\n",
      "iteration 6946train_loss: 0.426874300132617 test_loss: 0.46041868330946817\n",
      "iteration 6947train_loss: 0.42687430010397254 test_loss: 0.46041872426442493\n",
      "iteration 6948train_loss: 0.42687430007536226 test_loss: 0.46041876519493646\n",
      "iteration 6949train_loss: 0.42687430004678617 test_loss: 0.4604188061010172\n",
      "iteration 6950train_loss: 0.4268743000182443 test_loss: 0.460418846982682\n",
      "iteration 6951train_loss: 0.42687429998973647 test_loss: 0.4604188878399451\n",
      "iteration 6952train_loss: 0.42687429996126275 test_loss: 0.46041892867282114\n",
      "iteration 6953train_loss: 0.42687429993282294 test_loss: 0.4604189694813249\n",
      "iteration 6954train_loss: 0.4268742999044172 test_loss: 0.46041901026547055\n",
      "iteration 6955train_loss: 0.4268742998760454 test_loss: 0.460419051025273\n",
      "iteration 6956train_loss: 0.4268742998477075 test_loss: 0.4604190917607464\n",
      "iteration 6957train_loss: 0.42687429981940345 test_loss: 0.4604191324719056\n",
      "iteration 6958train_loss: 0.4268742997911332 test_loss: 0.4604191731587649\n",
      "iteration 6959train_loss: 0.4268742997628967 test_loss: 0.460419213821339\n",
      "iteration 6960train_loss: 0.426874299734694 test_loss: 0.46041925445964216\n",
      "iteration 6961train_loss: 0.42687429970652485 test_loss: 0.460419295073689\n",
      "iteration 6962train_loss: 0.4268742996783895 test_loss: 0.46041933566349397\n",
      "iteration 6963train_loss: 0.4268742996502877 test_loss: 0.4604193762290715\n",
      "iteration 6964train_loss: 0.42687429962221957 test_loss: 0.4604194167704361\n",
      "iteration 6965train_loss: 0.4268742995941849 test_loss: 0.4604194572876021\n",
      "iteration 6966train_loss: 0.4268742995661837 test_loss: 0.46041949778058405\n",
      "iteration 6967train_loss: 0.42687429953821593 test_loss: 0.4604195382493964\n",
      "iteration 6968train_loss: 0.4268742995102817 test_loss: 0.46041957869405353\n",
      "iteration 6969train_loss: 0.42687429948238076 test_loss: 0.46041961911456986\n",
      "iteration 6970train_loss: 0.4268742994545132 test_loss: 0.4604196595109597\n",
      "iteration 6971train_loss: 0.4268742994266789 test_loss: 0.46041969988323755\n",
      "iteration 6972train_loss: 0.42687429939887783 test_loss: 0.46041974023141785\n",
      "iteration 6973train_loss: 0.42687429937111004 test_loss: 0.4604197805555149\n",
      "iteration 6974train_loss: 0.4268742993433755 test_loss: 0.46041982085554306\n",
      "iteration 6975train_loss: 0.4268742993156739 test_loss: 0.46041986113151673\n",
      "iteration 6976train_loss: 0.42687429928800547 test_loss: 0.4604199013834501\n",
      "iteration 6977train_loss: 0.4268742992603701 test_loss: 0.46041994161135774\n",
      "iteration 6978train_loss: 0.4268742992327679 test_loss: 0.4604199818152539\n",
      "iteration 6979train_loss: 0.42687429920519837 test_loss: 0.4604200219951529\n",
      "iteration 6980train_loss: 0.42687429917766206 test_loss: 0.4604200621510692\n",
      "iteration 6981train_loss: 0.4268742991501584 test_loss: 0.4604201022830167\n",
      "iteration 6982train_loss: 0.4268742991226878 test_loss: 0.46042014239101026\n",
      "iteration 6983train_loss: 0.42687429909524993 test_loss: 0.46042018247506367\n",
      "iteration 6984train_loss: 0.42687429906784496 test_loss: 0.4604202225351916\n",
      "iteration 6985train_loss: 0.4268742990404726 test_loss: 0.46042026257140806\n",
      "iteration 6986train_loss: 0.42687429901313295 test_loss: 0.46042030258372746\n",
      "iteration 6987train_loss: 0.426874298985826 test_loss: 0.460420342572164\n",
      "iteration 6988train_loss: 0.42687429895855167 test_loss: 0.460420382536732\n",
      "iteration 6989train_loss: 0.42687429893130996 test_loss: 0.4604204224774456\n",
      "iteration 6990train_loss: 0.4268742989041008 test_loss: 0.4604204623943191\n",
      "iteration 6991train_loss: 0.4268742988769241 test_loss: 0.4604205022873667\n",
      "iteration 6992train_loss: 0.4268742988497799 test_loss: 0.46042054215660266\n",
      "iteration 6993train_loss: 0.4268742988226681 test_loss: 0.46042058200204106\n",
      "iteration 6994train_loss: 0.4268742987955887 test_loss: 0.46042062182369625\n",
      "iteration 6995train_loss: 0.4268742987685416 test_loss: 0.4604206616215824\n",
      "iteration 6996train_loss: 0.4268742987415269 test_loss: 0.4604207013957136\n",
      "iteration 6997train_loss: 0.42687429871454446 test_loss: 0.4604207411461041\n",
      "iteration 6998train_loss: 0.42687429868759424 test_loss: 0.46042078087276817\n",
      "iteration 6999train_loss: 0.4268742986606763 test_loss: 0.4604208205757197\n",
      "iteration 7000train_loss: 0.42687429863379045 test_loss: 0.46042086025497303\n",
      "iteration 7001train_loss: 0.4268742986069367 test_loss: 0.4604208999105423\n",
      "iteration 7002train_loss: 0.426874298580115 test_loss: 0.4604209395424415\n",
      "iteration 7003train_loss: 0.4268742985533253 test_loss: 0.460420979150685\n",
      "iteration 7004train_loss: 0.4268742985265678 test_loss: 0.46042101873528657\n",
      "iteration 7005train_loss: 0.42687429849984215 test_loss: 0.4604210582962606\n",
      "iteration 7006train_loss: 0.42687429847314845 test_loss: 0.4604210978336211\n",
      "iteration 7007train_loss: 0.4268742984464867 test_loss: 0.46042113734738227\n",
      "iteration 7008train_loss: 0.4268742984198568 test_loss: 0.46042117683755795\n",
      "iteration 7009train_loss: 0.42687429839325863 test_loss: 0.4604212163041624\n",
      "iteration 7010train_loss: 0.4268742983666922 test_loss: 0.46042125574720966\n",
      "iteration 7011train_loss: 0.4268742983401576 test_loss: 0.46042129516671365\n",
      "iteration 7012train_loss: 0.4268742983136547 test_loss: 0.4604213345626886\n",
      "iteration 7013train_loss: 0.42687429828718354 test_loss: 0.4604213739351484\n",
      "iteration 7014train_loss: 0.42687429826074375 test_loss: 0.46042141328410735\n",
      "iteration 7015train_loss: 0.42687429823433576 test_loss: 0.4604214526095792\n",
      "iteration 7016train_loss: 0.4268742982079593 test_loss: 0.46042149191157783\n",
      "iteration 7017train_loss: 0.42687429818161426 test_loss: 0.46042153119011764\n",
      "iteration 7018train_loss: 0.42687429815530087 test_loss: 0.4604215704452125\n",
      "iteration 7019train_loss: 0.4268742981290188 test_loss: 0.46042160967687623\n",
      "iteration 7020train_loss: 0.42687429810276817 test_loss: 0.460421648885123\n",
      "iteration 7021train_loss: 0.4268742980765488 test_loss: 0.46042168806996675\n",
      "iteration 7022train_loss: 0.42687429805036087 test_loss: 0.46042172723142133\n",
      "iteration 7023train_loss: 0.4268742980242042 test_loss: 0.4604217663695008\n",
      "iteration 7024train_loss: 0.42687429799807874 test_loss: 0.46042180548421896\n",
      "iteration 7025train_loss: 0.4268742979719845 test_loss: 0.46042184457559\n",
      "iteration 7026train_loss: 0.42687429794592135 test_loss: 0.46042188364362774\n",
      "iteration 7027train_loss: 0.42687429791988957 test_loss: 0.460421922688346\n",
      "iteration 7028train_loss: 0.42687429789388875 test_loss: 0.46042196170975885\n",
      "iteration 7029train_loss: 0.426874297867919 test_loss: 0.4604220007078801\n",
      "iteration 7030train_loss: 0.4268742978419802 test_loss: 0.46042203968272366\n",
      "iteration 7031train_loss: 0.42687429781607256 test_loss: 0.46042207863430346\n",
      "iteration 7032train_loss: 0.4268742977901957 test_loss: 0.4604221175626334\n",
      "iteration 7033train_loss: 0.4268742977643498 test_loss: 0.46042215646772716\n",
      "iteration 7034train_loss: 0.42687429773853486 test_loss: 0.46042219534959883\n",
      "iteration 7035train_loss: 0.42687429771275076 test_loss: 0.4604222342082622\n",
      "iteration 7036train_loss: 0.4268742976869974 test_loss: 0.46042227304373123\n",
      "iteration 7037train_loss: 0.4268742976612749 test_loss: 0.46042231185601945\n",
      "iteration 7038train_loss: 0.42687429763558304 test_loss: 0.4604223506451409\n",
      "iteration 7039train_loss: 0.42687429760992185 test_loss: 0.46042238941110947\n",
      "iteration 7040train_loss: 0.4268742975842914 test_loss: 0.46042242815393886\n",
      "iteration 7041train_loss: 0.4268742975586915 test_loss: 0.4604224668736429\n",
      "iteration 7042train_loss: 0.4268742975331223 test_loss: 0.4604225055702354\n",
      "iteration 7043train_loss: 0.42687429750758354 test_loss: 0.46042254424373014\n",
      "iteration 7044train_loss: 0.4268742974820752 test_loss: 0.460422582894141\n",
      "iteration 7045train_loss: 0.42687429745659755 test_loss: 0.46042262152148145\n",
      "iteration 7046train_loss: 0.4268742974311503 test_loss: 0.4604226601257656\n",
      "iteration 7047train_loss: 0.42687429740573335 test_loss: 0.460422698707007\n",
      "iteration 7048train_loss: 0.42687429738034677 test_loss: 0.4604227372652196\n",
      "iteration 7049train_loss: 0.4268742973549906 test_loss: 0.46042277580041696\n",
      "iteration 7050train_loss: 0.4268742973296647 test_loss: 0.4604228143126129\n",
      "iteration 7051train_loss: 0.42687429730436904 test_loss: 0.460422852801821\n",
      "iteration 7052train_loss: 0.42687429727910353 test_loss: 0.46042289126805525\n",
      "iteration 7053train_loss: 0.42687429725386833 test_loss: 0.4604229297113291\n",
      "iteration 7054train_loss: 0.42687429722866327 test_loss: 0.4604229681316564\n",
      "iteration 7055train_loss: 0.4268742972034883 test_loss: 0.46042300652905077\n",
      "iteration 7056train_loss: 0.42687429717834335 test_loss: 0.46042304490352587\n",
      "iteration 7057train_loss: 0.4268742971532285 test_loss: 0.46042308325509557\n",
      "iteration 7058train_loss: 0.4268742971281437 test_loss: 0.4604231215837733\n",
      "iteration 7059train_loss: 0.4268742971030888 test_loss: 0.4604231598895728\n",
      "iteration 7060train_loss: 0.4268742970780638 test_loss: 0.4604231981725077\n",
      "iteration 7061train_loss: 0.42687429705306873 test_loss: 0.46042323643259164\n",
      "iteration 7062train_loss: 0.4268742970281034 test_loss: 0.46042327466983846\n",
      "iteration 7063train_loss: 0.4268742970031681 test_loss: 0.4604233128842614\n",
      "iteration 7064train_loss: 0.42687429697826246 test_loss: 0.4604233510758745\n",
      "iteration 7065train_loss: 0.42687429695338674 test_loss: 0.46042338924469106\n",
      "iteration 7066train_loss: 0.4268742969285406 test_loss: 0.4604234273907248\n",
      "iteration 7067train_loss: 0.42687429690372414 test_loss: 0.4604234655139893\n",
      "iteration 7068train_loss: 0.42687429687893735 test_loss: 0.4604235036144983\n",
      "iteration 7069train_loss: 0.4268742968541802 test_loss: 0.4604235416922651\n",
      "iteration 7070train_loss: 0.42687429682945255 test_loss: 0.46042357974730347\n",
      "iteration 7071train_loss: 0.42687429680475464 test_loss: 0.46042361777962676\n",
      "iteration 7072train_loss: 0.42687429678008604 test_loss: 0.46042365578924876\n",
      "iteration 7073train_loss: 0.4268742967554469 test_loss: 0.4604236937761829\n",
      "iteration 7074train_loss: 0.4268742967308374 test_loss: 0.4604237317404428\n",
      "iteration 7075train_loss: 0.4268742967062571 test_loss: 0.4604237696820419\n",
      "iteration 7076train_loss: 0.42687429668170623 test_loss: 0.4604238076009938\n",
      "iteration 7077train_loss: 0.42687429665718474 test_loss: 0.46042384549731186\n",
      "iteration 7078train_loss: 0.4268742966326925 test_loss: 0.46042388337100987\n",
      "iteration 7079train_loss: 0.4268742966082295 test_loss: 0.460423921222101\n",
      "iteration 7080train_loss: 0.4268742965837958 test_loss: 0.46042395905059896\n",
      "iteration 7081train_loss: 0.42687429655939124 test_loss: 0.46042399685651714\n",
      "iteration 7082train_loss: 0.4268742965350158 test_loss: 0.460424034639869\n",
      "iteration 7083train_loss: 0.4268742965106695 test_loss: 0.4604240724006681\n",
      "iteration 7084train_loss: 0.4268742964863523 test_loss: 0.4604241101389278\n",
      "iteration 7085train_loss: 0.42687429646206415 test_loss: 0.46042414785466157\n",
      "iteration 7086train_loss: 0.42687429643780506 test_loss: 0.460424185547883\n",
      "iteration 7087train_loss: 0.4268742964135748 test_loss: 0.4604242232186053\n",
      "iteration 7088train_loss: 0.42687429638937363 test_loss: 0.460424260866842\n",
      "iteration 7089train_loss: 0.42687429636520136 test_loss: 0.46042429849260647\n",
      "iteration 7090train_loss: 0.42687429634105795 test_loss: 0.46042433609591216\n",
      "iteration 7091train_loss: 0.42687429631694346 test_loss: 0.4604243736767725\n",
      "iteration 7092train_loss: 0.4268742962928576 test_loss: 0.4604244112352008\n",
      "iteration 7093train_loss: 0.42687429626880075 test_loss: 0.46042444877121064\n",
      "iteration 7094train_loss: 0.42687429624477247 test_loss: 0.4604244862848152\n",
      "iteration 7095train_loss: 0.4268742962207729 test_loss: 0.4604245237760278\n",
      "iteration 7096train_loss: 0.426874296196802 test_loss: 0.4604245612448619\n",
      "iteration 7097train_loss: 0.4268742961728598 test_loss: 0.46042459869133096\n",
      "iteration 7098train_loss: 0.42687429614894623 test_loss: 0.46042463611544826\n",
      "iteration 7099train_loss: 0.42687429612506117 test_loss: 0.460424673517227\n",
      "iteration 7100train_loss: 0.42687429610120464 test_loss: 0.4604247108966807\n",
      "iteration 7101train_loss: 0.42687429607737665 test_loss: 0.4604247482538225\n",
      "iteration 7102train_loss: 0.42687429605357713 test_loss: 0.46042478558866584\n",
      "iteration 7103train_loss: 0.426874296029806 test_loss: 0.4604248229012241\n",
      "iteration 7104train_loss: 0.42687429600606325 test_loss: 0.46042486019151024\n",
      "iteration 7105train_loss: 0.4268742959823489 test_loss: 0.46042489745953796\n",
      "iteration 7106train_loss: 0.4268742959586629 test_loss: 0.46042493470532037\n",
      "iteration 7107train_loss: 0.42687429593500514 test_loss: 0.4604249719288707\n",
      "iteration 7108train_loss: 0.4268742959113757 test_loss: 0.4604250091302023\n",
      "iteration 7109train_loss: 0.4268742958877745 test_loss: 0.4604250463093283\n",
      "iteration 7110train_loss: 0.4268742958642015 test_loss: 0.46042508346626204\n",
      "iteration 7111train_loss: 0.42687429584065656 test_loss: 0.46042512060101676\n",
      "iteration 7112train_loss: 0.4268742958171398 test_loss: 0.4604251577136057\n",
      "iteration 7113train_loss: 0.42687429579365127 test_loss: 0.4604251948040421\n",
      "iteration 7114train_loss: 0.4268742957701907 test_loss: 0.4604252318723392\n",
      "iteration 7115train_loss: 0.42687429574675806 test_loss: 0.46042526891851016\n",
      "iteration 7116train_loss: 0.42687429572335345 test_loss: 0.4604253059425681\n",
      "iteration 7117train_loss: 0.4268742956999769 test_loss: 0.4604253429445263\n",
      "iteration 7118train_loss: 0.42687429567662827 test_loss: 0.46042537992439797\n",
      "iteration 7119train_loss: 0.4268742956533074 test_loss: 0.4604254168821963\n",
      "iteration 7120train_loss: 0.42687429563001444 test_loss: 0.4604254538179345\n",
      "iteration 7121train_loss: 0.42687429560674944 test_loss: 0.4604254907316256\n",
      "iteration 7122train_loss: 0.4268742955835122 test_loss: 0.4604255276232828\n",
      "iteration 7123train_loss: 0.42687429556030265 test_loss: 0.46042556449291927\n",
      "iteration 7124train_loss: 0.4268742955371208 test_loss: 0.4604256013405482\n",
      "iteration 7125train_loss: 0.42687429551396666 test_loss: 0.46042563816618265\n",
      "iteration 7126train_loss: 0.42687429549084027 test_loss: 0.4604256749698359\n",
      "iteration 7127train_loss: 0.42687429546774136 test_loss: 0.4604257117515207\n",
      "iteration 7128train_loss: 0.42687429544467015 test_loss: 0.46042574851125057\n",
      "iteration 7129train_loss: 0.42687429542162647 test_loss: 0.4604257852490383\n",
      "iteration 7130train_loss: 0.4268742953986104 test_loss: 0.4604258219648973\n",
      "iteration 7131train_loss: 0.4268742953756217 test_loss: 0.46042585865884034\n",
      "iteration 7132train_loss: 0.4268742953526605 test_loss: 0.46042589533088063\n",
      "iteration 7133train_loss: 0.4268742953297268 test_loss: 0.46042593198103127\n",
      "iteration 7134train_loss: 0.4268742953068205 test_loss: 0.4604259686093054\n",
      "iteration 7135train_loss: 0.4268742952839415 test_loss: 0.46042600521571575\n",
      "iteration 7136train_loss: 0.4268742952610899 test_loss: 0.46042604180027574\n",
      "iteration 7137train_loss: 0.4268742952382655 test_loss: 0.46042607836299826\n",
      "iteration 7138train_loss: 0.42687429521546844 test_loss: 0.4604261149038963\n",
      "iteration 7139train_loss: 0.4268742951926986 test_loss: 0.4604261514229829\n",
      "iteration 7140train_loss: 0.42687429516995595 test_loss: 0.46042618792027107\n",
      "iteration 7141train_loss: 0.42687429514724046 test_loss: 0.46042622439577385\n",
      "iteration 7142train_loss: 0.4268742951245521 test_loss: 0.4604262608495042\n",
      "iteration 7143train_loss: 0.42687429510189095 test_loss: 0.46042629728147527\n",
      "iteration 7144train_loss: 0.4268742950792568 test_loss: 0.4604263336916998\n",
      "iteration 7145train_loss: 0.4268742950566496 test_loss: 0.46042637008019094\n",
      "iteration 7146train_loss: 0.42687429503406954 test_loss: 0.4604264064469616\n",
      "iteration 7147train_loss: 0.42687429501151636 test_loss: 0.46042644279202477\n",
      "iteration 7148train_loss: 0.42687429498899016 test_loss: 0.4604264791153933\n",
      "iteration 7149train_loss: 0.42687429496649093 test_loss: 0.4604265154170803\n",
      "iteration 7150train_loss: 0.4268742949440185 test_loss: 0.46042655169709856\n",
      "iteration 7151train_loss: 0.4268742949215729 test_loss: 0.46042658795546115\n",
      "iteration 7152train_loss: 0.4268742948991543 test_loss: 0.4604266241921808\n",
      "iteration 7153train_loss: 0.4268742948767623 test_loss: 0.4604266604072706\n",
      "iteration 7154train_loss: 0.42687429485439704 test_loss: 0.4604266966007434\n",
      "iteration 7155train_loss: 0.4268742948320586 test_loss: 0.4604267327726121\n",
      "iteration 7156train_loss: 0.42687429480974676 test_loss: 0.46042676892288953\n",
      "iteration 7157train_loss: 0.4268742947874616 test_loss: 0.4604268050515887\n",
      "iteration 7158train_loss: 0.42687429476520306 test_loss: 0.4604268411587224\n",
      "iteration 7159train_loss: 0.42687429474297117 test_loss: 0.46042687724430365\n",
      "iteration 7160train_loss: 0.4268742947207658 test_loss: 0.4604269133083449\n",
      "iteration 7161train_loss: 0.42687429469858695 test_loss: 0.4604269493508594\n",
      "iteration 7162train_loss: 0.4268742946764346 test_loss: 0.46042698537186\n",
      "iteration 7163train_loss: 0.42687429465430876 test_loss: 0.4604270213713593\n",
      "iteration 7164train_loss: 0.4268742946322093 test_loss: 0.46042705734937023\n",
      "iteration 7165train_loss: 0.4268742946101362 test_loss: 0.46042709330590564\n",
      "iteration 7166train_loss: 0.42687429458808956 test_loss: 0.4604271292409784\n",
      "iteration 7167train_loss: 0.4268742945660692 test_loss: 0.46042716515460125\n",
      "iteration 7168train_loss: 0.4268742945440752 test_loss: 0.4604272010467869\n",
      "iteration 7169train_loss: 0.4268742945221074 test_loss: 0.46042723691754833\n",
      "iteration 7170train_loss: 0.42687429450016595 test_loss: 0.46042727276689815\n",
      "iteration 7171train_loss: 0.42687429447825065 test_loss: 0.46042730859484926\n",
      "iteration 7172train_loss: 0.42687429445636144 test_loss: 0.4604273444014145\n",
      "iteration 7173train_loss: 0.4268742944344985 test_loss: 0.46042738018660634\n",
      "iteration 7174train_loss: 0.42687429441266167 test_loss: 0.4604274159504377\n",
      "iteration 7175train_loss: 0.42687429439085095 test_loss: 0.46042745169292143\n",
      "iteration 7176train_loss: 0.4268742943690662 test_loss: 0.4604274874140702\n",
      "iteration 7177train_loss: 0.4268742943473074 test_loss: 0.46042752311389656\n",
      "iteration 7178train_loss: 0.4268742943255749 test_loss: 0.46042755879241354\n",
      "iteration 7179train_loss: 0.4268742943038681 test_loss: 0.46042759444963366\n",
      "iteration 7180train_loss: 0.4268742942821873 test_loss: 0.4604276300855697\n",
      "iteration 7181train_loss: 0.4268742942605324 test_loss: 0.46042766570023436\n",
      "iteration 7182train_loss: 0.42687429423890344 test_loss: 0.46042770129364025\n",
      "iteration 7183train_loss: 0.4268742942173002 test_loss: 0.46042773686580013\n",
      "iteration 7184train_loss: 0.4268742941957228 test_loss: 0.4604277724167266\n",
      "iteration 7185train_loss: 0.4268742941741712 test_loss: 0.4604278079464325\n",
      "iteration 7186train_loss: 0.42687429415264533 test_loss: 0.4604278434549304\n",
      "iteration 7187train_loss: 0.4268742941311452 test_loss: 0.46042787894223286\n",
      "iteration 7188train_loss: 0.4268742941096707 test_loss: 0.46042791440835257\n",
      "iteration 7189train_loss: 0.426874294088222 test_loss: 0.4604279498533023\n",
      "iteration 7190train_loss: 0.4268742940667987 test_loss: 0.46042798527709466\n",
      "iteration 7191train_loss: 0.42687429404540117 test_loss: 0.4604280206797421\n",
      "iteration 7192train_loss: 0.4268742940240292 test_loss: 0.46042805606125736\n",
      "iteration 7193train_loss: 0.42687429400268273 test_loss: 0.46042809142165303\n",
      "iteration 7194train_loss: 0.42687429398136173 test_loss: 0.46042812676094175\n",
      "iteration 7195train_loss: 0.42687429396006615 test_loss: 0.4604281620791361\n",
      "iteration 7196train_loss: 0.4268742939387961 test_loss: 0.4604281973762487\n",
      "iteration 7197train_loss: 0.42687429391755144 test_loss: 0.460428232652292\n",
      "iteration 7198train_loss: 0.42687429389633214 test_loss: 0.46042826790727864\n",
      "iteration 7199train_loss: 0.42687429387513826 test_loss: 0.4604283031412213\n",
      "iteration 7200train_loss: 0.4268742938539696 test_loss: 0.46042833835413244\n",
      "iteration 7201train_loss: 0.42687429383282627 test_loss: 0.4604283735460245\n",
      "iteration 7202train_loss: 0.42687429381170816 test_loss: 0.4604284087169101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7203train_loss: 0.42687429379061526 test_loss: 0.46042844386680193\n",
      "iteration 7204train_loss: 0.4268742937695477 test_loss: 0.46042847899571243\n",
      "iteration 7205train_loss: 0.4268742937485052 test_loss: 0.46042851410365404\n",
      "iteration 7206train_loss: 0.42687429372748786 test_loss: 0.4604285491906393\n",
      "iteration 7207train_loss: 0.42687429370649566 test_loss: 0.46042858425668076\n",
      "iteration 7208train_loss: 0.4268742936855285 test_loss: 0.4604286193017909\n",
      "iteration 7209train_loss: 0.4268742936645864 test_loss: 0.4604286543259821\n",
      "iteration 7210train_loss: 0.4268742936436693 test_loss: 0.4604286893292671\n",
      "iteration 7211train_loss: 0.4268742936227772 test_loss: 0.46042872431165816\n",
      "iteration 7212train_loss: 0.4268742936019101 test_loss: 0.4604287592731679\n",
      "iteration 7213train_loss: 0.4268742935810679 test_loss: 0.4604287942138085\n",
      "iteration 7214train_loss: 0.42687429356025053 test_loss: 0.4604288291335927\n",
      "iteration 7215train_loss: 0.42687429353945816 test_loss: 0.4604288640325329\n",
      "iteration 7216train_loss: 0.4268742935186905 test_loss: 0.4604288989106415\n",
      "iteration 7217train_loss: 0.42687429349794764 test_loss: 0.4604289337679308\n",
      "iteration 7218train_loss: 0.4268742934772297 test_loss: 0.4604289686044134\n",
      "iteration 7219train_loss: 0.42687429345653644 test_loss: 0.4604290034201018\n",
      "iteration 7220train_loss: 0.4268742934358679 test_loss: 0.4604290382150081\n",
      "iteration 7221train_loss: 0.42687429341522404 test_loss: 0.4604290729891449\n",
      "iteration 7222train_loss: 0.42687429339460475 test_loss: 0.46042910774252466\n",
      "iteration 7223train_loss: 0.4268742933740103 test_loss: 0.4604291424751596\n",
      "iteration 7224train_loss: 0.4268742933534403 test_loss: 0.46042917718706217\n",
      "iteration 7225train_loss: 0.426874293332895 test_loss: 0.4604292118782447\n",
      "iteration 7226train_loss: 0.42687429331237414 test_loss: 0.4604292465487198\n",
      "iteration 7227train_loss: 0.42687429329187776 test_loss: 0.46042928119849946\n",
      "iteration 7228train_loss: 0.42687429327140586 test_loss: 0.4604293158275961\n",
      "iteration 7229train_loss: 0.42687429325095855 test_loss: 0.4604293504360223\n",
      "iteration 7230train_loss: 0.4268742932305355 test_loss: 0.4604293850237903\n",
      "iteration 7231train_loss: 0.42687429321013703 test_loss: 0.46042941959091227\n",
      "iteration 7232train_loss: 0.42687429318976283 test_loss: 0.4604294541374007\n",
      "iteration 7233train_loss: 0.42687429316941305 test_loss: 0.4604294886632678\n",
      "iteration 7234train_loss: 0.4268742931490875 test_loss: 0.460429523168526\n",
      "iteration 7235train_loss: 0.42687429312878616 test_loss: 0.4604295576531875\n",
      "iteration 7236train_loss: 0.4268742931085092 test_loss: 0.4604295921172645\n",
      "iteration 7237train_loss: 0.42687429308825636 test_loss: 0.4604296265607695\n",
      "iteration 7238train_loss: 0.4268742930680278 test_loss: 0.4604296609837147\n",
      "iteration 7239train_loss: 0.4268742930478233 test_loss: 0.4604296953861123\n",
      "iteration 7240train_loss: 0.426874293027643 test_loss: 0.46042972976797447\n",
      "iteration 7241train_loss: 0.42687429300748686 test_loss: 0.46042976412931386\n",
      "iteration 7242train_loss: 0.4268742929873548 test_loss: 0.4604297984701422\n",
      "iteration 7243train_loss: 0.4268742929672467 test_loss: 0.4604298327904721\n",
      "iteration 7244train_loss: 0.4268742929471628 test_loss: 0.46042986709031575\n",
      "iteration 7245train_loss: 0.4268742929271027 test_loss: 0.46042990136968515\n",
      "iteration 7246train_loss: 0.4268742929070667 test_loss: 0.46042993562859275\n",
      "iteration 7247train_loss: 0.4268742928870545 test_loss: 0.46042996986705065\n",
      "iteration 7248train_loss: 0.4268742928670663 test_loss: 0.46043000408507107\n",
      "iteration 7249train_loss: 0.42687429284710193 test_loss: 0.46043003828266627\n",
      "iteration 7250train_loss: 0.4268742928271614 test_loss: 0.4604300724598484\n",
      "iteration 7251train_loss: 0.42687429280724476 test_loss: 0.4604301066166295\n",
      "iteration 7252train_loss: 0.42687429278735184 test_loss: 0.46043014075302197\n",
      "iteration 7253train_loss: 0.4268742927674826 test_loss: 0.46043017486903787\n",
      "iteration 7254train_loss: 0.4268742927476373 test_loss: 0.4604302089646894\n",
      "iteration 7255train_loss: 0.4268742927278156 test_loss: 0.46043024303998864\n",
      "iteration 7256train_loss: 0.4268742927080176 test_loss: 0.4604302770949478\n",
      "iteration 7257train_loss: 0.4268742926882433 test_loss: 0.46043031112957894\n",
      "iteration 7258train_loss: 0.4268742926684925 test_loss: 0.46043034514389425\n",
      "iteration 7259train_loss: 0.42687429264876536 test_loss: 0.4604303791379059\n",
      "iteration 7260train_loss: 0.4268742926290618 test_loss: 0.4604304131116259\n",
      "iteration 7261train_loss: 0.42687429260938176 test_loss: 0.4604304470650665\n",
      "iteration 7262train_loss: 0.4268742925897253 test_loss: 0.4604304809982397\n",
      "iteration 7263train_loss: 0.4268742925700922 test_loss: 0.46043051491115755\n",
      "iteration 7264train_loss: 0.42687429255048265 test_loss: 0.46043054880383216\n",
      "iteration 7265train_loss: 0.4268742925308965 test_loss: 0.4604305826762759\n",
      "iteration 7266train_loss: 0.4268742925113337 test_loss: 0.46043061652850026\n",
      "iteration 7267train_loss: 0.4268742924917943 test_loss: 0.46043065036051783\n",
      "iteration 7268train_loss: 0.42687429247227826 test_loss: 0.4604306841723404\n",
      "iteration 7269train_loss: 0.4268742924527856 test_loss: 0.4604307179639801\n",
      "iteration 7270train_loss: 0.42687429243331615 test_loss: 0.46043075173544906\n",
      "iteration 7271train_loss: 0.42687429241386987 test_loss: 0.4604307854867592\n",
      "iteration 7272train_loss: 0.4268742923944469 test_loss: 0.4604308192179225\n",
      "iteration 7273train_loss: 0.42687429237504715 test_loss: 0.46043085292895114\n",
      "iteration 7274train_loss: 0.4268742923556706 test_loss: 0.460430886619857\n",
      "iteration 7275train_loss: 0.4268742923363171 test_loss: 0.4604309202906522\n",
      "iteration 7276train_loss: 0.42687429231698687 test_loss: 0.4604309539413487\n",
      "iteration 7277train_loss: 0.42687429229767954 test_loss: 0.46043098757195844\n",
      "iteration 7278train_loss: 0.42687429227839546 test_loss: 0.4604310211824934\n",
      "iteration 7279train_loss: 0.4268742922591343 test_loss: 0.4604310547729657\n",
      "iteration 7280train_loss: 0.42687429223989615 test_loss: 0.4604310883433871\n",
      "iteration 7281train_loss: 0.4268742922206811 test_loss: 0.4604311218937698\n",
      "iteration 7282train_loss: 0.42687429220148887 test_loss: 0.46043115542412566\n",
      "iteration 7283train_loss: 0.4268742921823196 test_loss: 0.4604311889344665\n",
      "iteration 7284train_loss: 0.42687429216317324 test_loss: 0.4604312224248045\n",
      "iteration 7285train_loss: 0.42687429214404976 test_loss: 0.4604312558951513\n",
      "iteration 7286train_loss: 0.42687429212494915 test_loss: 0.4604312893455192\n",
      "iteration 7287train_loss: 0.4268742921058713 test_loss: 0.46043132277591986\n",
      "iteration 7288train_loss: 0.4268742920868164 test_loss: 0.4604313561863652\n",
      "iteration 7289train_loss: 0.42687429206778404 test_loss: 0.46043138957686724\n",
      "iteration 7290train_loss: 0.42687429204877453 test_loss: 0.4604314229474379\n",
      "iteration 7291train_loss: 0.4268742920297877 test_loss: 0.46043145629808896\n",
      "iteration 7292train_loss: 0.42687429201082344 test_loss: 0.46043148962883224\n",
      "iteration 7293train_loss: 0.426874291991882 test_loss: 0.46043152293967987\n",
      "iteration 7294train_loss: 0.42687429197296317 test_loss: 0.4604315562306436\n",
      "iteration 7295train_loss: 0.4268742919540669 test_loss: 0.4604315895017352\n",
      "iteration 7296train_loss: 0.4268742919351932 test_loss: 0.4604316227529667\n",
      "iteration 7297train_loss: 0.42687429191634213 test_loss: 0.46043165598434976\n",
      "iteration 7298train_loss: 0.42687429189751336 test_loss: 0.46043168919589633\n",
      "iteration 7299train_loss: 0.4268742918787073 test_loss: 0.4604317223876184\n",
      "iteration 7300train_loss: 0.4268742918599237 test_loss: 0.4604317555595276\n",
      "iteration 7301train_loss: 0.42687429184116243 test_loss: 0.4604317887116357\n",
      "iteration 7302train_loss: 0.42687429182242365 test_loss: 0.46043182184395465\n",
      "iteration 7303train_loss: 0.42687429180370723 test_loss: 0.4604318549564962\n",
      "iteration 7304train_loss: 0.4268742917850131 test_loss: 0.4604318880492723\n",
      "iteration 7305train_loss: 0.4268742917663414 test_loss: 0.4604319211222946\n",
      "iteration 7306train_loss: 0.426874291747692 test_loss: 0.46043195417557475\n",
      "iteration 7307train_loss: 0.4268742917290649 test_loss: 0.4604319872091249\n",
      "iteration 7308train_loss: 0.42687429171045993 test_loss: 0.46043202022295654\n",
      "iteration 7309train_loss: 0.42687429169187724 test_loss: 0.4604320532170814\n",
      "iteration 7310train_loss: 0.4268742916733168 test_loss: 0.46043208619151155\n",
      "iteration 7311train_loss: 0.42687429165477847 test_loss: 0.4604321191462584\n",
      "iteration 7312train_loss: 0.4268742916362623 test_loss: 0.4604321520813339\n",
      "iteration 7313train_loss: 0.4268742916177682 test_loss: 0.46043218499674976\n",
      "iteration 7314train_loss: 0.42687429159929635 test_loss: 0.4604322178925177\n",
      "iteration 7315train_loss: 0.42687429158084644 test_loss: 0.46043225076864935\n",
      "iteration 7316train_loss: 0.4268742915624186 test_loss: 0.4604322836251566\n",
      "iteration 7317train_loss: 0.4268742915440128 test_loss: 0.46043231646205096\n",
      "iteration 7318train_loss: 0.42687429152562895 test_loss: 0.4604323492793444\n",
      "iteration 7319train_loss: 0.426874291507267 test_loss: 0.46043238207704834\n",
      "iteration 7320train_loss: 0.4268742914889271 test_loss: 0.4604324148551746\n",
      "iteration 7321train_loss: 0.42687429147060907 test_loss: 0.460432447613735\n",
      "iteration 7322train_loss: 0.42687429145231287 test_loss: 0.46043248035274087\n",
      "iteration 7323train_loss: 0.4268742914340386 test_loss: 0.46043251307220423\n",
      "iteration 7324train_loss: 0.4268742914157861 test_loss: 0.46043254577213655\n",
      "iteration 7325train_loss: 0.4268742913975554 test_loss: 0.46043257845254953\n",
      "iteration 7326train_loss: 0.4268742913793465 test_loss: 0.4604326111134548\n",
      "iteration 7327train_loss: 0.42687429136115945 test_loss: 0.46043264375486403\n",
      "iteration 7328train_loss: 0.42687429134299404 test_loss: 0.46043267637678886\n",
      "iteration 7329train_loss: 0.42687429132485033 test_loss: 0.4604327089792408\n",
      "iteration 7330train_loss: 0.4268742913067282 test_loss: 0.46043274156223174\n",
      "iteration 7331train_loss: 0.4268742912886278 test_loss: 0.46043277412577305\n",
      "iteration 7332train_loss: 0.42687429127054916 test_loss: 0.4604328066698764\n",
      "iteration 7333train_loss: 0.42687429125249193 test_loss: 0.46043283919455347\n",
      "iteration 7334train_loss: 0.4268742912344563 test_loss: 0.4604328716998157\n",
      "iteration 7335train_loss: 0.4268742912164423 test_loss: 0.46043290418567473\n",
      "iteration 7336train_loss: 0.4268742911984497 test_loss: 0.46043293665214224\n",
      "iteration 7337train_loss: 0.4268742911804787 test_loss: 0.4604329690992297\n",
      "iteration 7338train_loss: 0.4268742911625291 test_loss: 0.46043300152694877\n",
      "iteration 7339train_loss: 0.4268742911446009 test_loss: 0.46043303393531093\n",
      "iteration 7340train_loss: 0.4268742911266942 test_loss: 0.4604330663243278\n",
      "iteration 7341train_loss: 0.4268742911088089 test_loss: 0.4604330986940108\n",
      "iteration 7342train_loss: 0.42687429109094505 test_loss: 0.46043313104437156\n",
      "iteration 7343train_loss: 0.4268742910731024 test_loss: 0.46043316337542156\n",
      "iteration 7344train_loss: 0.426874291055281 test_loss: 0.4604331956871725\n",
      "iteration 7345train_loss: 0.426874291037481 test_loss: 0.4604332279796356\n",
      "iteration 7346train_loss: 0.42687429101970226 test_loss: 0.46043326025282255\n",
      "iteration 7347train_loss: 0.42687429100194474 test_loss: 0.46043329250674486\n",
      "iteration 7348train_loss: 0.4268742909842084 test_loss: 0.460433324741414\n",
      "iteration 7349train_loss: 0.4268742909664933 test_loss: 0.46043335695684134\n",
      "iteration 7350train_loss: 0.42687429094879936 test_loss: 0.46043338915303866\n",
      "iteration 7351train_loss: 0.4268742909311265 test_loss: 0.46043342133001713\n",
      "iteration 7352train_loss: 0.4268742909134748 test_loss: 0.4604334534877884\n",
      "iteration 7353train_loss: 0.42687429089584417 test_loss: 0.4604334856263638\n",
      "iteration 7354train_loss: 0.4268742908782346 test_loss: 0.460433517745755\n",
      "iteration 7355train_loss: 0.42687429086064616 test_loss: 0.4604335498459732\n",
      "iteration 7356train_loss: 0.4268742908430786 test_loss: 0.46043358192703\n",
      "iteration 7357train_loss: 0.4268742908255321 test_loss: 0.4604336139889368\n",
      "iteration 7358train_loss: 0.4268742908080065 test_loss: 0.4604336460317049\n",
      "iteration 7359train_loss: 0.4268742907905018 test_loss: 0.4604336780553459\n",
      "iteration 7360train_loss: 0.42687429077301814 test_loss: 0.4604337100598712\n",
      "iteration 7361train_loss: 0.4268742907555553 test_loss: 0.4604337420452922\n",
      "iteration 7362train_loss: 0.4268742907381133 test_loss: 0.46043377401162017\n",
      "iteration 7363train_loss: 0.42687429072069216 test_loss: 0.46043380595886657\n",
      "iteration 7364train_loss: 0.4268742907032919 test_loss: 0.46043383788704295\n",
      "iteration 7365train_loss: 0.4268742906859123 test_loss: 0.46043386979616036\n",
      "iteration 7366train_loss: 0.42687429066855354 test_loss: 0.46043390168623044\n",
      "iteration 7367train_loss: 0.42687429065121546 test_loss: 0.4604339335572646\n",
      "iteration 7368train_loss: 0.4268742906338981 test_loss: 0.4604339654092739\n",
      "iteration 7369train_loss: 0.4268742906166015 test_loss: 0.4604339972422699\n",
      "iteration 7370train_loss: 0.42687429059932547 test_loss: 0.46043402905626407\n",
      "iteration 7371train_loss: 0.42687429058207016 test_loss: 0.46043406085126753\n",
      "iteration 7372train_loss: 0.4268742905648354 test_loss: 0.46043409262729157\n",
      "iteration 7373train_loss: 0.4268742905476212 test_loss: 0.4604341243843479\n",
      "iteration 7374train_loss: 0.4268742905304276 test_loss: 0.46043415612244737\n",
      "iteration 7375train_loss: 0.4268742905132546 test_loss: 0.4604341878416015\n",
      "iteration 7376train_loss: 0.426874290496102 test_loss: 0.4604342195418216\n",
      "iteration 7377train_loss: 0.42687429047896996 test_loss: 0.460434251223119\n",
      "iteration 7378train_loss: 0.42687429046185843 test_loss: 0.460434282885505\n",
      "iteration 7379train_loss: 0.42687429044476727 test_loss: 0.46043431452899064\n",
      "iteration 7380train_loss: 0.4268742904276965 test_loss: 0.4604343461535875\n",
      "iteration 7381train_loss: 0.4268742904106462 test_loss: 0.46043437775930696\n",
      "iteration 7382train_loss: 0.42687429039361624 test_loss: 0.46043440934615976\n",
      "iteration 7383train_loss: 0.4268742903766066 test_loss: 0.46043444091415775\n",
      "iteration 7384train_loss: 0.4268742903596173 test_loss: 0.4604344724633117\n",
      "iteration 7385train_loss: 0.4268742903426483 test_loss: 0.4604345039936332\n",
      "iteration 7386train_loss: 0.42687429032569957 test_loss: 0.46043453550513325\n",
      "iteration 7387train_loss: 0.426874290308771 test_loss: 0.4604345669978232\n",
      "iteration 7388train_loss: 0.4268742902918627 test_loss: 0.46043459847171436\n",
      "iteration 7389train_loss: 0.4268742902749747 test_loss: 0.4604346299268177\n",
      "iteration 7390train_loss: 0.4268742902581068 test_loss: 0.46043466136314465\n",
      "iteration 7391train_loss: 0.426874290241259 test_loss: 0.46043469278070637\n",
      "iteration 7392train_loss: 0.4268742902244314 test_loss: 0.46043472417951414\n",
      "iteration 7393train_loss: 0.42687429020762385 test_loss: 0.46043475555957886\n",
      "iteration 7394train_loss: 0.42687429019083645 test_loss: 0.4604347869209119\n",
      "iteration 7395train_loss: 0.4268742901740691 test_loss: 0.4604348182635245\n",
      "iteration 7396train_loss: 0.4268742901573217 test_loss: 0.4604348495874278\n",
      "iteration 7397train_loss: 0.4268742901405943 test_loss: 0.46043488089263285\n",
      "iteration 7398train_loss: 0.426874290123887 test_loss: 0.4604349121791511\n",
      "iteration 7399train_loss: 0.4268742901071995 test_loss: 0.46043494344699326\n",
      "iteration 7400train_loss: 0.42687429009053196 test_loss: 0.46043497469617084\n",
      "iteration 7401train_loss: 0.42687429007388444 test_loss: 0.46043500592669484\n",
      "iteration 7402train_loss: 0.42687429005725674 test_loss: 0.4604350371385764\n",
      "iteration 7403train_loss: 0.4268742900406489 test_loss: 0.4604350683318266\n",
      "iteration 7404train_loss: 0.4268742900240609 test_loss: 0.4604350995064567\n",
      "iteration 7405train_loss: 0.4268742900074928 test_loss: 0.46043513066247777\n",
      "iteration 7406train_loss: 0.4268742899909444 test_loss: 0.4604351617999007\n",
      "iteration 7407train_loss: 0.42687428997441573 test_loss: 0.4604351929187368\n",
      "iteration 7408train_loss: 0.4268742899579069 test_loss: 0.46043522401899717\n",
      "iteration 7409train_loss: 0.42687428994141774 test_loss: 0.4604352551006929\n",
      "iteration 7410train_loss: 0.42687428992494825 test_loss: 0.4604352861638349\n",
      "iteration 7411train_loss: 0.42687428990849846 test_loss: 0.4604353172084344\n",
      "iteration 7412train_loss: 0.4268742898920683 test_loss: 0.4604353482345023\n",
      "iteration 7413train_loss: 0.42687428987565784 test_loss: 0.46043537924204997\n",
      "iteration 7414train_loss: 0.42687428985926695 test_loss: 0.4604354102310882\n",
      "iteration 7415train_loss: 0.42687428984289555 test_loss: 0.4604354412016281\n",
      "iteration 7416train_loss: 0.42687428982654385 test_loss: 0.46043547215368075\n",
      "iteration 7417train_loss: 0.42687428981021164 test_loss: 0.4604355030872571\n",
      "iteration 7418train_loss: 0.42687428979389885 test_loss: 0.4604355340023684\n",
      "iteration 7419train_loss: 0.42687428977760555 test_loss: 0.4604355648990253\n",
      "iteration 7420train_loss: 0.42687428976133185 test_loss: 0.46043559577723914\n",
      "iteration 7421train_loss: 0.4268742897450775 test_loss: 0.4604356266370207\n",
      "iteration 7422train_loss: 0.42687428972884256 test_loss: 0.46043565747838106\n",
      "iteration 7423train_loss: 0.4268742897126271 test_loss: 0.4604356883013313\n",
      "iteration 7424train_loss: 0.4268742896964309 test_loss: 0.46043571910588227\n",
      "iteration 7425train_loss: 0.42687428968025415 test_loss: 0.4604357498920451\n",
      "iteration 7426train_loss: 0.4268742896640966 test_loss: 0.46043578065983065\n",
      "iteration 7427train_loss: 0.4268742896479584 test_loss: 0.46043581140925005\n",
      "iteration 7428train_loss: 0.42687428963183954 test_loss: 0.46043584214031397\n",
      "iteration 7429train_loss: 0.42687428961573987 test_loss: 0.4604358728530335\n",
      "iteration 7430train_loss: 0.4268742895996595 test_loss: 0.46043590354741976\n",
      "iteration 7431train_loss: 0.42687428958359824 test_loss: 0.46043593422348345\n",
      "iteration 7432train_loss: 0.4268742895675562 test_loss: 0.4604359648812356\n",
      "iteration 7433train_loss: 0.4268742895515333 test_loss: 0.4604359955206872\n",
      "iteration 7434train_loss: 0.42687428953552964 test_loss: 0.460436026141849\n",
      "iteration 7435train_loss: 0.4268742895195449 test_loss: 0.46043605674473215\n",
      "iteration 7436train_loss: 0.42687428950357953 test_loss: 0.4604360873293473\n",
      "iteration 7437train_loss: 0.42687428948763306 test_loss: 0.4604361178957056\n",
      "iteration 7438train_loss: 0.4268742894717056 test_loss: 0.4604361484438178\n",
      "iteration 7439train_loss: 0.42687428945579725 test_loss: 0.46043617897369477\n",
      "iteration 7440train_loss: 0.42687428943990785 test_loss: 0.46043620948534747\n",
      "iteration 7441train_loss: 0.4268742894240374 test_loss: 0.4604362399787867\n",
      "iteration 7442train_loss: 0.426874289408186 test_loss: 0.46043627045402336\n",
      "iteration 7443train_loss: 0.42687428939235356 test_loss: 0.46043630091106846\n",
      "iteration 7444train_loss: 0.42687428937654004 test_loss: 0.4604363313499326\n",
      "iteration 7445train_loss: 0.42687428936074534 test_loss: 0.4604363617706269\n",
      "iteration 7446train_loss: 0.4268742893449694 test_loss: 0.460436392173162\n",
      "iteration 7447train_loss: 0.42687428932921256 test_loss: 0.46043642255754885\n",
      "iteration 7448train_loss: 0.4268742893134743 test_loss: 0.4604364529237981\n",
      "iteration 7449train_loss: 0.4268742892977549 test_loss: 0.46043648327192077\n",
      "iteration 7450train_loss: 0.42687428928205434 test_loss: 0.46043651360192756\n",
      "iteration 7451train_loss: 0.42687428926637255 test_loss: 0.46043654391382943\n",
      "iteration 7452train_loss: 0.4268742892507094 test_loss: 0.460436574207637\n",
      "iteration 7453train_loss: 0.42687428923506504 test_loss: 0.46043660448336116\n",
      "iteration 7454train_loss: 0.4268742892194393 test_loss: 0.46043663474101276\n",
      "iteration 7455train_loss: 0.4268742892038323 test_loss: 0.46043666498060243\n",
      "iteration 7456train_loss: 0.4268742891882439 test_loss: 0.46043669520214114\n",
      "iteration 7457train_loss: 0.42687428917267417 test_loss: 0.46043672540563957\n",
      "iteration 7458train_loss: 0.426874289157123 test_loss: 0.46043675559110836\n",
      "iteration 7459train_loss: 0.42687428914159037 test_loss: 0.46043678575855845\n",
      "iteration 7460train_loss: 0.42687428912607633 test_loss: 0.4604368159080006\n",
      "iteration 7461train_loss: 0.4268742891105808 test_loss: 0.46043684603944546\n",
      "iteration 7462train_loss: 0.4268742890951038 test_loss: 0.46043687615290385\n",
      "iteration 7463train_loss: 0.4268742890796453 test_loss: 0.46043690624838635\n",
      "iteration 7464train_loss: 0.42687428906420527 test_loss: 0.4604369363259038\n",
      "iteration 7465train_loss: 0.4268742890487836 test_loss: 0.460436966385467\n",
      "iteration 7466train_loss: 0.4268742890333804 test_loss: 0.46043699642708646\n",
      "iteration 7467train_loss: 0.4268742890179956 test_loss: 0.4604370264507731\n",
      "iteration 7468train_loss: 0.42687428900262925 test_loss: 0.4604370564565376\n",
      "iteration 7469train_loss: 0.42687428898728125 test_loss: 0.46043708644439046\n",
      "iteration 7470train_loss: 0.42687428897195145 test_loss: 0.4604371164143426\n",
      "iteration 7471train_loss: 0.4268742889566401 test_loss: 0.4604371463664044\n",
      "iteration 7472train_loss: 0.4268742889413469 test_loss: 0.460437176300587\n",
      "iteration 7473train_loss: 0.4268742889260721 test_loss: 0.4604372062169007\n",
      "iteration 7474train_loss: 0.4268742889108155 test_loss: 0.46043723611535625\n",
      "iteration 7475train_loss: 0.426874288895577 test_loss: 0.4604372659959643\n",
      "iteration 7476train_loss: 0.42687428888035694 test_loss: 0.46043729585873555\n",
      "iteration 7477train_loss: 0.42687428886515494 test_loss: 0.4604373257036806\n",
      "iteration 7478train_loss: 0.4268742888499711 test_loss: 0.4604373555308102\n",
      "iteration 7479train_loss: 0.4268742888348054 test_loss: 0.46043738534013484\n",
      "iteration 7480train_loss: 0.4268742888196577 test_loss: 0.46043741513166525\n",
      "iteration 7481train_loss: 0.42687428880452827 test_loss: 0.46043744490541205\n",
      "iteration 7482train_loss: 0.42687428878941686 test_loss: 0.4604374746613857\n",
      "iteration 7483train_loss: 0.42687428877432354 test_loss: 0.460437504399597\n",
      "iteration 7484train_loss: 0.42687428875924815 test_loss: 0.4604375341200564\n",
      "iteration 7485train_loss: 0.42687428874419087 test_loss: 0.46043756382277445\n",
      "iteration 7486train_loss: 0.42687428872915145 test_loss: 0.460437593507762\n",
      "iteration 7487train_loss: 0.4268742887141301 test_loss: 0.46043762317502934\n",
      "iteration 7488train_loss: 0.4268742886991266 test_loss: 0.4604376528245873\n",
      "iteration 7489train_loss: 0.426874288684141 test_loss: 0.46043768245644634\n",
      "iteration 7490train_loss: 0.4268742886691735 test_loss: 0.460437712070617\n",
      "iteration 7491train_loss: 0.4268742886542238 test_loss: 0.46043774166710977\n",
      "iteration 7492train_loss: 0.42687428863929183 test_loss: 0.4604377712459353\n",
      "iteration 7493train_loss: 0.42687428862437776 test_loss: 0.46043780080710417\n",
      "iteration 7494train_loss: 0.4268742886094816 test_loss: 0.46043783035062685\n",
      "iteration 7495train_loss: 0.42687428859460314 test_loss: 0.46043785987651387\n",
      "iteration 7496train_loss: 0.42687428857974247 test_loss: 0.46043788938477576\n",
      "iteration 7497train_loss: 0.42687428856489956 test_loss: 0.46043791887542307\n",
      "iteration 7498train_loss: 0.4268742885500743 test_loss: 0.4604379483484663\n",
      "iteration 7499train_loss: 0.42687428853526693 test_loss: 0.46043797780391593\n",
      "iteration 7500train_loss: 0.42687428852047704 test_loss: 0.4604380072417824\n",
      "iteration 7501train_loss: 0.4268742885057051 test_loss: 0.4604380366620764\n",
      "iteration 7502train_loss: 0.4268742884909505 test_loss: 0.4604380660648082\n",
      "iteration 7503train_loss: 0.4268742884762136 test_loss: 0.4604380954499884\n",
      "iteration 7504train_loss: 0.4268742884614944 test_loss: 0.46043812481762747\n",
      "iteration 7505train_loss: 0.42687428844679276 test_loss: 0.4604381541677358\n",
      "iteration 7506train_loss: 0.4268742884321086 test_loss: 0.460438183500324\n",
      "iteration 7507train_loss: 0.4268742884174421 test_loss: 0.4604382128154025\n",
      "iteration 7508train_loss: 0.426874288402793 test_loss: 0.46043824211298157\n",
      "iteration 7509train_loss: 0.42687428838816144 test_loss: 0.4604382713930717\n",
      "iteration 7510train_loss: 0.4268742883735474 test_loss: 0.4604383006556835\n",
      "iteration 7511train_loss: 0.42687428835895075 test_loss: 0.46043832990082734\n",
      "iteration 7512train_loss: 0.4268742883443716 test_loss: 0.4604383591285136\n",
      "iteration 7513train_loss: 0.4268742883298098 test_loss: 0.46043838833875256\n",
      "iteration 7514train_loss: 0.42687428831526536 test_loss: 0.46043841753155496\n",
      "iteration 7515train_loss: 0.4268742883007385 test_loss: 0.4604384467069309\n",
      "iteration 7516train_loss: 0.4268742882862288 test_loss: 0.46043847586489095\n",
      "iteration 7517train_loss: 0.42687428827173646 test_loss: 0.4604385050054454\n",
      "iteration 7518train_loss: 0.4268742882572614 test_loss: 0.4604385341286048\n",
      "iteration 7519train_loss: 0.42687428824280377 test_loss: 0.4604385632343793\n",
      "iteration 7520train_loss: 0.42687428822836326 test_loss: 0.46043859232277945\n",
      "iteration 7521train_loss: 0.42687428821394013 test_loss: 0.46043862139381564\n",
      "iteration 7522train_loss: 0.42687428819953416 test_loss: 0.4604386504474981\n",
      "iteration 7523train_loss: 0.42687428818514533 test_loss: 0.46043867948383727\n",
      "iteration 7524train_loss: 0.4268742881707738 test_loss: 0.46043870850284346\n",
      "iteration 7525train_loss: 0.4268742881564194 test_loss: 0.460438737504527\n",
      "iteration 7526train_loss: 0.42687428814208217 test_loss: 0.46043876648889837\n",
      "iteration 7527train_loss: 0.426874288127762 test_loss: 0.4604387954559678\n",
      "iteration 7528train_loss: 0.426874288113459 test_loss: 0.4604388244057455\n",
      "iteration 7529train_loss: 0.426874288099173 test_loss: 0.46043885333824197\n",
      "iteration 7530train_loss: 0.4268742880849041 test_loss: 0.4604388822534674\n",
      "iteration 7531train_loss: 0.42687428807065225 test_loss: 0.4604389111514322\n",
      "iteration 7532train_loss: 0.4268742880564174 test_loss: 0.4604389400321466\n",
      "iteration 7533train_loss: 0.4268742880421996 test_loss: 0.46043896889562097\n",
      "iteration 7534train_loss: 0.4268742880279988 test_loss: 0.4604389977418655\n",
      "iteration 7535train_loss: 0.42687428801381494 test_loss: 0.4604390265708904\n",
      "iteration 7536train_loss: 0.426874287999648 test_loss: 0.46043905538270624\n",
      "iteration 7537train_loss: 0.426874287985498 test_loss: 0.46043908417732315\n",
      "iteration 7538train_loss: 0.4268742879713649 test_loss: 0.4604391129547512\n",
      "iteration 7539train_loss: 0.42687428795724874 test_loss: 0.46043914171500105\n",
      "iteration 7540train_loss: 0.42687428794314936 test_loss: 0.46043917045808247\n",
      "iteration 7541train_loss: 0.4268742879290668 test_loss: 0.460439199184006\n",
      "iteration 7542train_loss: 0.4268742879150012 test_loss: 0.4604392278927819\n",
      "iteration 7543train_loss: 0.4268742879009522 test_loss: 0.4604392565844203\n",
      "iteration 7544train_loss: 0.42687428788692017 test_loss: 0.46043928525893146\n",
      "iteration 7545train_loss: 0.42687428787290477 test_loss: 0.4604393139163256\n",
      "iteration 7546train_loss: 0.42687428785890613 test_loss: 0.4604393425566129\n",
      "iteration 7547train_loss: 0.4268742878449243 test_loss: 0.4604393711798036\n",
      "iteration 7548train_loss: 0.4268742878309591 test_loss: 0.4604393997859079\n",
      "iteration 7549train_loss: 0.42687428781701064 test_loss: 0.4604394283749361\n",
      "iteration 7550train_loss: 0.4268742878030788 test_loss: 0.4604394569468983\n",
      "iteration 7551train_loss: 0.4268742877891636 test_loss: 0.4604394855018046\n",
      "iteration 7552train_loss: 0.4268742877752649 test_loss: 0.4604395140396653\n",
      "iteration 7553train_loss: 0.42687428776138303 test_loss: 0.4604395425604906\n",
      "iteration 7554train_loss: 0.42687428774751757 test_loss: 0.46043957106429045\n",
      "iteration 7555train_loss: 0.4268742877336687 test_loss: 0.4604395995510753\n",
      "iteration 7556train_loss: 0.4268742877198365 test_loss: 0.4604396280208552\n",
      "iteration 7557train_loss: 0.4268742877060207 test_loss: 0.46043965647364016\n",
      "iteration 7558train_loss: 0.42687428769222135 test_loss: 0.4604396849094405\n",
      "iteration 7559train_loss: 0.4268742876784386 test_loss: 0.4604397133282662\n",
      "iteration 7560train_loss: 0.4268742876646723 test_loss: 0.4604397417301275\n",
      "iteration 7561train_loss: 0.4268742876509224 test_loss: 0.4604397701150346\n",
      "iteration 7562train_loss: 0.4268742876371889 test_loss: 0.4604397984829974\n",
      "iteration 7563train_loss: 0.4268742876234719 test_loss: 0.4604398268340261\n",
      "iteration 7564train_loss: 0.4268742876097712 test_loss: 0.460439855168131\n",
      "iteration 7565train_loss: 0.4268742875960869 test_loss: 0.460439883485322\n",
      "iteration 7566train_loss: 0.42687428758241897 test_loss: 0.4604399117856091\n",
      "iteration 7567train_loss: 0.42687428756876733 test_loss: 0.46043994006900263\n",
      "iteration 7568train_loss: 0.426874287555132 test_loss: 0.4604399683355126\n",
      "iteration 7569train_loss: 0.4268742875415129 test_loss: 0.460439996585149\n",
      "iteration 7570train_loss: 0.4268742875279101 test_loss: 0.46044002481792207\n",
      "iteration 7571train_loss: 0.4268742875143236 test_loss: 0.4604400530338415\n",
      "iteration 7572train_loss: 0.4268742875007534 test_loss: 0.4604400812329178\n",
      "iteration 7573train_loss: 0.4268742874871993 test_loss: 0.4604401094151607\n",
      "iteration 7574train_loss: 0.4268742874736614 test_loss: 0.46044013758058044\n",
      "iteration 7575train_loss: 0.4268742874601396 test_loss: 0.46044016572918706\n",
      "iteration 7576train_loss: 0.4268742874466341 test_loss: 0.4604401938609904\n",
      "iteration 7577train_loss: 0.42687428743314465 test_loss: 0.4604402219760008\n",
      "iteration 7578train_loss: 0.4268742874196713 test_loss: 0.46044025007422784\n",
      "iteration 7579train_loss: 0.4268742874062141 test_loss: 0.46044027815568195\n",
      "iteration 7580train_loss: 0.4268742873927729 test_loss: 0.460440306220373\n",
      "iteration 7581train_loss: 0.4268742873793478 test_loss: 0.460440334268311\n",
      "iteration 7582train_loss: 0.4268742873659387 test_loss: 0.46044036229950586\n",
      "iteration 7583train_loss: 0.4268742873525457 test_loss: 0.46044039031396766\n",
      "iteration 7584train_loss: 0.4268742873391686 test_loss: 0.4604404183117063\n",
      "iteration 7585train_loss: 0.4268742873258075 test_loss: 0.46044044629273195\n",
      "iteration 7586train_loss: 0.4268742873124624 test_loss: 0.4604404742570544\n",
      "iteration 7587train_loss: 0.42687428729913324 test_loss: 0.46044050220468374\n",
      "iteration 7588train_loss: 0.42687428728581994 test_loss: 0.46044053013562974\n",
      "iteration 7589train_loss: 0.4268742872725226 test_loss: 0.4604405580499026\n",
      "iteration 7590train_loss: 0.4268742872592411 test_loss: 0.46044058594751214\n",
      "iteration 7591train_loss: 0.42687428724597554 test_loss: 0.4604406138284683\n",
      "iteration 7592train_loss: 0.4268742872327258 test_loss: 0.460440641692781\n",
      "iteration 7593train_loss: 0.4268742872194918 test_loss: 0.46044066954046026\n",
      "iteration 7594train_loss: 0.42687428720627363 test_loss: 0.4604406973715161\n",
      "iteration 7595train_loss: 0.42687428719307136 test_loss: 0.46044072518595813\n",
      "iteration 7596train_loss: 0.4268742871798848 test_loss: 0.4604407529837965\n",
      "iteration 7597train_loss: 0.4268742871667139 test_loss: 0.460440780765041\n",
      "iteration 7598train_loss: 0.4268742871535588 test_loss: 0.4604408085297017\n",
      "iteration 7599train_loss: 0.4268742871404195 test_loss: 0.46044083627778837\n",
      "iteration 7600train_loss: 0.42687428712729575 test_loss: 0.4604408640093109\n",
      "iteration 7601train_loss: 0.42687428711418784 test_loss: 0.4604408917242792\n",
      "iteration 7602train_loss: 0.42687428710109543 test_loss: 0.4604409194227031\n",
      "iteration 7603train_loss: 0.4268742870880187 test_loss: 0.4604409471045926\n",
      "iteration 7604train_loss: 0.42687428707495767 test_loss: 0.46044097476995755\n",
      "iteration 7605train_loss: 0.42687428706191216 test_loss: 0.46044100241880764\n",
      "iteration 7606train_loss: 0.42687428704888214 test_loss: 0.46044103005115283\n",
      "iteration 7607train_loss: 0.4268742870358678 test_loss: 0.4604410576670031\n",
      "iteration 7608train_loss: 0.426874287022869 test_loss: 0.4604410852663681\n",
      "iteration 7609train_loss: 0.42687428700988583 test_loss: 0.4604411128492577\n",
      "iteration 7610train_loss: 0.42687428699691804 test_loss: 0.46044114041568185\n",
      "iteration 7611train_loss: 0.4268742869839657 test_loss: 0.4604411679656502\n",
      "iteration 7612train_loss: 0.4268742869710289 test_loss: 0.46044119549917284\n",
      "iteration 7613train_loss: 0.4268742869581076 test_loss: 0.4604412230162593\n",
      "iteration 7614train_loss: 0.42687428694520163 test_loss: 0.46044125051691953\n",
      "iteration 7615train_loss: 0.42687428693231116 test_loss: 0.46044127800116336\n",
      "iteration 7616train_loss: 0.4268742869194361 test_loss: 0.4604413054690005\n",
      "iteration 7617train_loss: 0.4268742869065763 test_loss: 0.46044133292044076\n",
      "iteration 7618train_loss: 0.4268742868937319 test_loss: 0.460441360355494\n",
      "iteration 7619train_loss: 0.4268742868809029 test_loss: 0.4604413877741699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7620train_loss: 0.4268742868680892 test_loss: 0.4604414151764783\n",
      "iteration 7621train_loss: 0.4268742868552908 test_loss: 0.46044144256242897\n",
      "iteration 7622train_loss: 0.42687428684250767 test_loss: 0.4604414699320316\n",
      "iteration 7623train_loss: 0.4268742868297398 test_loss: 0.460441497285296\n",
      "iteration 7624train_loss: 0.42687428681698725 test_loss: 0.46044152462223203\n",
      "iteration 7625train_loss: 0.4268742868042499 test_loss: 0.46044155194284925\n",
      "iteration 7626train_loss: 0.4268742867915278 test_loss: 0.4604415792471575\n",
      "iteration 7627train_loss: 0.4268742867788208 test_loss: 0.4604416065351664\n",
      "iteration 7628train_loss: 0.426874286766129 test_loss: 0.46044163380688585\n",
      "iteration 7629train_loss: 0.4268742867534524 test_loss: 0.4604416610623255\n",
      "iteration 7630train_loss: 0.426874286740791 test_loss: 0.46044168830149507\n",
      "iteration 7631train_loss: 0.42687428672814454 test_loss: 0.4604417155244042\n",
      "iteration 7632train_loss: 0.42687428671551325 test_loss: 0.4604417427310627\n",
      "iteration 7633train_loss: 0.42687428670289723 test_loss: 0.46044176992148006\n",
      "iteration 7634train_loss: 0.42687428669029603 test_loss: 0.46044179709566635\n",
      "iteration 7635train_loss: 0.42687428667771005 test_loss: 0.46044182425363095\n",
      "iteration 7636train_loss: 0.42687428666513905 test_loss: 0.4604418513953836\n",
      "iteration 7637train_loss: 0.42687428665258303 test_loss: 0.4604418785209341\n",
      "iteration 7638train_loss: 0.4268742866400421 test_loss: 0.46044190563029197\n",
      "iteration 7639train_loss: 0.42687428662751603 test_loss: 0.4604419327234669\n",
      "iteration 7640train_loss: 0.426874286615005 test_loss: 0.46044195980046854\n",
      "iteration 7641train_loss: 0.4268742866025089 test_loss: 0.4604419868613067\n",
      "iteration 7642train_loss: 0.42687428659002774 test_loss: 0.4604420139059909\n",
      "iteration 7643train_loss: 0.4268742865775615 test_loss: 0.46044204093453067\n",
      "iteration 7644train_loss: 0.4268742865651102 test_loss: 0.46044206794693604\n",
      "iteration 7645train_loss: 0.42687428655267357 test_loss: 0.4604420949432161\n",
      "iteration 7646train_loss: 0.4268742865402519 test_loss: 0.46044212192338074\n",
      "iteration 7647train_loss: 0.42687428652784515 test_loss: 0.4604421488874397\n",
      "iteration 7648train_loss: 0.4268742865154531 test_loss: 0.46044217583540237\n",
      "iteration 7649train_loss: 0.4268742865030759 test_loss: 0.46044220276727843\n",
      "iteration 7650train_loss: 0.4268742864907134 test_loss: 0.46044222968307763\n",
      "iteration 7651train_loss: 0.42687428647836584 test_loss: 0.4604422565828094\n",
      "iteration 7652train_loss: 0.426874286466033 test_loss: 0.46044228346648336\n",
      "iteration 7653train_loss: 0.4268742864537148 test_loss: 0.4604423103341091\n",
      "iteration 7654train_loss: 0.4268742864414113 test_loss: 0.46044233718569616\n",
      "iteration 7655train_loss: 0.42687428642912256 test_loss: 0.4604423640212542\n",
      "iteration 7656train_loss: 0.4268742864168485 test_loss: 0.4604423908407927\n",
      "iteration 7657train_loss: 0.42687428640458897 test_loss: 0.4604424176443213\n",
      "iteration 7658train_loss: 0.4268742863923443 test_loss: 0.4604424444318496\n",
      "iteration 7659train_loss: 0.4268742863801141 test_loss: 0.4604424712033868\n",
      "iteration 7660train_loss: 0.4268742863678986 test_loss: 0.46044249795894304\n",
      "iteration 7661train_loss: 0.42687428635569763 test_loss: 0.4604425246985273\n",
      "iteration 7662train_loss: 0.42687428634351127 test_loss: 0.46044255142214946\n",
      "iteration 7663train_loss: 0.42687428633133934 test_loss: 0.4604425781298189\n",
      "iteration 7664train_loss: 0.4268742863191822 test_loss: 0.46044260482154514\n",
      "iteration 7665train_loss: 0.4268742863070394 test_loss: 0.46044263149733783\n",
      "iteration 7666train_loss: 0.42687428629491114 test_loss: 0.4604426581572063\n",
      "iteration 7667train_loss: 0.4268742862827973 test_loss: 0.4604426848011601\n",
      "iteration 7668train_loss: 0.4268742862706979 test_loss: 0.46044271142920873\n",
      "iteration 7669train_loss: 0.4268742862586132 test_loss: 0.46044273804136177\n",
      "iteration 7670train_loss: 0.4268742862465428 test_loss: 0.4604427646376286\n",
      "iteration 7671train_loss: 0.42687428623448676 test_loss: 0.4604427912180187\n",
      "iteration 7672train_loss: 0.42687428622244505 test_loss: 0.46044281778254176\n",
      "iteration 7673train_loss: 0.4268742862104179 test_loss: 0.46044284433120697\n",
      "iteration 7674train_loss: 0.42687428619840506 test_loss: 0.46044287086402397\n",
      "iteration 7675train_loss: 0.42687428618640655 test_loss: 0.4604428973810021\n",
      "iteration 7676train_loss: 0.4268742861744223 test_loss: 0.4604429238821509\n",
      "iteration 7677train_loss: 0.42687428616245254 test_loss: 0.4604429503674798\n",
      "iteration 7678train_loss: 0.4268742861504969 test_loss: 0.46044297683699814\n",
      "iteration 7679train_loss: 0.42687428613855566 test_loss: 0.4604430032907155\n",
      "iteration 7680train_loss: 0.4268742861266286 test_loss: 0.46044302972864126\n",
      "iteration 7681train_loss: 0.42687428611471584 test_loss: 0.46044305615078485\n",
      "iteration 7682train_loss: 0.4268742861028173 test_loss: 0.4604430825571556\n",
      "iteration 7683train_loss: 0.4268742860909329 test_loss: 0.46044310894776314\n",
      "iteration 7684train_loss: 0.42687428607906286 test_loss: 0.4604431353226166\n",
      "iteration 7685train_loss: 0.4268742860672069 test_loss: 0.4604431616817256\n",
      "iteration 7686train_loss: 0.42687428605536504 test_loss: 0.4604431880250994\n",
      "iteration 7687train_loss: 0.4268742860435374 test_loss: 0.4604432143527476\n",
      "iteration 7688train_loss: 0.4268742860317239 test_loss: 0.46044324066467923\n",
      "iteration 7689train_loss: 0.42687428601992444 test_loss: 0.46044326696090404\n",
      "iteration 7690train_loss: 0.4268742860081392 test_loss: 0.46044329324143113\n",
      "iteration 7691train_loss: 0.42687428599636795 test_loss: 0.46044331950627015\n",
      "iteration 7692train_loss: 0.42687428598461075 test_loss: 0.4604433457554301\n",
      "iteration 7693train_loss: 0.42687428597286753 test_loss: 0.4604433719889206\n",
      "iteration 7694train_loss: 0.4268742859611384 test_loss: 0.46044339820675095\n",
      "iteration 7695train_loss: 0.4268742859494233 test_loss: 0.46044342440893055\n",
      "iteration 7696train_loss: 0.42687428593772214 test_loss: 0.46044345059546854\n",
      "iteration 7697train_loss: 0.426874285926035 test_loss: 0.4604434767663744\n",
      "iteration 7698train_loss: 0.4268742859143619 test_loss: 0.4604435029216576\n",
      "iteration 7699train_loss: 0.42687428590270265 test_loss: 0.4604435290613272\n",
      "iteration 7700train_loss: 0.42687428589105725 test_loss: 0.4604435551853926\n",
      "iteration 7701train_loss: 0.4268742858794259 test_loss: 0.46044358129386315\n",
      "iteration 7702train_loss: 0.4268742858678084 test_loss: 0.4604436073867482\n",
      "iteration 7703train_loss: 0.42687428585620474 test_loss: 0.4604436334640569\n",
      "iteration 7704train_loss: 0.426874285844615 test_loss: 0.4604436595257988\n",
      "iteration 7705train_loss: 0.426874285833039 test_loss: 0.4604436855719829\n",
      "iteration 7706train_loss: 0.4268742858214769 test_loss: 0.4604437116026187\n",
      "iteration 7707train_loss: 0.42687428580992864 test_loss: 0.46044373761771534\n",
      "iteration 7708train_loss: 0.4268742857983941 test_loss: 0.46044376361728223\n",
      "iteration 7709train_loss: 0.42687428578687336 test_loss: 0.4604437896013285\n",
      "iteration 7710train_loss: 0.4268742857753664 test_loss: 0.4604438155698635\n",
      "iteration 7711train_loss: 0.42687428576387315 test_loss: 0.4604438415228965\n",
      "iteration 7712train_loss: 0.4268742857523937 test_loss: 0.4604438674604368\n",
      "iteration 7713train_loss: 0.426874285740928 test_loss: 0.4604438933824934\n",
      "iteration 7714train_loss: 0.4268742857294758 test_loss: 0.46044391928907585\n",
      "iteration 7715train_loss: 0.42687428571803737 test_loss: 0.46044394518019316\n",
      "iteration 7716train_loss: 0.4268742857066127 test_loss: 0.4604439710558546\n",
      "iteration 7717train_loss: 0.42687428569520164 test_loss: 0.4604439969160695\n",
      "iteration 7718train_loss: 0.42687428568380414 test_loss: 0.460444022760847\n",
      "iteration 7719train_loss: 0.4268742856724202 test_loss: 0.46044404859019644\n",
      "iteration 7720train_loss: 0.42687428566104996 test_loss: 0.46044407440412666\n",
      "iteration 7721train_loss: 0.42687428564969326 test_loss: 0.4604441002026474\n",
      "iteration 7722train_loss: 0.4268742856383502 test_loss: 0.46044412598576734\n",
      "iteration 7723train_loss: 0.42687428562702057 test_loss: 0.4604441517534961\n",
      "iteration 7724train_loss: 0.4268742856157045 test_loss: 0.4604441775058426\n",
      "iteration 7725train_loss: 0.426874285604402 test_loss: 0.46044420324281604\n",
      "iteration 7726train_loss: 0.4268742855931131 test_loss: 0.46044422896442583\n",
      "iteration 7727train_loss: 0.42687428558183754 test_loss: 0.4604442546706808\n",
      "iteration 7728train_loss: 0.42687428557057544 test_loss: 0.46044428036159035\n",
      "iteration 7729train_loss: 0.4268742855593268 test_loss: 0.46044430603716346\n",
      "iteration 7730train_loss: 0.4268742855480916 test_loss: 0.46044433169740956\n",
      "iteration 7731train_loss: 0.42687428553686985 test_loss: 0.4604443573423375\n",
      "iteration 7732train_loss: 0.4268742855256615 test_loss: 0.4604443829719565\n",
      "iteration 7733train_loss: 0.4268742855144665 test_loss: 0.46044440858627594\n",
      "iteration 7734train_loss: 0.4268742855032849 test_loss: 0.4604444341853046\n",
      "iteration 7735train_loss: 0.42687428549211665 test_loss: 0.4604444597690518\n",
      "iteration 7736train_loss: 0.42687428548096173 test_loss: 0.4604444853375267\n",
      "iteration 7737train_loss: 0.42687428546982015 test_loss: 0.46044451089073823\n",
      "iteration 7738train_loss: 0.4268742854586919 test_loss: 0.4604445364286956\n",
      "iteration 7739train_loss: 0.42687428544757694 test_loss: 0.46044456195140804\n",
      "iteration 7740train_loss: 0.4268742854364752 test_loss: 0.46044458745888445\n",
      "iteration 7741train_loss: 0.42687428542538663 test_loss: 0.460444612951134\n",
      "iteration 7742train_loss: 0.4268742854143115 test_loss: 0.46044463842816574\n",
      "iteration 7743train_loss: 0.42687428540324956 test_loss: 0.4604446638899889\n",
      "iteration 7744train_loss: 0.4268742853922008 test_loss: 0.46044468933661237\n",
      "iteration 7745train_loss: 0.42687428538116523 test_loss: 0.46044471476804527\n",
      "iteration 7746train_loss: 0.42687428537014277 test_loss: 0.46044474018429676\n",
      "iteration 7747train_loss: 0.4268742853591337 test_loss: 0.460444765585376\n",
      "iteration 7748train_loss: 0.42687428534813754 test_loss: 0.46044479097129165\n",
      "iteration 7749train_loss: 0.42687428533715455 test_loss: 0.4604448163420531\n",
      "iteration 7750train_loss: 0.42687428532618477 test_loss: 0.46044484169766936\n",
      "iteration 7751train_loss: 0.42687428531522803 test_loss: 0.4604448670381493\n",
      "iteration 7752train_loss: 0.42687428530428445 test_loss: 0.460444892363502\n",
      "iteration 7753train_loss: 0.4268742852933538 test_loss: 0.4604449176737366\n",
      "iteration 7754train_loss: 0.4268742852824363 test_loss: 0.4604449429688621\n",
      "iteration 7755train_loss: 0.4268742852715319 test_loss: 0.4604449682488875\n",
      "iteration 7756train_loss: 0.42687428526064036 test_loss: 0.4604449935138218\n",
      "iteration 7757train_loss: 0.426874285249762 test_loss: 0.46044501876367383\n",
      "iteration 7758train_loss: 0.4268742852388964 test_loss: 0.460445043998453\n",
      "iteration 7759train_loss: 0.4268742852280441 test_loss: 0.46044506921816797\n",
      "iteration 7760train_loss: 0.4268742852172045 test_loss: 0.46044509442282777\n",
      "iteration 7761train_loss: 0.426874285206378 test_loss: 0.46044511961244156\n",
      "iteration 7762train_loss: 0.4268742851955644 test_loss: 0.46044514478701815\n",
      "iteration 7763train_loss: 0.4268742851847636 test_loss: 0.46044516994656665\n",
      "iteration 7764train_loss: 0.4268742851739758 test_loss: 0.4604451950910959\n",
      "iteration 7765train_loss: 0.426874285163201 test_loss: 0.4604452202206149\n",
      "iteration 7766train_loss: 0.42687428515243897 test_loss: 0.4604452453351326\n",
      "iteration 7767train_loss: 0.4268742851416896 test_loss: 0.46044527043465805\n",
      "iteration 7768train_loss: 0.42687428513095327 test_loss: 0.46044529551920005\n",
      "iteration 7769train_loss: 0.42687428512022974 test_loss: 0.46044532058876764\n",
      "iteration 7770train_loss: 0.42687428510951897 test_loss: 0.4604453456433698\n",
      "iteration 7771train_loss: 0.42687428509882114 test_loss: 0.4604453706830154\n",
      "iteration 7772train_loss: 0.4268742850881359 test_loss: 0.4604453957077133\n",
      "iteration 7773train_loss: 0.4268742850774635 test_loss: 0.4604454207174726\n",
      "iteration 7774train_loss: 0.42687428506680386 test_loss: 0.4604454457123019\n",
      "iteration 7775train_loss: 0.426874285056157 test_loss: 0.4604454706922104\n",
      "iteration 7776train_loss: 0.42687428504552277 test_loss: 0.46044549565720694\n",
      "iteration 7777train_loss: 0.42687428503490127 test_loss: 0.4604455206073005\n",
      "iteration 7778train_loss: 0.4268742850242925 test_loss: 0.4604455455424997\n",
      "iteration 7779train_loss: 0.42687428501369634 test_loss: 0.46044557046281376\n",
      "iteration 7780train_loss: 0.42687428500311286 test_loss: 0.4604455953682513\n",
      "iteration 7781train_loss: 0.426874284992542 test_loss: 0.4604456202588214\n",
      "iteration 7782train_loss: 0.4268742849819837 test_loss: 0.46044564513453273\n",
      "iteration 7783train_loss: 0.4268742849714382 test_loss: 0.4604456699953943\n",
      "iteration 7784train_loss: 0.4268742849609051 test_loss: 0.46044569484141507\n",
      "iteration 7785train_loss: 0.4268742849503847 test_loss: 0.4604457196726036\n",
      "iteration 7786train_loss: 0.4268742849398769 test_loss: 0.460445744488969\n",
      "iteration 7787train_loss: 0.42687428492938156 test_loss: 0.46044576929052\n",
      "iteration 7788train_loss: 0.4268742849188987 test_loss: 0.4604457940772655\n",
      "iteration 7789train_loss: 0.42687428490842844 test_loss: 0.46044581884921426\n",
      "iteration 7790train_loss: 0.4268742848979707 test_loss: 0.4604458436063752\n",
      "iteration 7791train_loss: 0.42687428488752543 test_loss: 0.46044586834875706\n",
      "iteration 7792train_loss: 0.4268742848770926 test_loss: 0.4604458930763688\n",
      "iteration 7793train_loss: 0.4268742848666722 test_loss: 0.460445917789219\n",
      "iteration 7794train_loss: 0.4268742848562644 test_loss: 0.4604459424873168\n",
      "iteration 7795train_loss: 0.4268742848458689 test_loss: 0.46044596717067066\n",
      "iteration 7796train_loss: 0.4268742848354859 test_loss: 0.4604459918392896\n",
      "iteration 7797train_loss: 0.4268742848251153 test_loss: 0.46044601649318234\n",
      "iteration 7798train_loss: 0.42687428481475703 test_loss: 0.46044604113235776\n",
      "iteration 7799train_loss: 0.42687428480441114 test_loss: 0.4604460657568245\n",
      "iteration 7800train_loss: 0.42687428479407763 test_loss: 0.46044609036659134\n",
      "iteration 7801train_loss: 0.4268742847837564 test_loss: 0.4604461149616672\n",
      "iteration 7802train_loss: 0.4268742847734476 test_loss: 0.46044613954206065\n",
      "iteration 7803train_loss: 0.42687428476315104 test_loss: 0.4604461641077808\n",
      "iteration 7804train_loss: 0.42687428475286676 test_loss: 0.46044618865883596\n",
      "iteration 7805train_loss: 0.4268742847425949 test_loss: 0.4604462131952351\n",
      "iteration 7806train_loss: 0.4268742847323352 test_loss: 0.4604462377169871\n",
      "iteration 7807train_loss: 0.42687428472208777 test_loss: 0.46044626222410057\n",
      "iteration 7808train_loss: 0.42687428471185246 test_loss: 0.46044628671658416\n",
      "iteration 7809train_loss: 0.4268742847016296 test_loss: 0.46044631119444673\n",
      "iteration 7810train_loss: 0.42687428469141875 test_loss: 0.460446335657697\n",
      "iteration 7811train_loss: 0.4268742846812202 test_loss: 0.46044636010634354\n",
      "iteration 7812train_loss: 0.4268742846710339 test_loss: 0.4604463845403952\n",
      "iteration 7813train_loss: 0.42687428466085964 test_loss: 0.4604464089598607\n",
      "iteration 7814train_loss: 0.42687428465069754 test_loss: 0.4604464333647488\n",
      "iteration 7815train_loss: 0.42687428464054766 test_loss: 0.46044645775506804\n",
      "iteration 7816train_loss: 0.42687428463040983 test_loss: 0.46044648213082723\n",
      "iteration 7817train_loss: 0.4268742846202841 test_loss: 0.460446506492035\n",
      "iteration 7818train_loss: 0.42687428461017046 test_loss: 0.4604465308387001\n",
      "iteration 7819train_loss: 0.42687428460006893 test_loss: 0.4604465551708312\n",
      "iteration 7820train_loss: 0.42687428458997945 test_loss: 0.4604465794884368\n",
      "iteration 7821train_loss: 0.4268742845799021 test_loss: 0.46044660379152585\n",
      "iteration 7822train_loss: 0.4268742845698367 test_loss: 0.46044662808010683\n",
      "iteration 7823train_loss: 0.4268742845597834 test_loss: 0.46044665235418847\n",
      "iteration 7824train_loss: 0.42687428454974197 test_loss: 0.4604466766137793\n",
      "iteration 7825train_loss: 0.4268742845397126 test_loss: 0.46044670085888817\n",
      "iteration 7826train_loss: 0.42687428452969534 test_loss: 0.46044672508952356\n",
      "iteration 7827train_loss: 0.4268742845196899 test_loss: 0.46044674930569424\n",
      "iteration 7828train_loss: 0.42687428450969644 test_loss: 0.4604467735074088\n",
      "iteration 7829train_loss: 0.42687428449971493 test_loss: 0.46044679769467584\n",
      "iteration 7830train_loss: 0.42687428448974524 test_loss: 0.46044682186750385\n",
      "iteration 7831train_loss: 0.42687428447978754 test_loss: 0.46044684602590163\n",
      "iteration 7832train_loss: 0.42687428446984177 test_loss: 0.46044687016987784\n",
      "iteration 7833train_loss: 0.4268742844599078 test_loss: 0.46044689429944097\n",
      "iteration 7834train_loss: 0.42687428444998576 test_loss: 0.4604469184145996\n",
      "iteration 7835train_loss: 0.4268742844400756 test_loss: 0.46044694251536245\n",
      "iteration 7836train_loss: 0.42687428443017716 test_loss: 0.460446966601738\n",
      "iteration 7837train_loss: 0.42687428442029074 test_loss: 0.46044699067373485\n",
      "iteration 7838train_loss: 0.4268742844104159 test_loss: 0.46044701473136157\n",
      "iteration 7839train_loss: 0.42687428440055303 test_loss: 0.4604470387746269\n",
      "iteration 7840train_loss: 0.42687428439070185 test_loss: 0.4604470628035392\n",
      "iteration 7841train_loss: 0.42687428438086245 test_loss: 0.4604470868181071\n",
      "iteration 7842train_loss: 0.42687428437103475 test_loss: 0.4604471108183392\n",
      "iteration 7843train_loss: 0.4268742843612188 test_loss: 0.460447134804244\n",
      "iteration 7844train_loss: 0.42687428435141467 test_loss: 0.4604471587758302\n",
      "iteration 7845train_loss: 0.4268742843416223 test_loss: 0.4604471827331061\n",
      "iteration 7846train_loss: 0.4268742843318413 test_loss: 0.4604472066760806\n",
      "iteration 7847train_loss: 0.4268742843220723 test_loss: 0.46044723060476184\n",
      "iteration 7848train_loss: 0.4268742843123149 test_loss: 0.46044725451915863\n",
      "iteration 7849train_loss: 0.4268742843025691 test_loss: 0.46044727841927924\n",
      "iteration 7850train_loss: 0.4268742842928349 test_loss: 0.4604473023051325\n",
      "iteration 7851train_loss: 0.4268742842831124 test_loss: 0.4604473261767266\n",
      "iteration 7852train_loss: 0.42687428427340146 test_loss: 0.4604473500340703\n",
      "iteration 7853train_loss: 0.4268742842637022 test_loss: 0.46044737387717216\n",
      "iteration 7854train_loss: 0.4268742842540146 test_loss: 0.46044739770604026\n",
      "iteration 7855train_loss: 0.4268742842443383 test_loss: 0.46044742152068363\n",
      "iteration 7856train_loss: 0.4268742842346738 test_loss: 0.46044744532111037\n",
      "iteration 7857train_loss: 0.4268742842250207 test_loss: 0.46044746910732914\n",
      "iteration 7858train_loss: 0.4268742842153792 test_loss: 0.46044749287934844\n",
      "iteration 7859train_loss: 0.42687428420574913 test_loss: 0.46044751663717665\n",
      "iteration 7860train_loss: 0.42687428419613066 test_loss: 0.4604475403808222\n",
      "iteration 7861train_loss: 0.4268742841865237 test_loss: 0.46044756411029375\n",
      "iteration 7862train_loss: 0.4268742841769282 test_loss: 0.4604475878255996\n",
      "iteration 7863train_loss: 0.42687428416734396 test_loss: 0.4604476115267483\n",
      "iteration 7864train_loss: 0.42687428415777146 test_loss: 0.46044763521374815\n",
      "iteration 7865train_loss: 0.42687428414821027 test_loss: 0.46044765888660777\n",
      "iteration 7866train_loss: 0.4268742841386604 test_loss: 0.46044768254533547\n",
      "iteration 7867train_loss: 0.4268742841291221 test_loss: 0.4604477061899398\n",
      "iteration 7868train_loss: 0.42687428411959505 test_loss: 0.4604477298204291\n",
      "iteration 7869train_loss: 0.4268742841100795 test_loss: 0.4604477534368118\n",
      "iteration 7870train_loss: 0.4268742841005753 test_loss: 0.4604477770390964\n",
      "iteration 7871train_loss: 0.4268742840910823 test_loss: 0.46044780062729124\n",
      "iteration 7872train_loss: 0.42687428408160083 test_loss: 0.4604478242014047\n",
      "iteration 7873train_loss: 0.4268742840721306 test_loss: 0.46044784776144526\n",
      "iteration 7874train_loss: 0.42687428406267164 test_loss: 0.4604478713074213\n",
      "iteration 7875train_loss: 0.42687428405322403 test_loss: 0.4604478948393411\n",
      "iteration 7876train_loss: 0.42687428404378763 test_loss: 0.46044791835721327\n",
      "iteration 7877train_loss: 0.42687428403436267 test_loss: 0.460447941861046\n",
      "iteration 7878train_loss: 0.42687428402494887 test_loss: 0.4604479653508479\n",
      "iteration 7879train_loss: 0.42687428401554633 test_loss: 0.4604479888266271\n",
      "iteration 7880train_loss: 0.426874284006155 test_loss: 0.4604480122883921\n",
      "iteration 7881train_loss: 0.42687428399677485 test_loss: 0.4604480357361512\n",
      "iteration 7882train_loss: 0.4268742839874059 test_loss: 0.4604480591699127\n",
      "iteration 7883train_loss: 0.42687428397804816 test_loss: 0.46044808258968517\n",
      "iteration 7884train_loss: 0.4268742839687016 test_loss: 0.4604481059954768\n",
      "iteration 7885train_loss: 0.4268742839593662 test_loss: 0.4604481293872959\n",
      "iteration 7886train_loss: 0.42687428395004207 test_loss: 0.46044815276515094\n",
      "iteration 7887train_loss: 0.4268742839407289 test_loss: 0.4604481761290502\n",
      "iteration 7888train_loss: 0.426874283931427 test_loss: 0.460448199479002\n",
      "iteration 7889train_loss: 0.42687428392213606 test_loss: 0.4604482228150146\n",
      "iteration 7890train_loss: 0.42687428391285626 test_loss: 0.4604482461370964\n",
      "iteration 7891train_loss: 0.42687428390358767 test_loss: 0.46044826944525574\n",
      "iteration 7892train_loss: 0.42687428389432996 test_loss: 0.4604482927395009\n",
      "iteration 7893train_loss: 0.42687428388508347 test_loss: 0.46044831601984015\n",
      "iteration 7894train_loss: 0.4268742838758479 test_loss: 0.4604483392862818\n",
      "iteration 7895train_loss: 0.4268742838666234 test_loss: 0.46044836253883414\n",
      "iteration 7896train_loss: 0.42687428385740994 test_loss: 0.46044838577750546\n",
      "iteration 7897train_loss: 0.4268742838482075 test_loss: 0.46044840900230416\n",
      "iteration 7898train_loss: 0.42687428383901604 test_loss: 0.4604484322132384\n",
      "iteration 7899train_loss: 0.4268742838298356 test_loss: 0.46044845541031637\n",
      "iteration 7900train_loss: 0.4268742838206661 test_loss: 0.4604484785935466\n",
      "iteration 7901train_loss: 0.42687428381150744 test_loss: 0.4604485017629371\n",
      "iteration 7902train_loss: 0.42687428380235976 test_loss: 0.4604485249184963\n",
      "iteration 7903train_loss: 0.42687428379322306 test_loss: 0.4604485480602323\n",
      "iteration 7904train_loss: 0.42687428378409725 test_loss: 0.4604485711881535\n",
      "iteration 7905train_loss: 0.42687428377498243 test_loss: 0.46044859430226814\n",
      "iteration 7906train_loss: 0.4268742837658783 test_loss: 0.4604486174025843\n",
      "iteration 7907train_loss: 0.4268742837567852 test_loss: 0.46044864048911044\n",
      "iteration 7908train_loss: 0.42687428374770287 test_loss: 0.4604486635618546\n",
      "iteration 7909train_loss: 0.42687428373863145 test_loss: 0.46044868662082505\n",
      "iteration 7910train_loss: 0.4268742837295709 test_loss: 0.4604487096660302\n",
      "iteration 7911train_loss: 0.4268742837205211 test_loss: 0.4604487326974778\n",
      "iteration 7912train_loss: 0.4268742837114821 test_loss: 0.4604487557151767\n",
      "iteration 7913train_loss: 0.42687428370245395 test_loss: 0.4604487787191346\n",
      "iteration 7914train_loss: 0.42687428369343655 test_loss: 0.4604488017093598\n",
      "iteration 7915train_loss: 0.4268742836844299 test_loss: 0.46044882468586074\n",
      "iteration 7916train_loss: 0.426874283675434 test_loss: 0.4604488476486453\n",
      "iteration 7917train_loss: 0.4268742836664489 test_loss: 0.46044887059772194\n",
      "iteration 7918train_loss: 0.4268742836574746 test_loss: 0.4604488935330987\n",
      "iteration 7919train_loss: 0.4268742836485108 test_loss: 0.46044891645478375\n",
      "iteration 7920train_loss: 0.42687428363955787 test_loss: 0.46044893936278525\n",
      "iteration 7921train_loss: 0.42687428363061564 test_loss: 0.4604489622571114\n",
      "iteration 7922train_loss: 0.426874283621684 test_loss: 0.4604489851377704\n",
      "iteration 7923train_loss: 0.42687428361276314 test_loss: 0.46044900800477034\n",
      "iteration 7924train_loss: 0.4268742836038529 test_loss: 0.4604490308581194\n",
      "iteration 7925train_loss: 0.4268742835949532 test_loss: 0.4604490536978256\n",
      "iteration 7926train_loss: 0.42687428358606416 test_loss: 0.46044907652389744\n",
      "iteration 7927train_loss: 0.42687428357718576 test_loss: 0.46044909933634265\n",
      "iteration 7928train_loss: 0.42687428356831797 test_loss: 0.4604491221351696\n",
      "iteration 7929train_loss: 0.42687428355946083 test_loss: 0.46044914492038636\n",
      "iteration 7930train_loss: 0.42687428355061424 test_loss: 0.46044916769200117\n",
      "iteration 7931train_loss: 0.42687428354177825 test_loss: 0.4604491904500219\n",
      "iteration 7932train_loss: 0.42687428353295265 test_loss: 0.46044921319445686\n",
      "iteration 7933train_loss: 0.4268742835241378 test_loss: 0.46044923592531417\n",
      "iteration 7934train_loss: 0.42687428351533335 test_loss: 0.4604492586426017\n",
      "iteration 7935train_loss: 0.42687428350653944 test_loss: 0.4604492813463278\n",
      "iteration 7936train_loss: 0.42687428349775597 test_loss: 0.4604493040365004\n",
      "iteration 7937train_loss: 0.426874283488983 test_loss: 0.4604493267131277\n",
      "iteration 7938train_loss: 0.42687428348022055 test_loss: 0.46044934937621773\n",
      "iteration 7939train_loss: 0.42687428347146866 test_loss: 0.46044937202577857\n",
      "iteration 7940train_loss: 0.4268742834627272 test_loss: 0.4604493946618184\n",
      "iteration 7941train_loss: 0.42687428345399614 test_loss: 0.4604494172843452\n",
      "iteration 7942train_loss: 0.42687428344527545 test_loss: 0.46044943989336695\n",
      "iteration 7943train_loss: 0.4268742834365651 test_loss: 0.46044946248889185\n",
      "iteration 7944train_loss: 0.4268742834278653 test_loss: 0.4604494850709279\n",
      "iteration 7945train_loss: 0.4268742834191758 test_loss: 0.4604495076394831\n",
      "iteration 7946train_loss: 0.4268742834104968 test_loss: 0.4604495301945657\n",
      "iteration 7947train_loss: 0.4268742834018281 test_loss: 0.4604495527361835\n",
      "iteration 7948train_loss: 0.42687428339316974 test_loss: 0.46044957526434455\n",
      "iteration 7949train_loss: 0.42687428338452177 test_loss: 0.46044959777905714\n",
      "iteration 7950train_loss: 0.4268742833758841 test_loss: 0.460449620280329\n",
      "iteration 7951train_loss: 0.4268742833672567 test_loss: 0.46044964276816835\n",
      "iteration 7952train_loss: 0.4268742833586397 test_loss: 0.46044966524258313\n",
      "iteration 7953train_loss: 0.42687428335003286 test_loss: 0.46044968770358125\n",
      "iteration 7954train_loss: 0.4268742833414364 test_loss: 0.4604497101511709\n",
      "iteration 7955train_loss: 0.4268742833328501 test_loss: 0.4604497325853599\n",
      "iteration 7956train_loss: 0.42687428332427424 test_loss: 0.4604497550061565\n",
      "iteration 7957train_loss: 0.42687428331570837 test_loss: 0.4604497774135685\n",
      "iteration 7958train_loss: 0.42687428330715305 test_loss: 0.4604497998076038\n",
      "iteration 7959train_loss: 0.42687428329860766 test_loss: 0.46044982218827063\n",
      "iteration 7960train_loss: 0.42687428329007265 test_loss: 0.46044984455557686\n",
      "iteration 7961train_loss: 0.4268742832815478 test_loss: 0.46044986690953044\n",
      "iteration 7962train_loss: 0.42687428327303306 test_loss: 0.46044988925013947\n",
      "iteration 7963train_loss: 0.4268742832645286 test_loss: 0.4604499115774116\n",
      "iteration 7964train_loss: 0.4268742832560342 test_loss: 0.4604499338913552\n",
      "iteration 7965train_loss: 0.42687428324754995 test_loss: 0.4604499561919779\n",
      "iteration 7966train_loss: 0.4268742832390759 test_loss: 0.46044997847928776\n",
      "iteration 7967train_loss: 0.4268742832306119 test_loss: 0.4604500007532928\n",
      "iteration 7968train_loss: 0.426874283222158 test_loss: 0.4604500230140009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7969train_loss: 0.42687428321371435 test_loss: 0.46045004526142\n",
      "iteration 7970train_loss: 0.4268742832052806 test_loss: 0.460450067495558\n",
      "iteration 7971train_loss: 0.42687428319685694 test_loss: 0.4604500897164228\n",
      "iteration 7972train_loss: 0.42687428318844356 test_loss: 0.46045011192402246\n",
      "iteration 7973train_loss: 0.42687428318003984 test_loss: 0.46045013411836483\n",
      "iteration 7974train_loss: 0.42687428317164655 test_loss: 0.4604501562994578\n",
      "iteration 7975train_loss: 0.42687428316326304 test_loss: 0.46045017846730923\n",
      "iteration 7976train_loss: 0.4268742831548897 test_loss: 0.4604502006219272\n",
      "iteration 7977train_loss: 0.42687428314652615 test_loss: 0.4604502227633193\n",
      "iteration 7978train_loss: 0.4268742831381728 test_loss: 0.46045024489149383\n",
      "iteration 7979train_loss: 0.42687428312982933 test_loss: 0.46045026700645836\n",
      "iteration 7980train_loss: 0.4268742831214958 test_loss: 0.46045028910822083\n",
      "iteration 7981train_loss: 0.42687428311317227 test_loss: 0.4604503111967892\n",
      "iteration 7982train_loss: 0.42687428310485864 test_loss: 0.4604503332721714\n",
      "iteration 7983train_loss: 0.42687428309655495 test_loss: 0.46045035533437506\n",
      "iteration 7984train_loss: 0.4268742830882612 test_loss: 0.4604503773834083\n",
      "iteration 7985train_loss: 0.4268742830799774 test_loss: 0.4604503994192789\n",
      "iteration 7986train_loss: 0.4268742830717034 test_loss: 0.4604504214419946\n",
      "iteration 7987train_loss: 0.42687428306343933 test_loss: 0.4604504434515634\n",
      "iteration 7988train_loss: 0.42687428305518516 test_loss: 0.4604504654479931\n",
      "iteration 7989train_loss: 0.42687428304694086 test_loss: 0.4604504874312916\n",
      "iteration 7990train_loss: 0.42687428303870634 test_loss: 0.4604505094014666\n",
      "iteration 7991train_loss: 0.42687428303048164 test_loss: 0.460450531358526\n",
      "iteration 7992train_loss: 0.4268742830222669 test_loss: 0.46045055330247764\n",
      "iteration 7993train_loss: 0.4268742830140617 test_loss: 0.4604505752333294\n",
      "iteration 7994train_loss: 0.4268742830058665 test_loss: 0.46045059715108894\n",
      "iteration 7995train_loss: 0.42687428299768104 test_loss: 0.4604506190557643\n",
      "iteration 7996train_loss: 0.42687428298950547 test_loss: 0.4604506409473631\n",
      "iteration 7997train_loss: 0.4268742829813395 test_loss: 0.46045066282589314\n",
      "iteration 7998train_loss: 0.42687428297318347 test_loss: 0.46045068469136236\n",
      "iteration 7999train_loss: 0.426874282965037 test_loss: 0.4604507065437785\n",
      "iteration 8000train_loss: 0.4268742829569002 test_loss: 0.46045072838314927\n",
      "iteration 8001train_loss: 0.4268742829487733 test_loss: 0.4604507502094826\n",
      "iteration 8002train_loss: 0.42687428294065605 test_loss: 0.46045077202278617\n",
      "iteration 8003train_loss: 0.42687428293254853 test_loss: 0.4604507938230678\n",
      "iteration 8004train_loss: 0.42687428292445057 test_loss: 0.4604508156103352\n",
      "iteration 8005train_loss: 0.4268742829163625 test_loss: 0.46045083738459625\n",
      "iteration 8006train_loss: 0.4268742829082839 test_loss: 0.4604508591458586\n",
      "iteration 8007train_loss: 0.42687428290021495 test_loss: 0.46045088089413\n",
      "iteration 8008train_loss: 0.4268742828921557 test_loss: 0.4604509026294183\n",
      "iteration 8009train_loss: 0.42687428288410606 test_loss: 0.46045092435173124\n",
      "iteration 8010train_loss: 0.42687428287606605 test_loss: 0.46045094606107645\n",
      "iteration 8011train_loss: 0.4268742828680355 test_loss: 0.46045096775746186\n",
      "iteration 8012train_loss: 0.4268742828600148 test_loss: 0.46045098944089496\n",
      "iteration 8013train_loss: 0.4268742828520035 test_loss: 0.46045101111138376\n",
      "iteration 8014train_loss: 0.4268742828440018 test_loss: 0.4604510327689358\n",
      "iteration 8015train_loss: 0.4268742828360098 test_loss: 0.4604510544135588\n",
      "iteration 8016train_loss: 0.4268742828280271 test_loss: 0.46045107604526053\n",
      "iteration 8017train_loss: 0.426874282820054 test_loss: 0.4604510976640488\n",
      "iteration 8018train_loss: 0.42687428281209056 test_loss: 0.46045111926993104\n",
      "iteration 8019train_loss: 0.42687428280413653 test_loss: 0.4604511408629152\n",
      "iteration 8020train_loss: 0.42687428279619205 test_loss: 0.460451162443009\n",
      "iteration 8021train_loss: 0.42687428278825695 test_loss: 0.4604511840102199\n",
      "iteration 8022train_loss: 0.42687428278033135 test_loss: 0.4604512055645558\n",
      "iteration 8023train_loss: 0.42687428277241535 test_loss: 0.4604512271060243\n",
      "iteration 8024train_loss: 0.4268742827645086 test_loss: 0.4604512486346331\n",
      "iteration 8025train_loss: 0.4268742827566115 test_loss: 0.46045127015038984\n",
      "iteration 8026train_loss: 0.42687428274872374 test_loss: 0.46045129165330234\n",
      "iteration 8027train_loss: 0.42687428274084543 test_loss: 0.460451313143378\n",
      "iteration 8028train_loss: 0.4268742827329765 test_loss: 0.46045133462062465\n",
      "iteration 8029train_loss: 0.42687428272511696 test_loss: 0.46045135608505\n",
      "iteration 8030train_loss: 0.4268742827172668 test_loss: 0.4604513775366615\n",
      "iteration 8031train_loss: 0.42687428270942596 test_loss: 0.460451398975467\n",
      "iteration 8032train_loss: 0.4268742827015946 test_loss: 0.46045142040147413\n",
      "iteration 8033train_loss: 0.4268742826937726 test_loss: 0.4604514418146904\n",
      "iteration 8034train_loss: 0.4268742826859599 test_loss: 0.4604514632151236\n",
      "iteration 8035train_loss: 0.4268742826781565 test_loss: 0.4604514846027812\n",
      "iteration 8036train_loss: 0.4268742826703624 test_loss: 0.4604515059776709\n",
      "iteration 8037train_loss: 0.4268742826625777 test_loss: 0.4604515273398003\n",
      "iteration 8038train_loss: 0.42687428265480226 test_loss: 0.4604515486891771\n",
      "iteration 8039train_loss: 0.4268742826470361 test_loss: 0.4604515700258088\n",
      "iteration 8040train_loss: 0.4268742826392792 test_loss: 0.4604515913497031\n",
      "iteration 8041train_loss: 0.42687428263153154 test_loss: 0.4604516126608675\n",
      "iteration 8042train_loss: 0.4268742826237931 test_loss: 0.4604516339593096\n",
      "iteration 8043train_loss: 0.42687428261606397 test_loss: 0.46045165524503723\n",
      "iteration 8044train_loss: 0.42687428260834415 test_loss: 0.46045167651805774\n",
      "iteration 8045train_loss: 0.42687428260063354 test_loss: 0.46045169777837874\n",
      "iteration 8046train_loss: 0.42687428259293203 test_loss: 0.46045171902600773\n",
      "iteration 8047train_loss: 0.42687428258523974 test_loss: 0.4604517402609526\n",
      "iteration 8048train_loss: 0.42687428257755655 test_loss: 0.46045176148322065\n",
      "iteration 8049train_loss: 0.4268742825698827 test_loss: 0.4604517826928195\n",
      "iteration 8050train_loss: 0.42687428256221793 test_loss: 0.46045180388975676\n",
      "iteration 8051train_loss: 0.42687428255456233 test_loss: 0.46045182507404\n",
      "iteration 8052train_loss: 0.42687428254691584 test_loss: 0.4604518462456766\n",
      "iteration 8053train_loss: 0.42687428253927856 test_loss: 0.46045186740467436\n",
      "iteration 8054train_loss: 0.42687428253165033 test_loss: 0.4604518885510406\n",
      "iteration 8055train_loss: 0.4268742825240313 test_loss: 0.460451909684783\n",
      "iteration 8056train_loss: 0.4268742825164213 test_loss: 0.460451930805909\n",
      "iteration 8057train_loss: 0.42687428250882037 test_loss: 0.4604519519144263\n",
      "iteration 8058train_loss: 0.4268742825012285 test_loss: 0.46045197301034224\n",
      "iteration 8059train_loss: 0.4268742824936458 test_loss: 0.46045199409366455\n",
      "iteration 8060train_loss: 0.426874282486072 test_loss: 0.46045201516440043\n",
      "iteration 8061train_loss: 0.4268742824785075 test_loss: 0.46045203622255765\n",
      "iteration 8062train_loss: 0.4268742824709519 test_loss: 0.4604520572681437\n",
      "iteration 8063train_loss: 0.4268742824634053 test_loss: 0.4604520783011659\n",
      "iteration 8064train_loss: 0.42687428245586767 test_loss: 0.46045209932163195\n",
      "iteration 8065train_loss: 0.42687428244833914 test_loss: 0.4604521203295493\n",
      "iteration 8066train_loss: 0.4268742824408195 test_loss: 0.46045214132492535\n",
      "iteration 8067train_loss: 0.42687428243330894 test_loss: 0.46045216230776775\n",
      "iteration 8068train_loss: 0.42687428242580727 test_loss: 0.4604521832780837\n",
      "iteration 8069train_loss: 0.4268742824183145 test_loss: 0.46045220423588096\n",
      "iteration 8070train_loss: 0.4268742824108308 test_loss: 0.4604522251811669\n",
      "iteration 8071train_loss: 0.426874282403356 test_loss: 0.4604522461139489\n",
      "iteration 8072train_loss: 0.4268742823958902 test_loss: 0.4604522670342347\n",
      "iteration 8073train_loss: 0.4268742823884332 test_loss: 0.46045228794203136\n",
      "iteration 8074train_loss: 0.4268742823809852 test_loss: 0.46045230883734667\n",
      "iteration 8075train_loss: 0.42687428237354597 test_loss: 0.4604523297201879\n",
      "iteration 8076train_loss: 0.42687428236611574 test_loss: 0.4604523505905625\n",
      "iteration 8077train_loss: 0.4268742823586944 test_loss: 0.46045237144847806\n",
      "iteration 8078train_loss: 0.4268742823512818 test_loss: 0.46045239229394186\n",
      "iteration 8079train_loss: 0.4268742823438781 test_loss: 0.4604524131269614\n",
      "iteration 8080train_loss: 0.4268742823364832 test_loss: 0.460452433947544\n",
      "iteration 8081train_loss: 0.42687428232909735 test_loss: 0.46045245475569724\n",
      "iteration 8082train_loss: 0.4268742823217201 test_loss: 0.46045247555142854\n",
      "iteration 8083train_loss: 0.4268742823143517 test_loss: 0.4604524963347452\n",
      "iteration 8084train_loss: 0.4268742823069921 test_loss: 0.46045251710565466\n",
      "iteration 8085train_loss: 0.4268742822996414 test_loss: 0.4604525378641644\n",
      "iteration 8086train_loss: 0.4268742822922994 test_loss: 0.4604525586102817\n",
      "iteration 8087train_loss: 0.42687428228496616 test_loss: 0.460452579344014\n",
      "iteration 8088train_loss: 0.42687428227764174 test_loss: 0.4604526000653688\n",
      "iteration 8089train_loss: 0.426874282270326 test_loss: 0.4604526207743534\n",
      "iteration 8090train_loss: 0.42687428226301893 test_loss: 0.4604526414709753\n",
      "iteration 8091train_loss: 0.4268742822557207 test_loss: 0.46045266215524155\n",
      "iteration 8092train_loss: 0.42687428224843116 test_loss: 0.4604526828271598\n",
      "iteration 8093train_loss: 0.42687428224115026 test_loss: 0.4604527034867374\n",
      "iteration 8094train_loss: 0.42687428223387813 test_loss: 0.4604527241339817\n",
      "iteration 8095train_loss: 0.42687428222661467 test_loss: 0.4604527447689\n",
      "iteration 8096train_loss: 0.4268742822193599 test_loss: 0.46045276539149965\n",
      "iteration 8097train_loss: 0.42687428221211376 test_loss: 0.46045278600178813\n",
      "iteration 8098train_loss: 0.42687428220487633 test_loss: 0.46045280659977267\n",
      "iteration 8099train_loss: 0.42687428219764745 test_loss: 0.46045282718546077\n",
      "iteration 8100train_loss: 0.42687428219042733 test_loss: 0.4604528477588595\n",
      "iteration 8101train_loss: 0.4268742821832156 test_loss: 0.46045286831997645\n",
      "iteration 8102train_loss: 0.4268742821760127 test_loss: 0.46045288886881874\n",
      "iteration 8103train_loss: 0.4268742821688184 test_loss: 0.46045290940539385\n",
      "iteration 8104train_loss: 0.4268742821616326 test_loss: 0.46045292992970904\n",
      "iteration 8105train_loss: 0.42687428215445544 test_loss: 0.4604529504417717\n",
      "iteration 8106train_loss: 0.4268742821472868 test_loss: 0.4604529709415891\n",
      "iteration 8107train_loss: 0.42687428214012685 test_loss: 0.46045299142916846\n",
      "iteration 8108train_loss: 0.4268742821329753 test_loss: 0.46045301190451726\n",
      "iteration 8109train_loss: 0.4268742821258323 test_loss: 0.4604530323676426\n",
      "iteration 8110train_loss: 0.42687428211869793 test_loss: 0.460453052818552\n",
      "iteration 8111train_loss: 0.42687428211157197 test_loss: 0.4604530732572525\n",
      "iteration 8112train_loss: 0.4268742821044546 test_loss: 0.46045309368375165\n",
      "iteration 8113train_loss: 0.42687428209734574 test_loss: 0.46045311409805645\n",
      "iteration 8114train_loss: 0.4268742820902453 test_loss: 0.46045313450017444\n",
      "iteration 8115train_loss: 0.4268742820831533 test_loss: 0.46045315489011274\n",
      "iteration 8116train_loss: 0.42687428207606987 test_loss: 0.4604531752678786\n",
      "iteration 8117train_loss: 0.42687428206899497 test_loss: 0.4604531956334795\n",
      "iteration 8118train_loss: 0.42687428206192835 test_loss: 0.46045321598692246\n",
      "iteration 8119train_loss: 0.42687428205487027 test_loss: 0.4604532363282149\n",
      "iteration 8120train_loss: 0.4268742820478206 test_loss: 0.4604532566573638\n",
      "iteration 8121train_loss: 0.4268742820407793 test_loss: 0.4604532769743767\n",
      "iteration 8122train_loss: 0.42687428203374644 test_loss: 0.4604532972792609\n",
      "iteration 8123train_loss: 0.426874282026722 test_loss: 0.46045331757202335\n",
      "iteration 8124train_loss: 0.426874282019706 test_loss: 0.4604533378526715\n",
      "iteration 8125train_loss: 0.4268742820126983 test_loss: 0.4604533581212124\n",
      "iteration 8126train_loss: 0.426874282005699 test_loss: 0.4604533783776535\n",
      "iteration 8127train_loss: 0.42687428199870797 test_loss: 0.4604533986220019\n",
      "iteration 8128train_loss: 0.42687428199172534 test_loss: 0.4604534188542648\n",
      "iteration 8129train_loss: 0.42687428198475114 test_loss: 0.4604534390744494\n",
      "iteration 8130train_loss: 0.4268742819777851 test_loss: 0.4604534592825629\n",
      "iteration 8131train_loss: 0.4268742819708276 test_loss: 0.46045347947861265\n",
      "iteration 8132train_loss: 0.42687428196387817 test_loss: 0.4604534996626057\n",
      "iteration 8133train_loss: 0.4268742819569373 test_loss: 0.4604535198345493\n",
      "iteration 8134train_loss: 0.4268742819500045 test_loss: 0.4604535399944507\n",
      "iteration 8135train_loss: 0.4268742819430801 test_loss: 0.460453560142317\n",
      "iteration 8136train_loss: 0.42687428193616395 test_loss: 0.46045358027815536\n",
      "iteration 8137train_loss: 0.42687428192925597 test_loss: 0.4604536004019732\n",
      "iteration 8138train_loss: 0.4268742819223563 test_loss: 0.4604536205137773\n",
      "iteration 8139train_loss: 0.42687428191546484 test_loss: 0.4604536406135751\n",
      "iteration 8140train_loss: 0.4268742819085818 test_loss: 0.4604536607013737\n",
      "iteration 8141train_loss: 0.42687428190170673 test_loss: 0.46045368077718024\n",
      "iteration 8142train_loss: 0.4268742818948399 test_loss: 0.46045370084100207\n",
      "iteration 8143train_loss: 0.42687428188798143 test_loss: 0.46045372089284603\n",
      "iteration 8144train_loss: 0.4268742818811311 test_loss: 0.46045374093271935\n",
      "iteration 8145train_loss: 0.42687428187428883 test_loss: 0.4604537609606294\n",
      "iteration 8146train_loss: 0.42687428186745485 test_loss: 0.46045378097658307\n",
      "iteration 8147train_loss: 0.42687428186062903 test_loss: 0.46045380098058764\n",
      "iteration 8148train_loss: 0.42687428185381127 test_loss: 0.46045382097265014\n",
      "iteration 8149train_loss: 0.4268742818470017 test_loss: 0.46045384095277786\n",
      "iteration 8150train_loss: 0.4268742818402003 test_loss: 0.4604538609209777\n",
      "iteration 8151train_loss: 0.4268742818334069 test_loss: 0.4604538808772569\n",
      "iteration 8152train_loss: 0.4268742818266218 test_loss: 0.4604539008216227\n",
      "iteration 8153train_loss: 0.4268742818198447 test_loss: 0.460453920754082\n",
      "iteration 8154train_loss: 0.4268742818130757 test_loss: 0.4604539406746419\n",
      "iteration 8155train_loss: 0.4268742818063148 test_loss: 0.46045396058330973\n",
      "iteration 8156train_loss: 0.42687428179956194 test_loss: 0.4604539804800924\n",
      "iteration 8157train_loss: 0.42687428179281717 test_loss: 0.46045400036499695\n",
      "iteration 8158train_loss: 0.42687428178608045 test_loss: 0.4604540202380307\n",
      "iteration 8159train_loss: 0.42687428177935177 test_loss: 0.46045404009920055\n",
      "iteration 8160train_loss: 0.4268742817726311 test_loss: 0.46045405994851374\n",
      "iteration 8161train_loss: 0.42687428176591846 test_loss: 0.46045407978597713\n",
      "iteration 8162train_loss: 0.42687428175921394 test_loss: 0.46045409961159806\n",
      "iteration 8163train_loss: 0.4268742817525174 test_loss: 0.4604541194253833\n",
      "iteration 8164train_loss: 0.42687428174582875 test_loss: 0.46045413922734013\n",
      "iteration 8165train_loss: 0.4268742817391482 test_loss: 0.46045415901747544\n",
      "iteration 8166train_loss: 0.42687428173247555 test_loss: 0.4604541787957965\n",
      "iteration 8167train_loss: 0.4268742817258108 test_loss: 0.4604541985623103\n",
      "iteration 8168train_loss: 0.4268742817191542 test_loss: 0.4604542183170237\n",
      "iteration 8169train_loss: 0.42687428171250535 test_loss: 0.460454238059944\n",
      "iteration 8170train_loss: 0.4268742817058646 test_loss: 0.460454257791078\n",
      "iteration 8171train_loss: 0.42687428169923175 test_loss: 0.460454277510433\n",
      "iteration 8172train_loss: 0.42687428169260677 test_loss: 0.46045429721801584\n",
      "iteration 8173train_loss: 0.4268742816859898 test_loss: 0.4604543169138335\n",
      "iteration 8174train_loss: 0.42687428167938063 test_loss: 0.46045433659789314\n",
      "iteration 8175train_loss: 0.4268742816727794 test_loss: 0.46045435627020187\n",
      "iteration 8176train_loss: 0.426874281666186 test_loss: 0.4604543759307665\n",
      "iteration 8177train_loss: 0.42687428165960056 test_loss: 0.46045439557959406\n",
      "iteration 8178train_loss: 0.4268742816530229 test_loss: 0.46045441521669167\n",
      "iteration 8179train_loss: 0.42687428164645314 test_loss: 0.4604544348420662\n",
      "iteration 8180train_loss: 0.4268742816398912 test_loss: 0.4604544544557247\n",
      "iteration 8181train_loss: 0.4268742816333372 test_loss: 0.46045447405767426\n",
      "iteration 8182train_loss: 0.42687428162679086 test_loss: 0.46045449364792174\n",
      "iteration 8183train_loss: 0.4268742816202524 test_loss: 0.4604545132264741\n",
      "iteration 8184train_loss: 0.4268742816137218 test_loss: 0.4604545327933386\n",
      "iteration 8185train_loss: 0.426874281607199 test_loss: 0.4604545523485219\n",
      "iteration 8186train_loss: 0.42687428160068386 test_loss: 0.460454571892031\n",
      "iteration 8187train_loss: 0.42687428159417673 test_loss: 0.4604545914238731\n",
      "iteration 8188train_loss: 0.4268742815876772 test_loss: 0.4604546109440549\n",
      "iteration 8189train_loss: 0.4268742815811854 test_loss: 0.4604546304525836\n",
      "iteration 8190train_loss: 0.4268742815747015 test_loss: 0.4604546499494659\n",
      "iteration 8191train_loss: 0.42687428156822527 test_loss: 0.46045466943470886\n",
      "iteration 8192train_loss: 0.4268742815617567 test_loss: 0.46045468890831953\n",
      "iteration 8193train_loss: 0.426874281555296 test_loss: 0.4604547083703048\n",
      "iteration 8194train_loss: 0.42687428154884294 test_loss: 0.46045472782067154\n",
      "iteration 8195train_loss: 0.42687428154239754 test_loss: 0.46045474725942676\n",
      "iteration 8196train_loss: 0.4268742815359599 test_loss: 0.46045476668657725\n",
      "iteration 8197train_loss: 0.42687428152952994 test_loss: 0.46045478610213014\n",
      "iteration 8198train_loss: 0.42687428152310763 test_loss: 0.46045480550609225\n",
      "iteration 8199train_loss: 0.42687428151669304 test_loss: 0.4604548248984705\n",
      "iteration 8200train_loss: 0.42687428151028606 test_loss: 0.4604548442792718\n",
      "iteration 8201train_loss: 0.42687428150388673 test_loss: 0.4604548636485031\n",
      "iteration 8202train_loss: 0.42687428149749507 test_loss: 0.46045488300617127\n",
      "iteration 8203train_loss: 0.4268742814911111 test_loss: 0.4604549023522832\n",
      "iteration 8204train_loss: 0.4268742814847346 test_loss: 0.46045492168684576\n",
      "iteration 8205train_loss: 0.42687428147836576 test_loss: 0.46045494100986595\n",
      "iteration 8206train_loss: 0.4268742814720047 test_loss: 0.4604549603213506\n",
      "iteration 8207train_loss: 0.4268742814656511 test_loss: 0.4604549796213066\n",
      "iteration 8208train_loss: 0.42687428145930506 test_loss: 0.46045499890974084\n",
      "iteration 8209train_loss: 0.4268742814529667 test_loss: 0.4604550181866602\n",
      "iteration 8210train_loss: 0.42687428144663586 test_loss: 0.46045503745207145\n",
      "iteration 8211train_loss: 0.42687428144031253 test_loss: 0.46045505670598164\n",
      "iteration 8212train_loss: 0.4268742814339968 test_loss: 0.4604550759483974\n",
      "iteration 8213train_loss: 0.4268742814276886 test_loss: 0.46045509517932587\n",
      "iteration 8214train_loss: 0.4268742814213879 test_loss: 0.46045511439877373\n",
      "iteration 8215train_loss: 0.42687428141509476 test_loss: 0.4604551336067479\n",
      "iteration 8216train_loss: 0.4268742814088091 test_loss: 0.4604551528032552\n",
      "iteration 8217train_loss: 0.426874281402531 test_loss: 0.4604551719883024\n",
      "iteration 8218train_loss: 0.42687428139626055 test_loss: 0.46045519116189654\n",
      "iteration 8219train_loss: 0.42687428138999733 test_loss: 0.46045521032404424\n",
      "iteration 8220train_loss: 0.42687428138374167 test_loss: 0.4604552294747524\n",
      "iteration 8221train_loss: 0.42687428137749345 test_loss: 0.4604552486140279\n",
      "iteration 8222train_loss: 0.4268742813712527 test_loss: 0.46045526774187756\n",
      "iteration 8223train_loss: 0.4268742813650194 test_loss: 0.4604552868583082\n",
      "iteration 8224train_loss: 0.4268742813587937 test_loss: 0.4604553059633266\n",
      "iteration 8225train_loss: 0.4268742813525753 test_loss: 0.4604553250569396\n",
      "iteration 8226train_loss: 0.4268742813463643 test_loss: 0.460455344139154\n",
      "iteration 8227train_loss: 0.4268742813401607 test_loss: 0.4604553632099766\n",
      "iteration 8228train_loss: 0.4268742813339646 test_loss: 0.4604553822694142\n",
      "iteration 8229train_loss: 0.4268742813277759 test_loss: 0.4604554013174735\n",
      "iteration 8230train_loss: 0.42687428132159455 test_loss: 0.46045542035416137\n",
      "iteration 8231train_loss: 0.4268742813154206 test_loss: 0.4604554393794847\n",
      "iteration 8232train_loss: 0.42687428130925403 test_loss: 0.4604554583934502\n",
      "iteration 8233train_loss: 0.4268742813030948 test_loss: 0.4604554773960645\n",
      "iteration 8234train_loss: 0.42687428129694294 test_loss: 0.46045549638733474\n",
      "iteration 8235train_loss: 0.42687428129079835 test_loss: 0.46045551536726725\n",
      "iteration 8236train_loss: 0.42687428128466126 test_loss: 0.4604555343358691\n",
      "iteration 8237train_loss: 0.42687428127853133 test_loss: 0.46045555329314697\n",
      "iteration 8238train_loss: 0.42687428127240884 test_loss: 0.4604555722391076\n",
      "iteration 8239train_loss: 0.4268742812662937 test_loss: 0.4604555911737578\n",
      "iteration 8240train_loss: 0.4268742812601857 test_loss: 0.46045561009710423\n",
      "iteration 8241train_loss: 0.426874281254085 test_loss: 0.46045562900915377\n",
      "iteration 8242train_loss: 0.4268742812479917 test_loss: 0.46045564790991295\n",
      "iteration 8243train_loss: 0.42687428124190563 test_loss: 0.46045566679938865\n",
      "iteration 8244train_loss: 0.4268742812358269 test_loss: 0.46045568567758766\n",
      "iteration 8245train_loss: 0.4268742812297553 test_loss: 0.4604557045445166\n",
      "iteration 8246train_loss: 0.42687428122369103 test_loss: 0.46045572340018226\n",
      "iteration 8247train_loss: 0.42687428121763404 test_loss: 0.46045574224459146\n",
      "iteration 8248train_loss: 0.4268742812115842 test_loss: 0.46045576107775055\n",
      "iteration 8249train_loss: 0.42687428120554166 test_loss: 0.4604557798996667\n",
      "iteration 8250train_loss: 0.4268742811995062 test_loss: 0.46045579871034625\n",
      "iteration 8251train_loss: 0.42687428119347803 test_loss: 0.46045581750979625\n",
      "iteration 8252train_loss: 0.4268742811874571 test_loss: 0.4604558362980232\n",
      "iteration 8253train_loss: 0.4268742811814434 test_loss: 0.4604558550750338\n",
      "iteration 8254train_loss: 0.4268742811754368 test_loss: 0.4604558738408348\n",
      "iteration 8255train_loss: 0.4268742811694374 test_loss: 0.4604558925954328\n",
      "iteration 8256train_loss: 0.426874281163445 test_loss: 0.4604559113388347\n",
      "iteration 8257train_loss: 0.42687428115745996 test_loss: 0.4604559300710468\n",
      "iteration 8258train_loss: 0.42687428115148207 test_loss: 0.4604559487920762\n",
      "iteration 8259train_loss: 0.4268742811455112 test_loss: 0.46045596750192935\n",
      "iteration 8260train_loss: 0.4268742811395476 test_loss: 0.4604559862006129\n",
      "iteration 8261train_loss: 0.426874281133591 test_loss: 0.46045600488813365\n",
      "iteration 8262train_loss: 0.4268742811276416 test_loss: 0.46045602356449816\n",
      "iteration 8263train_loss: 0.42687428112169923 test_loss: 0.46045604222971315\n",
      "iteration 8264train_loss: 0.426874281115764 test_loss: 0.46045606088378516\n",
      "iteration 8265train_loss: 0.42687428110983583 test_loss: 0.460456079526721\n",
      "iteration 8266train_loss: 0.42687428110391484 test_loss: 0.4604560981585272\n",
      "iteration 8267train_loss: 0.42687428109800074 test_loss: 0.4604561167792105\n",
      "iteration 8268train_loss: 0.42687428109209385 test_loss: 0.46045613538877744\n",
      "iteration 8269train_loss: 0.426874281086194 test_loss: 0.4604561539872348\n",
      "iteration 8270train_loss: 0.42687428108030107 test_loss: 0.460456172574589\n",
      "iteration 8271train_loss: 0.4268742810744153 test_loss: 0.4604561911508468\n",
      "iteration 8272train_loss: 0.4268742810685366 test_loss: 0.46045620971601486\n",
      "iteration 8273train_loss: 0.42687428106266484 test_loss: 0.46045622827009974\n",
      "iteration 8274train_loss: 0.42687428105680003 test_loss: 0.460456246813108\n",
      "iteration 8275train_loss: 0.42687428105094233 test_loss: 0.46045626534504636\n",
      "iteration 8276train_loss: 0.42687428104509156 test_loss: 0.46045628386592136\n",
      "iteration 8277train_loss: 0.42687428103924774 test_loss: 0.46045630237573965\n",
      "iteration 8278train_loss: 0.426874281033411 test_loss: 0.4604563208745077\n",
      "iteration 8279train_loss: 0.42687428102758124 test_loss: 0.4604563393622323\n",
      "iteration 8280train_loss: 0.4268742810217584 test_loss: 0.46045635783891997\n",
      "iteration 8281train_loss: 0.4268742810159425 test_loss: 0.46045637630457725\n",
      "iteration 8282train_loss: 0.4268742810101336 test_loss: 0.4604563947592108\n",
      "iteration 8283train_loss: 0.42687428100433156 test_loss: 0.4604564132028271\n",
      "iteration 8284train_loss: 0.4268742809985365 test_loss: 0.4604564316354327\n",
      "iteration 8285train_loss: 0.4268742809927483 test_loss: 0.46045645005703434\n",
      "iteration 8286train_loss: 0.4268742809869671 test_loss: 0.46045646846763855\n",
      "iteration 8287train_loss: 0.4268742809811928 test_loss: 0.46045648686725177\n",
      "iteration 8288train_loss: 0.42687428097542524 test_loss: 0.46045650525588067\n",
      "iteration 8289train_loss: 0.42687428096966473 test_loss: 0.46045652363353173\n",
      "iteration 8290train_loss: 0.4268742809639111 test_loss: 0.4604565420002116\n",
      "iteration 8291train_loss: 0.4268742809581642 test_loss: 0.4604565603559268\n",
      "iteration 8292train_loss: 0.42687428095242436 test_loss: 0.4604565787006837\n",
      "iteration 8293train_loss: 0.42687428094669133 test_loss: 0.46045659703448916\n",
      "iteration 8294train_loss: 0.42687428094096497 test_loss: 0.4604566153573494\n",
      "iteration 8295train_loss: 0.4268742809352456 test_loss: 0.4604566336692712\n",
      "iteration 8296train_loss: 0.426874280929533 test_loss: 0.4604566519702609\n",
      "iteration 8297train_loss: 0.4268742809238273 test_loss: 0.46045667026032516\n",
      "iteration 8298train_loss: 0.42687428091812835 test_loss: 0.4604566885394705\n",
      "iteration 8299train_loss: 0.4268742809124362 test_loss: 0.4604567068077034\n",
      "iteration 8300train_loss: 0.4268742809067509 test_loss: 0.4604567250650303\n",
      "iteration 8301train_loss: 0.4268742809010723 test_loss: 0.46045674331145786\n",
      "iteration 8302train_loss: 0.4268742808954006 test_loss: 0.4604567615469925\n",
      "iteration 8303train_loss: 0.42687428088973567 test_loss: 0.4604567797716405\n",
      "iteration 8304train_loss: 0.4268742808840773 test_loss: 0.4604567979854089\n",
      "iteration 8305train_loss: 0.42687428087842594 test_loss: 0.46045681618830364\n",
      "iteration 8306train_loss: 0.4268742808727812 test_loss: 0.4604568343803316\n",
      "iteration 8307train_loss: 0.4268742808671432 test_loss: 0.4604568525614991\n",
      "iteration 8308train_loss: 0.42687428086151197 test_loss: 0.4604568707318126\n",
      "iteration 8309train_loss: 0.42687428085588747 test_loss: 0.4604568888912786\n",
      "iteration 8310train_loss: 0.4268742808502696 test_loss: 0.4604569070399036\n",
      "iteration 8311train_loss: 0.42687428084465856 test_loss: 0.4604569251776941\n",
      "iteration 8312train_loss: 0.42687428083905415 test_loss: 0.4604569433046565\n",
      "iteration 8313train_loss: 0.42687428083345647 test_loss: 0.46045696142079723\n",
      "iteration 8314train_loss: 0.4268742808278654 test_loss: 0.4604569795261229\n",
      "iteration 8315train_loss: 0.4268742808222811 test_loss: 0.4604569976206398\n",
      "iteration 8316train_loss: 0.4268742808167035 test_loss: 0.4604570157043546\n",
      "iteration 8317train_loss: 0.42687428081113243 test_loss: 0.4604570337772734\n",
      "iteration 8318train_loss: 0.42687428080556805 test_loss: 0.46045705183940294\n",
      "iteration 8319train_loss: 0.4268742808000103 test_loss: 0.4604570698907497\n",
      "iteration 8320train_loss: 0.4268742807944593 test_loss: 0.4604570879313199\n",
      "iteration 8321train_loss: 0.42687428078891476 test_loss: 0.46045710596111994\n",
      "iteration 8322train_loss: 0.42687428078337697 test_loss: 0.46045712398015654\n",
      "iteration 8323train_loss: 0.42687428077784584 test_loss: 0.4604571419884359\n",
      "iteration 8324train_loss: 0.42687428077232115 test_loss: 0.46045715998596454\n",
      "iteration 8325train_loss: 0.42687428076680317 test_loss: 0.4604571779727487\n",
      "iteration 8326train_loss: 0.4268742807612918 test_loss: 0.4604571959487951\n",
      "iteration 8327train_loss: 0.42687428075578693 test_loss: 0.4604572139141098\n",
      "iteration 8328train_loss: 0.4268742807502886 test_loss: 0.4604572318686995\n",
      "iteration 8329train_loss: 0.426874280744797 test_loss: 0.46045724981257047\n",
      "iteration 8330train_loss: 0.42687428073931194 test_loss: 0.46045726774572915\n",
      "iteration 8331train_loss: 0.4268742807338332 test_loss: 0.4604572856681818\n",
      "iteration 8332train_loss: 0.4268742807283613 test_loss: 0.46045730357993503\n",
      "iteration 8333train_loss: 0.42687428072289574 test_loss: 0.46045732148099516\n",
      "iteration 8334train_loss: 0.42687428071743677 test_loss: 0.46045733937136835\n",
      "iteration 8335train_loss: 0.4268742807119842 test_loss: 0.46045735725106124\n",
      "iteration 8336train_loss: 0.4268742807065384 test_loss: 0.4604573751200801\n",
      "iteration 8337train_loss: 0.4268742807010989 test_loss: 0.46045739297843136\n",
      "iteration 8338train_loss: 0.4268742806956659 test_loss: 0.46045741082612135\n",
      "iteration 8339train_loss: 0.4268742806902395 test_loss: 0.4604574286631564\n",
      "iteration 8340train_loss: 0.4268742806848194 test_loss: 0.46045744648954284\n",
      "iteration 8341train_loss: 0.42687428067940597 test_loss: 0.4604574643052872\n",
      "iteration 8342train_loss: 0.4268742806739989 test_loss: 0.4604574821103957\n",
      "iteration 8343train_loss: 0.42687428066859834 test_loss: 0.4604574999048748\n",
      "iteration 8344train_loss: 0.4268742806632042 test_loss: 0.46045751768873067\n",
      "iteration 8345train_loss: 0.4268742806578165 test_loss: 0.4604575354619697\n",
      "iteration 8346train_loss: 0.42687428065243527 test_loss: 0.46045755322459825\n",
      "iteration 8347train_loss: 0.42687428064706046 test_loss: 0.46045757097662277\n",
      "iteration 8348train_loss: 0.4268742806416921 test_loss: 0.4604575887180494\n",
      "iteration 8349train_loss: 0.42687428063633 test_loss: 0.4604576064488845\n",
      "iteration 8350train_loss: 0.4268742806309744 test_loss: 0.46045762416913466\n",
      "iteration 8351train_loss: 0.42687428062562516 test_loss: 0.4604576418788058\n",
      "iteration 8352train_loss: 0.4268742806202823 test_loss: 0.46045765957790447\n",
      "iteration 8353train_loss: 0.4268742806149459 test_loss: 0.460457677266437\n",
      "iteration 8354train_loss: 0.42687428060961585 test_loss: 0.46045769494440947\n",
      "iteration 8355train_loss: 0.4268742806042921 test_loss: 0.4604577126118285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8356train_loss: 0.42687428059897475 test_loss: 0.4604577302687\n",
      "iteration 8357train_loss: 0.4268742805936637 test_loss: 0.46045774791503075\n",
      "iteration 8358train_loss: 0.42687428058835913 test_loss: 0.4604577655508268\n",
      "iteration 8359train_loss: 0.42687428058306087 test_loss: 0.4604577831760943\n",
      "iteration 8360train_loss: 0.4268742805777688 test_loss: 0.46045780079083964\n",
      "iteration 8361train_loss: 0.42687428057248317 test_loss: 0.4604578183950691\n",
      "iteration 8362train_loss: 0.42687428056720383 test_loss: 0.46045783598878914\n",
      "iteration 8363train_loss: 0.4268742805619309 test_loss: 0.4604578535720058\n",
      "iteration 8364train_loss: 0.426874280556664 test_loss: 0.46045787114472536\n",
      "iteration 8365train_loss: 0.42687428055140353 test_loss: 0.46045788870695414\n",
      "iteration 8366train_loss: 0.4268742805461494 test_loss: 0.4604579062586985\n",
      "iteration 8367train_loss: 0.42687428054090154 test_loss: 0.46045792379996453\n",
      "iteration 8368train_loss: 0.42687428053565984 test_loss: 0.4604579413307586\n",
      "iteration 8369train_loss: 0.4268742805304244 test_loss: 0.460457958851087\n",
      "iteration 8370train_loss: 0.4268742805251953 test_loss: 0.46045797636095576\n",
      "iteration 8371train_loss: 0.4268742805199724 test_loss: 0.46045799386037134\n",
      "iteration 8372train_loss: 0.4268742805147558 test_loss: 0.4604580113493399\n",
      "iteration 8373train_loss: 0.42687428050954535 test_loss: 0.4604580288278677\n",
      "iteration 8374train_loss: 0.4268742805043411 test_loss: 0.46045804629596093\n",
      "iteration 8375train_loss: 0.4268742804991433 test_loss: 0.4604580637536258\n",
      "iteration 8376train_loss: 0.42687428049395143 test_loss: 0.46045808120086856\n",
      "iteration 8377train_loss: 0.42687428048876586 test_loss: 0.4604580986376956\n",
      "iteration 8378train_loss: 0.42687428048358644 test_loss: 0.4604581160641128\n",
      "iteration 8379train_loss: 0.4268742804784132 test_loss: 0.46045813348012665\n",
      "iteration 8380train_loss: 0.4268742804732462 test_loss: 0.46045815088574316\n",
      "iteration 8381train_loss: 0.42687428046808534 test_loss: 0.4604581682809688\n",
      "iteration 8382train_loss: 0.42687428046293074 test_loss: 0.4604581856658096\n",
      "iteration 8383train_loss: 0.42687428045778225 test_loss: 0.46045820304027174\n",
      "iteration 8384train_loss: 0.4268742804526398 test_loss: 0.4604582204043614\n",
      "iteration 8385train_loss: 0.42687428044750353 test_loss: 0.4604582377580849\n",
      "iteration 8386train_loss: 0.4268742804423734 test_loss: 0.4604582551014483\n",
      "iteration 8387train_loss: 0.42687428043724945 test_loss: 0.4604582724344578\n",
      "iteration 8388train_loss: 0.4268742804321316 test_loss: 0.4604582897571197\n",
      "iteration 8389train_loss: 0.4268742804270198 test_loss: 0.4604583070694401\n",
      "iteration 8390train_loss: 0.4268742804219142 test_loss: 0.4604583243714251\n",
      "iteration 8391train_loss: 0.42687428041681463 test_loss: 0.46045834166308086\n",
      "iteration 8392train_loss: 0.4268742804117212 test_loss: 0.4604583589444138\n",
      "iteration 8393train_loss: 0.4268742804066338 test_loss: 0.4604583762154298\n",
      "iteration 8394train_loss: 0.4268742804015525 test_loss: 0.46045839347613504\n",
      "iteration 8395train_loss: 0.4268742803964773 test_loss: 0.46045841072653587\n",
      "iteration 8396train_loss: 0.4268742803914081 test_loss: 0.46045842796663833\n",
      "iteration 8397train_loss: 0.426874280386345 test_loss: 0.4604584451964486\n",
      "iteration 8398train_loss: 0.4268742803812879 test_loss: 0.4604584624159727\n",
      "iteration 8399train_loss: 0.42687428037623687 test_loss: 0.4604584796252168\n",
      "iteration 8400train_loss: 0.4268742803711919 test_loss: 0.4604584968241872\n",
      "iteration 8401train_loss: 0.4268742803661528 test_loss: 0.46045851401288984\n",
      "iteration 8402train_loss: 0.42687428036111996 test_loss: 0.4604585311913311\n",
      "iteration 8403train_loss: 0.42687428035609304 test_loss: 0.46045854835951683\n",
      "iteration 8404train_loss: 0.4268742803510721 test_loss: 0.4604585655174531\n",
      "iteration 8405train_loss: 0.4268742803460571 test_loss: 0.4604585826651464\n",
      "iteration 8406train_loss: 0.42687428034104813 test_loss: 0.46045859980260256\n",
      "iteration 8407train_loss: 0.4268742803360452 test_loss: 0.46045861692982787\n",
      "iteration 8408train_loss: 0.42687428033104813 test_loss: 0.4604586340468282\n",
      "iteration 8409train_loss: 0.4268742803260572 test_loss: 0.46045865115360984\n",
      "iteration 8410train_loss: 0.4268742803210721 test_loss: 0.46045866825017884\n",
      "iteration 8411train_loss: 0.42687428031609304 test_loss: 0.4604586853365413\n",
      "iteration 8412train_loss: 0.42687428031111985 test_loss: 0.46045870241270326\n",
      "iteration 8413train_loss: 0.42687428030615254 test_loss: 0.46045871947867084\n",
      "iteration 8414train_loss: 0.42687428030119123 test_loss: 0.4604587365344502\n",
      "iteration 8415train_loss: 0.4268742802962358 test_loss: 0.46045875358004734\n",
      "iteration 8416train_loss: 0.42687428029128643 test_loss: 0.4604587706154683\n",
      "iteration 8417train_loss: 0.42687428028634283 test_loss: 0.4604587876407193\n",
      "iteration 8418train_loss: 0.42687428028140517 test_loss: 0.4604588046558062\n",
      "iteration 8419train_loss: 0.42687428027647345 test_loss: 0.46045882166073526\n",
      "iteration 8420train_loss: 0.4268742802715476 test_loss: 0.4604588386555125\n",
      "iteration 8421train_loss: 0.4268742802666276 test_loss: 0.46045885564014394\n",
      "iteration 8422train_loss: 0.42687428026171353 test_loss: 0.46045887261463564\n",
      "iteration 8423train_loss: 0.4268742802568052 test_loss: 0.46045888957899367\n",
      "iteration 8424train_loss: 0.4268742802519029 test_loss: 0.46045890653322397\n",
      "iteration 8425train_loss: 0.4268742802470063 test_loss: 0.46045892347733275\n",
      "iteration 8426train_loss: 0.4268742802421156 test_loss: 0.46045894041132607\n",
      "iteration 8427train_loss: 0.4268742802372309 test_loss: 0.4604589573352098\n",
      "iteration 8428train_loss: 0.42687428023235185 test_loss: 0.4604589742489901\n",
      "iteration 8429train_loss: 0.42687428022747864 test_loss: 0.4604589911526729\n",
      "iteration 8430train_loss: 0.42687428022261137 test_loss: 0.4604590080462643\n",
      "iteration 8431train_loss: 0.42687428021774976 test_loss: 0.4604590249297704\n",
      "iteration 8432train_loss: 0.42687428021289403 test_loss: 0.46045904180319713\n",
      "iteration 8433train_loss: 0.426874280208044 test_loss: 0.46045905866655035\n",
      "iteration 8434train_loss: 0.4268742802032 test_loss: 0.46045907551983634\n",
      "iteration 8435train_loss: 0.42687428019836154 test_loss: 0.4604590923630611\n",
      "iteration 8436train_loss: 0.42687428019352897 test_loss: 0.46045910919623034\n",
      "iteration 8437train_loss: 0.4268742801887022 test_loss: 0.4604591260193504\n",
      "iteration 8438train_loss: 0.42687428018388107 test_loss: 0.46045914283242706\n",
      "iteration 8439train_loss: 0.4268742801790658 test_loss: 0.4604591596354664\n",
      "iteration 8440train_loss: 0.4268742801742563 test_loss: 0.46045917642847434\n",
      "iteration 8441train_loss: 0.42687428016945256 test_loss: 0.46045919321145706\n",
      "iteration 8442train_loss: 0.4268742801646544 test_loss: 0.46045920998442047\n",
      "iteration 8443train_loss: 0.4268742801598622 test_loss: 0.4604592267473704\n",
      "iteration 8444train_loss: 0.4268742801550755 test_loss: 0.46045924350031286\n",
      "iteration 8445train_loss: 0.42687428015029455 test_loss: 0.46045926024325395\n",
      "iteration 8446train_loss: 0.42687428014551937 test_loss: 0.4604592769761996\n",
      "iteration 8447train_loss: 0.42687428014074996 test_loss: 0.4604592936991558\n",
      "iteration 8448train_loss: 0.42687428013598616 test_loss: 0.4604593104121283\n",
      "iteration 8449train_loss: 0.426874280131228 test_loss: 0.4604593271151234\n",
      "iteration 8450train_loss: 0.4268742801264757 test_loss: 0.4604593438081468\n",
      "iteration 8451train_loss: 0.4268742801217289 test_loss: 0.4604593604912045\n",
      "iteration 8452train_loss: 0.42687428011698786 test_loss: 0.4604593771643026\n",
      "iteration 8453train_loss: 0.42687428011225237 test_loss: 0.46045939382744683\n",
      "iteration 8454train_loss: 0.42687428010752265 test_loss: 0.4604594104806433\n",
      "iteration 8455train_loss: 0.42687428010279865 test_loss: 0.4604594271238978\n",
      "iteration 8456train_loss: 0.4268742800980802 test_loss: 0.46045944375721637\n",
      "iteration 8457train_loss: 0.4268742800933673 test_loss: 0.4604594603806049\n",
      "iteration 8458train_loss: 0.4268742800886601 test_loss: 0.4604594769940693\n",
      "iteration 8459train_loss: 0.4268742800839585 test_loss: 0.46045949359761573\n",
      "iteration 8460train_loss: 0.42687428007926254 test_loss: 0.46045951019124975\n",
      "iteration 8461train_loss: 0.4268742800745722 test_loss: 0.4604595267749774\n",
      "iteration 8462train_loss: 0.4268742800698875 test_loss: 0.46045954334880473\n",
      "iteration 8463train_loss: 0.4268742800652083 test_loss: 0.46045955991273746\n",
      "iteration 8464train_loss: 0.4268742800605347 test_loss: 0.4604595764667816\n",
      "iteration 8465train_loss: 0.4268742800558667 test_loss: 0.46045959301094314\n",
      "iteration 8466train_loss: 0.42687428005120437 test_loss: 0.46045960954522774\n",
      "iteration 8467train_loss: 0.42687428004654737 test_loss: 0.46045962606964147\n",
      "iteration 8468train_loss: 0.4268742800418961 test_loss: 0.46045964258419014\n",
      "iteration 8469train_loss: 0.4268742800372504 test_loss: 0.4604596590888797\n",
      "iteration 8470train_loss: 0.42687428003261024 test_loss: 0.46045967558371614\n",
      "iteration 8471train_loss: 0.42687428002797556 test_loss: 0.4604596920687051\n",
      "iteration 8472train_loss: 0.42687428002334654 test_loss: 0.4604597085438526\n",
      "iteration 8473train_loss: 0.42687428001872285 test_loss: 0.4604597250091645\n",
      "iteration 8474train_loss: 0.4268742800141049 test_loss: 0.46045974146464663\n",
      "iteration 8475train_loss: 0.42687428000949235 test_loss: 0.4604597579103049\n",
      "iteration 8476train_loss: 0.42687428000488525 test_loss: 0.4604597743461452\n",
      "iteration 8477train_loss: 0.4268742800002837 test_loss: 0.4604597907721733\n",
      "iteration 8478train_loss: 0.4268742799956876 test_loss: 0.4604598071883952\n",
      "iteration 8479train_loss: 0.42687427999109706 test_loss: 0.4604598235948166\n",
      "iteration 8480train_loss: 0.426874279986512 test_loss: 0.4604598399914434\n",
      "iteration 8481train_loss: 0.4268742799819324 test_loss: 0.4604598563782815\n",
      "iteration 8482train_loss: 0.4268742799773583 test_loss: 0.4604598727553367\n",
      "iteration 8483train_loss: 0.42687427997278965 test_loss: 0.46045988912261476\n",
      "iteration 8484train_loss: 0.42687427996822647 test_loss: 0.4604599054801217\n",
      "iteration 8485train_loss: 0.42687427996366867 test_loss: 0.46045992182786327\n",
      "iteration 8486train_loss: 0.42687427995911637 test_loss: 0.4604599381658452\n",
      "iteration 8487train_loss: 0.42687427995456945 test_loss: 0.46045995449407345\n",
      "iteration 8488train_loss: 0.426874279950028 test_loss: 0.4604599708125538\n",
      "iteration 8489train_loss: 0.42687427994549204 test_loss: 0.46045998712129205\n",
      "iteration 8490train_loss: 0.42687427994096133 test_loss: 0.460460003420294\n",
      "iteration 8491train_loss: 0.4268742799364361 test_loss: 0.4604600197095655\n",
      "iteration 8492train_loss: 0.4268742799319164 test_loss: 0.46046003598911234\n",
      "iteration 8493train_loss: 0.42687427992740196 test_loss: 0.46046005225894027\n",
      "iteration 8494train_loss: 0.42687427992289295 test_loss: 0.4604600685190553\n",
      "iteration 8495train_loss: 0.4268742799183893 test_loss: 0.46046008476946293\n",
      "iteration 8496train_loss: 0.4268742799138911 test_loss: 0.46046010101016926\n",
      "iteration 8497train_loss: 0.4268742799093982 test_loss: 0.4604601172411798\n",
      "iteration 8498train_loss: 0.4268742799049107 test_loss: 0.46046013346250053\n",
      "iteration 8499train_loss: 0.42687427990042853 test_loss: 0.46046014967413706\n",
      "iteration 8500train_loss: 0.4268742798959517 test_loss: 0.4604601658760954\n",
      "iteration 8501train_loss: 0.4268742798914803 test_loss: 0.46046018206838113\n",
      "iteration 8502train_loss: 0.4268742798870142 test_loss: 0.46046019825100015\n",
      "iteration 8503train_loss: 0.4268742798825534 test_loss: 0.4604602144239581\n",
      "iteration 8504train_loss: 0.4268742798780979 test_loss: 0.4604602305872608\n",
      "iteration 8505train_loss: 0.42687427987364784 test_loss: 0.4604602467409143\n",
      "iteration 8506train_loss: 0.42687427986920307 test_loss: 0.46046026288492387\n",
      "iteration 8507train_loss: 0.4268742798647635 test_loss: 0.4604602790192956\n",
      "iteration 8508train_loss: 0.4268742798603293 test_loss: 0.460460295144035\n",
      "iteration 8509train_loss: 0.42687427985590043 test_loss: 0.4604603112591481\n",
      "iteration 8510train_loss: 0.42687427985147675 test_loss: 0.46046032736464043\n",
      "iteration 8511train_loss: 0.4268742798470584 test_loss: 0.4604603434605178\n",
      "iteration 8512train_loss: 0.4268742798426454 test_loss: 0.460460359546786\n",
      "iteration 8513train_loss: 0.4268742798382375 test_loss: 0.4604603756234507\n",
      "iteration 8514train_loss: 0.426874279833835 test_loss: 0.4604603916905177\n",
      "iteration 8515train_loss: 0.42687427982943776 test_loss: 0.46046040774799263\n",
      "iteration 8516train_loss: 0.42687427982504567 test_loss: 0.4604604237958812\n",
      "iteration 8517train_loss: 0.42687427982065895 test_loss: 0.46046043983418933\n",
      "iteration 8518train_loss: 0.42687427981627735 test_loss: 0.46046045586292256\n",
      "iteration 8519train_loss: 0.42687427981190107 test_loss: 0.4604604718820868\n",
      "iteration 8520train_loss: 0.42687427980753 test_loss: 0.4604604878916874\n",
      "iteration 8521train_loss: 0.4268742798031641 test_loss: 0.46046050389173043\n",
      "iteration 8522train_loss: 0.4268742797988035 test_loss: 0.46046051988222136\n",
      "iteration 8523train_loss: 0.4268742797944481 test_loss: 0.4604605358631661\n",
      "iteration 8524train_loss: 0.4268742797900978 test_loss: 0.46046055183457013\n",
      "iteration 8525train_loss: 0.4268742797857527 test_loss: 0.46046056779643924\n",
      "iteration 8526train_loss: 0.4268742797814129 test_loss: 0.4604605837487792\n",
      "iteration 8527train_loss: 0.4268742797770782 test_loss: 0.46046059969159564\n",
      "iteration 8528train_loss: 0.4268742797727487 test_loss: 0.46046061562489415\n",
      "iteration 8529train_loss: 0.4268742797684244 test_loss: 0.4604606315486806\n",
      "iteration 8530train_loss: 0.4268742797641052 test_loss: 0.4604606474629605\n",
      "iteration 8531train_loss: 0.42687427975979125 test_loss: 0.46046066336773955\n",
      "iteration 8532train_loss: 0.42687427975548237 test_loss: 0.46046067926302353\n",
      "iteration 8533train_loss: 0.4268742797511787 test_loss: 0.460460695148818\n",
      "iteration 8534train_loss: 0.42687427974688014 test_loss: 0.46046071102512864\n",
      "iteration 8535train_loss: 0.42687427974258674 test_loss: 0.4604607268919612\n",
      "iteration 8536train_loss: 0.4268742797382984 test_loss: 0.46046074274932125\n",
      "iteration 8537train_loss: 0.42687427973401526 test_loss: 0.46046075859721447\n",
      "iteration 8538train_loss: 0.4268742797297371 test_loss: 0.4604607744356464\n",
      "iteration 8539train_loss: 0.4268742797254642 test_loss: 0.46046079026462294\n",
      "iteration 8540train_loss: 0.4268742797211962 test_loss: 0.46046080608414963\n",
      "iteration 8541train_loss: 0.4268742797169335 test_loss: 0.46046082189423193\n",
      "iteration 8542train_loss: 0.4268742797126758 test_loss: 0.46046083769487556\n",
      "iteration 8543train_loss: 0.42687427970842323 test_loss: 0.4604608534860864\n",
      "iteration 8544train_loss: 0.42687427970417574 test_loss: 0.4604608692678698\n",
      "iteration 8545train_loss: 0.4268742796999333 test_loss: 0.4604608850402314\n",
      "iteration 8546train_loss: 0.42687427969569597 test_loss: 0.4604609008031771\n",
      "iteration 8547train_loss: 0.42687427969146363 test_loss: 0.4604609165567122\n",
      "iteration 8548train_loss: 0.4268742796872364 test_loss: 0.4604609323008424\n",
      "iteration 8549train_loss: 0.4268742796830141 test_loss: 0.4604609480355734\n",
      "iteration 8550train_loss: 0.42687427967879693 test_loss: 0.46046096376091084\n",
      "iteration 8551train_loss: 0.42687427967458474 test_loss: 0.4604609794768602\n",
      "iteration 8552train_loss: 0.4268742796703777 test_loss: 0.46046099518342715\n",
      "iteration 8553train_loss: 0.4268742796661757 test_loss: 0.4604610108806172\n",
      "iteration 8554train_loss: 0.4268742796619786 test_loss: 0.4604610265684361\n",
      "iteration 8555train_loss: 0.4268742796577865 test_loss: 0.4604610422468893\n",
      "iteration 8556train_loss: 0.42687427965359953 test_loss: 0.4604610579159826\n",
      "iteration 8557train_loss: 0.4268742796494174 test_loss: 0.46046107357572136\n",
      "iteration 8558train_loss: 0.42687427964524033 test_loss: 0.4604610892261112\n",
      "iteration 8559train_loss: 0.42687427964106833 test_loss: 0.4604611048671578\n",
      "iteration 8560train_loss: 0.4268742796369012 test_loss: 0.4604611204988667\n",
      "iteration 8561train_loss: 0.42687427963273916 test_loss: 0.4604611361212435\n",
      "iteration 8562train_loss: 0.42687427962858204 test_loss: 0.4604611517342937\n",
      "iteration 8563train_loss: 0.4268742796244298 test_loss: 0.460461167338023\n",
      "iteration 8564train_loss: 0.4268742796202826 test_loss: 0.46046118293243665\n",
      "iteration 8565train_loss: 0.42687427961614033 test_loss: 0.4604611985175406\n",
      "iteration 8566train_loss: 0.4268742796120031 test_loss: 0.4604612140933401\n",
      "iteration 8567train_loss: 0.4268742796078708 test_loss: 0.46046122965984093\n",
      "iteration 8568train_loss: 0.42687427960374325 test_loss: 0.4604612452170485\n",
      "iteration 8569train_loss: 0.4268742795996207 test_loss: 0.4604612607649685\n",
      "iteration 8570train_loss: 0.42687427959550317 test_loss: 0.4604612763036062\n",
      "iteration 8571train_loss: 0.42687427959139046 test_loss: 0.4604612918329676\n",
      "iteration 8572train_loss: 0.42687427958728275 test_loss: 0.46046130735305774\n",
      "iteration 8573train_loss: 0.4268742795831798 test_loss: 0.46046132286388247\n",
      "iteration 8574train_loss: 0.4268742795790819 test_loss: 0.46046133836544717\n",
      "iteration 8575train_loss: 0.4268742795749888 test_loss: 0.46046135385775744\n",
      "iteration 8576train_loss: 0.4268742795709006 test_loss: 0.46046136934081877\n",
      "iteration 8577train_loss: 0.4268742795668174 test_loss: 0.4604613848146367\n",
      "iteration 8578train_loss: 0.42687427956273893 test_loss: 0.4604614002792167\n",
      "iteration 8579train_loss: 0.4268742795586654 test_loss: 0.46046141573456445\n",
      "iteration 8580train_loss: 0.4268742795545967 test_loss: 0.4604614311806853\n",
      "iteration 8581train_loss: 0.426874279550533 test_loss: 0.46046144661758476\n",
      "iteration 8582train_loss: 0.42687427954647406 test_loss: 0.4604614620452684\n",
      "iteration 8583train_loss: 0.4268742795424198 test_loss: 0.4604614774637416\n",
      "iteration 8584train_loss: 0.4268742795383706 test_loss: 0.46046149287301\n",
      "iteration 8585train_loss: 0.42687427953432616 test_loss: 0.46046150827307913\n",
      "iteration 8586train_loss: 0.4268742795302865 test_loss: 0.4604615236639543\n",
      "iteration 8587train_loss: 0.42687427952625173 test_loss: 0.46046153904564113\n",
      "iteration 8588train_loss: 0.4268742795222217 test_loss: 0.4604615544181451\n",
      "iteration 8589train_loss: 0.4268742795181967 test_loss: 0.4604615697814717\n",
      "iteration 8590train_loss: 0.4268742795141762 test_loss: 0.46046158513562635\n",
      "iteration 8591train_loss: 0.42687427951016066 test_loss: 0.4604616004806144\n",
      "iteration 8592train_loss: 0.42687427950615 test_loss: 0.4604616158164417\n",
      "iteration 8593train_loss: 0.426874279502144 test_loss: 0.46046163114311334\n",
      "iteration 8594train_loss: 0.4268742794981428 test_loss: 0.4604616464606349\n",
      "iteration 8595train_loss: 0.42687427949414636 test_loss: 0.46046166176901193\n",
      "iteration 8596train_loss: 0.4268742794901547 test_loss: 0.46046167706824975\n",
      "iteration 8597train_loss: 0.4268742794861678 test_loss: 0.46046169235835405\n",
      "iteration 8598train_loss: 0.42687427948218576 test_loss: 0.4604617076393301\n",
      "iteration 8599train_loss: 0.42687427947820844 test_loss: 0.4604617229111832\n",
      "iteration 8600train_loss: 0.4268742794742358 test_loss: 0.4604617381739192\n",
      "iteration 8601train_loss: 0.426874279470268 test_loss: 0.4604617534275432\n",
      "iteration 8602train_loss: 0.42687427946630485 test_loss: 0.4604617686720608\n",
      "iteration 8603train_loss: 0.4268742794623465 test_loss: 0.4604617839074774\n",
      "iteration 8604train_loss: 0.42687427945839285 test_loss: 0.46046179913379837\n",
      "iteration 8605train_loss: 0.4268742794544439 test_loss: 0.4604618143510292\n",
      "iteration 8606train_loss: 0.4268742794504996 test_loss: 0.4604618295591753\n",
      "iteration 8607train_loss: 0.4268742794465602 test_loss: 0.46046184475824214\n",
      "iteration 8608train_loss: 0.4268742794426254 test_loss: 0.46046185994823513\n",
      "iteration 8609train_loss: 0.42687427943869527 test_loss: 0.4604618751291596\n",
      "iteration 8610train_loss: 0.42687427943476985 test_loss: 0.4604618903010211\n",
      "iteration 8611train_loss: 0.42687427943084905 test_loss: 0.460461905463825\n",
      "iteration 8612train_loss: 0.42687427942693307 test_loss: 0.4604619206175765\n",
      "iteration 8613train_loss: 0.4268742794230217 test_loss: 0.4604619357622812\n",
      "iteration 8614train_loss: 0.42687427941911493 test_loss: 0.46046195089794456\n",
      "iteration 8615train_loss: 0.4268742794152129 test_loss: 0.46046196602457184\n",
      "iteration 8616train_loss: 0.4268742794113155 test_loss: 0.4604619811421686\n",
      "iteration 8617train_loss: 0.4268742794074228 test_loss: 0.46046199625073997\n",
      "iteration 8618train_loss: 0.42687427940353473 test_loss: 0.4604620113502916\n",
      "iteration 8619train_loss: 0.42687427939965134 test_loss: 0.46046202644082856\n",
      "iteration 8620train_loss: 0.42687427939577255 test_loss: 0.46046204152235665\n",
      "iteration 8621train_loss: 0.42687427939189837 test_loss: 0.4604620565948809\n",
      "iteration 8622train_loss: 0.4268742793880289 test_loss: 0.4604620716584069\n",
      "iteration 8623train_loss: 0.4268742793841639 test_loss: 0.4604620867129398\n",
      "iteration 8624train_loss: 0.42687427938030365 test_loss: 0.46046210175848523\n",
      "iteration 8625train_loss: 0.42687427937644795 test_loss: 0.46046211679504834\n",
      "iteration 8626train_loss: 0.42687427937259687 test_loss: 0.4604621318226347\n",
      "iteration 8627train_loss: 0.4268742793687503 test_loss: 0.4604621468412494\n",
      "iteration 8628train_loss: 0.42687427936490846 test_loss: 0.460462161850898\n",
      "iteration 8629train_loss: 0.42687427936107114 test_loss: 0.4604621768515858\n",
      "iteration 8630train_loss: 0.4268742793572384 test_loss: 0.4604621918433182\n",
      "iteration 8631train_loss: 0.4268742793534102 test_loss: 0.4604622068261004\n",
      "iteration 8632train_loss: 0.42687427934958666 test_loss: 0.46046222179993784\n",
      "iteration 8633train_loss: 0.42687427934576766 test_loss: 0.4604622367648359\n",
      "iteration 8634train_loss: 0.4268742793419532 test_loss: 0.46046225172079985\n",
      "iteration 8635train_loss: 0.42687427933814337 test_loss: 0.460462266667835\n",
      "iteration 8636train_loss: 0.426874279334338 test_loss: 0.4604622816059468\n",
      "iteration 8637train_loss: 0.4268742793305373 test_loss: 0.4604622965351404\n",
      "iteration 8638train_loss: 0.42687427932674094 test_loss: 0.4604623114554212\n",
      "iteration 8639train_loss: 0.4268742793229493 test_loss: 0.46046232636679474\n",
      "iteration 8640train_loss: 0.42687427931916205 test_loss: 0.46046234126926594\n",
      "iteration 8641train_loss: 0.42687427931537936 test_loss: 0.46046235616284037\n",
      "iteration 8642train_loss: 0.4268742793116012 test_loss: 0.46046237104752336\n",
      "iteration 8643train_loss: 0.42687427930782756 test_loss: 0.46046238592332006\n",
      "iteration 8644train_loss: 0.42687427930405836 test_loss: 0.46046240079023587\n",
      "iteration 8645train_loss: 0.42687427930029376 test_loss: 0.46046241564827606\n",
      "iteration 8646train_loss: 0.4268742792965336 test_loss: 0.46046243049744606\n",
      "iteration 8647train_loss: 0.42687427929277794 test_loss: 0.4604624453377509\n",
      "iteration 8648train_loss: 0.4268742792890268 test_loss: 0.46046246016919606\n",
      "iteration 8649train_loss: 0.42687427928528004 test_loss: 0.4604624749917868\n",
      "iteration 8650train_loss: 0.42687427928153787 test_loss: 0.4604624898055284\n",
      "iteration 8651train_loss: 0.4268742792778002 test_loss: 0.46046250461042615\n",
      "iteration 8652train_loss: 0.42687427927406685 test_loss: 0.46046251940648525\n",
      "iteration 8653train_loss: 0.426874279270338 test_loss: 0.46046253419371114\n",
      "iteration 8654train_loss: 0.42687427926661364 test_loss: 0.4604625489721089\n",
      "iteration 8655train_loss: 0.42687427926289373 test_loss: 0.46046256374168404\n",
      "iteration 8656train_loss: 0.4268742792591782 test_loss: 0.46046257850244166\n",
      "iteration 8657train_loss: 0.4268742792554672 test_loss: 0.46046259325438693\n",
      "iteration 8658train_loss: 0.4268742792517605 test_loss: 0.46046260799752525\n",
      "iteration 8659train_loss: 0.4268742792480584 test_loss: 0.46046262273186195\n",
      "iteration 8660train_loss: 0.42687427924436067 test_loss: 0.4604626374574021\n",
      "iteration 8661train_loss: 0.4268742792406673 test_loss: 0.4604626521741511\n",
      "iteration 8662train_loss: 0.4268742792369783 test_loss: 0.460462666882114\n",
      "iteration 8663train_loss: 0.4268742792332938 test_loss: 0.4604626815812964\n",
      "iteration 8664train_loss: 0.4268742792296137 test_loss: 0.4604626962717031\n",
      "iteration 8665train_loss: 0.4268742792259379 test_loss: 0.46046271095333974\n",
      "iteration 8666train_loss: 0.42687427922226656 test_loss: 0.4604627256262114\n",
      "iteration 8667train_loss: 0.42687427921859955 test_loss: 0.4604627402903231\n",
      "iteration 8668train_loss: 0.4268742792149369 test_loss: 0.46046275494568034\n",
      "iteration 8669train_loss: 0.4268742792112787 test_loss: 0.46046276959228827\n",
      "iteration 8670train_loss: 0.42687427920762494 test_loss: 0.46046278423015213\n",
      "iteration 8671train_loss: 0.42687427920397536 test_loss: 0.4604627988592771\n",
      "iteration 8672train_loss: 0.4268742792003303 test_loss: 0.4604628134796685\n",
      "iteration 8673train_loss: 0.4268742791966896 test_loss: 0.4604628280913314\n",
      "iteration 8674train_loss: 0.42687427919305304 test_loss: 0.460462842694271\n",
      "iteration 8675train_loss: 0.426874279189421 test_loss: 0.46046285728849257\n",
      "iteration 8676train_loss: 0.42687427918579335 test_loss: 0.4604628718740014\n",
      "iteration 8677train_loss: 0.4268742791821699 test_loss: 0.46046288645080263\n",
      "iteration 8678train_loss: 0.42687427917855075 test_loss: 0.46046290101890136\n",
      "iteration 8679train_loss: 0.426874279174936 test_loss: 0.46046291557830293\n",
      "iteration 8680train_loss: 0.42687427917132553 test_loss: 0.46046293012901235\n",
      "iteration 8681train_loss: 0.4268742791677194 test_loss: 0.460462944671035\n",
      "iteration 8682train_loss: 0.4268742791641175 test_loss: 0.4604629592043761\n",
      "iteration 8683train_loss: 0.42687427916052 test_loss: 0.46046297372904055\n",
      "iteration 8684train_loss: 0.4268742791569267 test_loss: 0.4604629882450338\n",
      "iteration 8685train_loss: 0.4268742791533378 test_loss: 0.4604630027523609\n",
      "iteration 8686train_loss: 0.42687427914975323 test_loss: 0.460463017251027\n",
      "iteration 8687train_loss: 0.4268742791461728 test_loss: 0.4604630317410373\n",
      "iteration 8688train_loss: 0.4268742791425967 test_loss: 0.4604630462223971\n",
      "iteration 8689train_loss: 0.42687427913902487 test_loss: 0.46046306069511145\n",
      "iteration 8690train_loss: 0.4268742791354572 test_loss: 0.46046307515918544\n",
      "iteration 8691train_loss: 0.4268742791318941 test_loss: 0.46046308961462434\n",
      "iteration 8692train_loss: 0.4268742791283349 test_loss: 0.46046310406143326\n",
      "iteration 8693train_loss: 0.42687427912478015 test_loss: 0.46046311849961735\n",
      "iteration 8694train_loss: 0.4268742791212295 test_loss: 0.4604631329291818\n",
      "iteration 8695train_loss: 0.4268742791176832 test_loss: 0.46046314735013155\n",
      "iteration 8696train_loss: 0.42687427911414105 test_loss: 0.4604631617624721\n",
      "iteration 8697train_loss: 0.4268742791106033 test_loss: 0.4604631761662083\n",
      "iteration 8698train_loss: 0.42687427910706965 test_loss: 0.4604631905613455\n",
      "iteration 8699train_loss: 0.42687427910354026 test_loss: 0.46046320494788867\n",
      "iteration 8700train_loss: 0.4268742791000151 test_loss: 0.4604632193258429\n",
      "iteration 8701train_loss: 0.42687427909649406 test_loss: 0.4604632336952135\n",
      "iteration 8702train_loss: 0.4268742790929773 test_loss: 0.4604632480560055\n",
      "iteration 8703train_loss: 0.4268742790894646 test_loss: 0.460463262408224\n",
      "iteration 8704train_loss: 0.4268742790859563 test_loss: 0.4604632767518741\n",
      "iteration 8705train_loss: 0.42687427908245207 test_loss: 0.4604632910869611\n",
      "iteration 8706train_loss: 0.42687427907895215 test_loss: 0.46046330541348984\n",
      "iteration 8707train_loss: 0.4268742790754562 test_loss: 0.46046331973146565\n",
      "iteration 8708train_loss: 0.4268742790719646 test_loss: 0.46046333404089346\n",
      "iteration 8709train_loss: 0.4268742790684771 test_loss: 0.4604633483417785\n",
      "iteration 8710train_loss: 0.42687427906499376 test_loss: 0.46046336263412574\n",
      "iteration 8711train_loss: 0.42687427906151465 test_loss: 0.4604633769179405\n",
      "iteration 8712train_loss: 0.4268742790580396 test_loss: 0.46046339119322766\n",
      "iteration 8713train_loss: 0.42687427905456876 test_loss: 0.46046340545999237\n",
      "iteration 8714train_loss: 0.426874279051102 test_loss: 0.4604634197182397\n",
      "iteration 8715train_loss: 0.42687427904763947 test_loss: 0.4604634339679749\n",
      "iteration 8716train_loss: 0.42687427904418096 test_loss: 0.46046344820920276\n",
      "iteration 8717train_loss: 0.4268742790407267 test_loss: 0.46046346244192854\n",
      "iteration 8718train_loss: 0.42687427903727654 test_loss: 0.46046347666615745\n",
      "iteration 8719train_loss: 0.4268742790338304 test_loss: 0.46046349088189414\n",
      "iteration 8720train_loss: 0.42687427903038855 test_loss: 0.46046350508914413\n",
      "iteration 8721train_loss: 0.42687427902695063 test_loss: 0.4604635192879123\n",
      "iteration 8722train_loss: 0.4268742790235169 test_loss: 0.46046353347820373\n",
      "iteration 8723train_loss: 0.4268742790200873 test_loss: 0.4604635476600234\n",
      "iteration 8724train_loss: 0.4268742790166618 test_loss: 0.46046356183337644\n",
      "iteration 8725train_loss: 0.42687427901324027 test_loss: 0.4604635759982679\n",
      "iteration 8726train_loss: 0.4268742790098229 test_loss: 0.4604635901547029\n",
      "iteration 8727train_loss: 0.42687427900640956 test_loss: 0.46046360430268624\n",
      "iteration 8728train_loss: 0.4268742790030004 test_loss: 0.4604636184422232\n",
      "iteration 8729train_loss: 0.4268742789995953 test_loss: 0.46046363257331885\n",
      "iteration 8730train_loss: 0.4268742789961941 test_loss: 0.4604636466959781\n",
      "iteration 8731train_loss: 0.42687427899279723 test_loss: 0.46046366081020607\n",
      "iteration 8732train_loss: 0.4268742789894042 test_loss: 0.4604636749160076\n",
      "iteration 8733train_loss: 0.42687427898601527 test_loss: 0.460463689013388\n",
      "iteration 8734train_loss: 0.4268742789826305 test_loss: 0.4604637031023521\n",
      "iteration 8735train_loss: 0.4268742789792496 test_loss: 0.46046371718290496\n",
      "iteration 8736train_loss: 0.4268742789758729 test_loss: 0.4604637312550518\n",
      "iteration 8737train_loss: 0.42687427897250013 test_loss: 0.46046374531879736\n",
      "iteration 8738train_loss: 0.4268742789691314 test_loss: 0.4604637593741467\n",
      "iteration 8739train_loss: 0.4268742789657667 test_loss: 0.46046377342110495\n",
      "iteration 8740train_loss: 0.426874278962406 test_loss: 0.46046378745967714\n",
      "iteration 8741train_loss: 0.4268742789590494 test_loss: 0.4604638014898681\n",
      "iteration 8742train_loss: 0.42687427895569674 test_loss: 0.46046381551168303\n",
      "iteration 8743train_loss: 0.4268742789523481 test_loss: 0.46046382952512666\n",
      "iteration 8744train_loss: 0.42687427894900337 test_loss: 0.4604638435302044\n",
      "iteration 8745train_loss: 0.4268742789456627 test_loss: 0.4604638575269208\n",
      "iteration 8746train_loss: 0.42687427894232605 test_loss: 0.4604638715152812\n",
      "iteration 8747train_loss: 0.4268742789389933 test_loss: 0.4604638854952904\n",
      "iteration 8748train_loss: 0.42687427893566465 test_loss: 0.4604638994669534\n",
      "iteration 8749train_loss: 0.4268742789323399 test_loss: 0.4604639134302751\n",
      "iteration 8750train_loss: 0.4268742789290192 test_loss: 0.46046392738526076\n",
      "iteration 8751train_loss: 0.4268742789257023 test_loss: 0.4604639413319152\n",
      "iteration 8752train_loss: 0.4268742789223895 test_loss: 0.46046395527024325\n",
      "iteration 8753train_loss: 0.42687427891908064 test_loss: 0.4604639692002501\n",
      "iteration 8754train_loss: 0.4268742789157758 test_loss: 0.4604639831219407\n",
      "iteration 8755train_loss: 0.42687427891247476 test_loss: 0.46046399703531976\n",
      "iteration 8756train_loss: 0.42687427890917773 test_loss: 0.4604640109403925\n",
      "iteration 8757train_loss: 0.42687427890588453 test_loss: 0.4604640248371639\n",
      "iteration 8758train_loss: 0.4268742789025954 test_loss: 0.46046403872563885\n",
      "iteration 8759train_loss: 0.4268742788993102 test_loss: 0.4604640526058221\n",
      "iteration 8760train_loss: 0.42687427889602886 test_loss: 0.4604640664777188\n",
      "iteration 8761train_loss: 0.42687427889275154 test_loss: 0.4604640803413339\n",
      "iteration 8762train_loss: 0.426874278889478 test_loss: 0.4604640941966724\n",
      "iteration 8763train_loss: 0.4268742788862083 test_loss: 0.4604641080437391\n",
      "iteration 8764train_loss: 0.4268742788829427 test_loss: 0.4604641218825389\n",
      "iteration 8765train_loss: 0.42687427887968094 test_loss: 0.46046413571307687\n",
      "iteration 8766train_loss: 0.42687427887642304 test_loss: 0.4604641495353579\n",
      "iteration 8767train_loss: 0.42687427887316903 test_loss: 0.460464163349387\n",
      "iteration 8768train_loss: 0.4268742788699189 test_loss: 0.4604641771551689\n",
      "iteration 8769train_loss: 0.42687427886667273 test_loss: 0.46046419095270863\n",
      "iteration 8770train_loss: 0.4268742788634304 test_loss: 0.4604642047420113\n",
      "iteration 8771train_loss: 0.42687427886019186 test_loss: 0.46046421852308145\n",
      "iteration 8772train_loss: 0.4268742788569572 test_loss: 0.4604642322959242\n",
      "iteration 8773train_loss: 0.4268742788537265 test_loss: 0.46046424606054454\n",
      "iteration 8774train_loss: 0.4268742788504996 test_loss: 0.4604642598169472\n",
      "iteration 8775train_loss: 0.42687427884727663 test_loss: 0.46046427356513725\n",
      "iteration 8776train_loss: 0.42687427884405743 test_loss: 0.4604642873051194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8777train_loss: 0.4268742788408421 test_loss: 0.4604643010368988\n",
      "iteration 8778train_loss: 0.42687427883763057 test_loss: 0.4604643147604801\n",
      "iteration 8779train_loss: 0.4268742788344229 test_loss: 0.4604643284758683\n",
      "iteration 8780train_loss: 0.42687427883121903 test_loss: 0.4604643421830685\n",
      "iteration 8781train_loss: 0.42687427882801904 test_loss: 0.4604643558820852\n",
      "iteration 8782train_loss: 0.4268742788248228 test_loss: 0.4604643695729234\n",
      "iteration 8783train_loss: 0.4268742788216305 test_loss: 0.46046438325558814\n",
      "iteration 8784train_loss: 0.426874278818442 test_loss: 0.4604643969300843\n",
      "iteration 8785train_loss: 0.4268742788152572 test_loss: 0.4604644105964167\n",
      "iteration 8786train_loss: 0.42687427881207624 test_loss: 0.46046442425458994\n",
      "iteration 8787train_loss: 0.426874278808899 test_loss: 0.4604644379046092\n",
      "iteration 8788train_loss: 0.42687427880572565 test_loss: 0.46046445154647936\n",
      "iteration 8789train_loss: 0.4268742788025562 test_loss: 0.46046446518020523\n",
      "iteration 8790train_loss: 0.4268742787993904 test_loss: 0.46046447880579155\n",
      "iteration 8791train_loss: 0.4268742787962283 test_loss: 0.46046449242324333\n",
      "iteration 8792train_loss: 0.4268742787930701 test_loss: 0.46046450603256533\n",
      "iteration 8793train_loss: 0.4268742787899157 test_loss: 0.46046451963376256\n",
      "iteration 8794train_loss: 0.4268742787867649 test_loss: 0.4604645332268396\n",
      "iteration 8795train_loss: 0.426874278783618 test_loss: 0.4604645468118015\n",
      "iteration 8796train_loss: 0.42687427878047485 test_loss: 0.460464560388653\n",
      "iteration 8797train_loss: 0.4268742787773354 test_loss: 0.46046457395739915\n",
      "iteration 8798train_loss: 0.4268742787741997 test_loss: 0.4604645875180445\n",
      "iteration 8799train_loss: 0.4268742787710678 test_loss: 0.4604646010705941\n",
      "iteration 8800train_loss: 0.42687427876793965 test_loss: 0.46046461461505267\n",
      "iteration 8801train_loss: 0.42687427876481515 test_loss: 0.46046462815142497\n",
      "iteration 8802train_loss: 0.42687427876169454 test_loss: 0.460464641679716\n",
      "iteration 8803train_loss: 0.42687427875857753 test_loss: 0.46046465519993063\n",
      "iteration 8804train_loss: 0.4268742787554642 test_loss: 0.4604646687120735\n",
      "iteration 8805train_loss: 0.4268742787523547 test_loss: 0.4604646822161494\n",
      "iteration 8806train_loss: 0.4268742787492488 test_loss: 0.4604646957121632\n",
      "iteration 8807train_loss: 0.4268742787461467 test_loss: 0.46046470920011995\n",
      "iteration 8808train_loss: 0.42687427874304823 test_loss: 0.4604647226800241\n",
      "iteration 8809train_loss: 0.4268742787399535 test_loss: 0.4604647361518807\n",
      "iteration 8810train_loss: 0.42687427873686246 test_loss: 0.4604647496156945\n",
      "iteration 8811train_loss: 0.4268742787337752 test_loss: 0.4604647630714702\n",
      "iteration 8812train_loss: 0.4268742787306915 test_loss: 0.4604647765192127\n",
      "iteration 8813train_loss: 0.42687427872761147 test_loss: 0.4604647899589268\n",
      "iteration 8814train_loss: 0.42687427872453526 test_loss: 0.46046480339061724\n",
      "iteration 8815train_loss: 0.4268742787214626 test_loss: 0.46046481681428886\n",
      "iteration 8816train_loss: 0.42687427871839373 test_loss: 0.46046483022994633\n",
      "iteration 8817train_loss: 0.42687427871532835 test_loss: 0.4604648436375945\n",
      "iteration 8818train_loss: 0.42687427871226674 test_loss: 0.4604648570372382\n",
      "iteration 8819train_loss: 0.4268742787092088 test_loss: 0.46046487042888223\n",
      "iteration 8820train_loss: 0.42687427870615446 test_loss: 0.4604648838125313\n",
      "iteration 8821train_loss: 0.42687427870310385 test_loss: 0.4604648971881902\n",
      "iteration 8822train_loss: 0.42687427870005673 test_loss: 0.4604649105558637\n",
      "iteration 8823train_loss: 0.4268742786970134 test_loss: 0.46046492391555655\n",
      "iteration 8824train_loss: 0.42687427869397354 test_loss: 0.46046493726727356\n",
      "iteration 8825train_loss: 0.4268742786909375 test_loss: 0.46046495061101944\n",
      "iteration 8826train_loss: 0.426874278687905 test_loss: 0.460464963946799\n",
      "iteration 8827train_loss: 0.42687427868487615 test_loss: 0.4604649772746168\n",
      "iteration 8828train_loss: 0.4268742786818508 test_loss: 0.460464990594478\n",
      "iteration 8829train_loss: 0.4268742786788292 test_loss: 0.46046500390638695\n",
      "iteration 8830train_loss: 0.4268742786758111 test_loss: 0.4604650172103486\n",
      "iteration 8831train_loss: 0.4268742786727967 test_loss: 0.46046503050636767\n",
      "iteration 8832train_loss: 0.42687427866978594 test_loss: 0.4604650437944489\n",
      "iteration 8833train_loss: 0.4268742786667786 test_loss: 0.4604650570745969\n",
      "iteration 8834train_loss: 0.4268742786637749 test_loss: 0.46046507034681655\n",
      "iteration 8835train_loss: 0.42687427866077493 test_loss: 0.46046508361111255\n",
      "iteration 8836train_loss: 0.4268742786577784 test_loss: 0.46046509686748965\n",
      "iteration 8837train_loss: 0.42687427865478544 test_loss: 0.46046511011595254\n",
      "iteration 8838train_loss: 0.4268742786517961 test_loss: 0.4604651233565059\n",
      "iteration 8839train_loss: 0.42687427864881045 test_loss: 0.46046513658915467\n",
      "iteration 8840train_loss: 0.4268742786458282 test_loss: 0.46046514981390324\n",
      "iteration 8841train_loss: 0.42687427864284955 test_loss: 0.4604651630307565\n",
      "iteration 8842train_loss: 0.4268742786398745 test_loss: 0.4604651762397194\n",
      "iteration 8843train_loss: 0.42687427863690286 test_loss: 0.4604651894407962\n",
      "iteration 8844train_loss: 0.42687427863393496 test_loss: 0.4604652026339918\n",
      "iteration 8845train_loss: 0.42687427863097055 test_loss: 0.460465215819311\n",
      "iteration 8846train_loss: 0.42687427862800964 test_loss: 0.4604652289967584\n",
      "iteration 8847train_loss: 0.42687427862505223 test_loss: 0.4604652421663386\n",
      "iteration 8848train_loss: 0.42687427862209837 test_loss: 0.4604652553280566\n",
      "iteration 8849train_loss: 0.4268742786191482 test_loss: 0.4604652684819168\n",
      "iteration 8850train_loss: 0.42687427861620136 test_loss: 0.46046528162792405\n",
      "iteration 8851train_loss: 0.426874278613258 test_loss: 0.460465294766083\n",
      "iteration 8852train_loss: 0.4268742786103184 test_loss: 0.4604653078963983\n",
      "iteration 8853train_loss: 0.42687427860738214 test_loss: 0.46046532101887466\n",
      "iteration 8854train_loss: 0.4268742786044494 test_loss: 0.46046533413351665\n",
      "iteration 8855train_loss: 0.42687427860152016 test_loss: 0.4604653472403292\n",
      "iteration 8856train_loss: 0.4268742785985944 test_loss: 0.46046536033931673\n",
      "iteration 8857train_loss: 0.42687427859567223 test_loss: 0.4604653734304841\n",
      "iteration 8858train_loss: 0.4268742785927535 test_loss: 0.4604653865138359\n",
      "iteration 8859train_loss: 0.4268742785898382 test_loss: 0.4604653995893768\n",
      "iteration 8860train_loss: 0.4268742785869264 test_loss: 0.46046541265711133\n",
      "iteration 8861train_loss: 0.42687427858401816 test_loss: 0.4604654257170444\n",
      "iteration 8862train_loss: 0.42687427858111326 test_loss: 0.4604654387691806\n",
      "iteration 8863train_loss: 0.4268742785782119 test_loss: 0.46046545181352433\n",
      "iteration 8864train_loss: 0.42687427857531396 test_loss: 0.4604654648500806\n",
      "iteration 8865train_loss: 0.4268742785724196 test_loss: 0.46046547787885383\n",
      "iteration 8866train_loss: 0.42687427856952864 test_loss: 0.46046549089984873\n",
      "iteration 8867train_loss: 0.4268742785666411 test_loss: 0.46046550391306995\n",
      "iteration 8868train_loss: 0.426874278563757 test_loss: 0.46046551691852206\n",
      "iteration 8869train_loss: 0.4268742785608763 test_loss: 0.46046552991620987\n",
      "iteration 8870train_loss: 0.4268742785579992 test_loss: 0.4604655429061379\n",
      "iteration 8871train_loss: 0.42687427855512533 test_loss: 0.46046555588831073\n",
      "iteration 8872train_loss: 0.42687427855225507 test_loss: 0.46046556886273315\n",
      "iteration 8873train_loss: 0.4268742785493882 test_loss: 0.4604655818294095\n",
      "iteration 8874train_loss: 0.42687427854652465 test_loss: 0.46046559478834476\n",
      "iteration 8875train_loss: 0.4268742785436646 test_loss: 0.4604656077395434\n",
      "iteration 8876train_loss: 0.42687427854080795 test_loss: 0.4604656206830099\n",
      "iteration 8877train_loss: 0.42687427853795473 test_loss: 0.4604656336187491\n",
      "iteration 8878train_loss: 0.4268742785351049 test_loss: 0.46046564654676536\n",
      "iteration 8879train_loss: 0.4268742785322585 test_loss: 0.46046565946706364\n",
      "iteration 8880train_loss: 0.4268742785294155 test_loss: 0.4604656723796482\n",
      "iteration 8881train_loss: 0.42687427852657583 test_loss: 0.4604656852845238\n",
      "iteration 8882train_loss: 0.42687427852373955 test_loss: 0.46046569818169514\n",
      "iteration 8883train_loss: 0.4268742785209067 test_loss: 0.4604657110711667\n",
      "iteration 8884train_loss: 0.4268742785180773 test_loss: 0.46046572395294305\n",
      "iteration 8885train_loss: 0.4268742785152512 test_loss: 0.4604657368270288\n",
      "iteration 8886train_loss: 0.4268742785124284 test_loss: 0.46046574969342846\n",
      "iteration 8887train_loss: 0.42687427850960913 test_loss: 0.460465762552147\n",
      "iteration 8888train_loss: 0.4268742785067931 test_loss: 0.46046577540318856\n",
      "iteration 8889train_loss: 0.4268742785039805 test_loss: 0.46046578824655787\n",
      "iteration 8890train_loss: 0.42687427850117127 test_loss: 0.46046580108225954\n",
      "iteration 8891train_loss: 0.42687427849836534 test_loss: 0.4604658139102982\n",
      "iteration 8892train_loss: 0.42687427849556286 test_loss: 0.4604658267306783\n",
      "iteration 8893train_loss: 0.42687427849276355 test_loss: 0.46046583954340453\n",
      "iteration 8894train_loss: 0.42687427848996773 test_loss: 0.4604658523484814\n",
      "iteration 8895train_loss: 0.4268742784871752 test_loss: 0.4604658651459134\n",
      "iteration 8896train_loss: 0.426874278484386 test_loss: 0.46046587793570515\n",
      "iteration 8897train_loss: 0.42687427848160014 test_loss: 0.46046589071786137\n",
      "iteration 8898train_loss: 0.4268742784788176 test_loss: 0.4604659034923865\n",
      "iteration 8899train_loss: 0.4268742784760385 test_loss: 0.46046591625928485\n",
      "iteration 8900train_loss: 0.42687427847326254 test_loss: 0.4604659290185614\n",
      "iteration 8901train_loss: 0.42687427847049 test_loss: 0.46046594177022043\n",
      "iteration 8902train_loss: 0.42687427846772064 test_loss: 0.4604659545142666\n",
      "iteration 8903train_loss: 0.42687427846495474 test_loss: 0.4604659672507044\n",
      "iteration 8904train_loss: 0.4268742784621921 test_loss: 0.4604659799795383\n",
      "iteration 8905train_loss: 0.4268742784594326 test_loss: 0.46046599270077304\n",
      "iteration 8906train_loss: 0.4268742784566766 test_loss: 0.46046600541441296\n",
      "iteration 8907train_loss: 0.4268742784539238 test_loss: 0.4604660181204628\n",
      "iteration 8908train_loss: 0.4268742784511743 test_loss: 0.46046603081892684\n",
      "iteration 8909train_loss: 0.4268742784484282 test_loss: 0.4604660435098098\n",
      "iteration 8910train_loss: 0.42687427844568526 test_loss: 0.4604660561931161\n",
      "iteration 8911train_loss: 0.4268742784429456 test_loss: 0.4604660688688502\n",
      "iteration 8912train_loss: 0.4268742784402092 test_loss: 0.46046608153701685\n",
      "iteration 8913train_loss: 0.4268742784374761 test_loss: 0.4604660941976205\n",
      "iteration 8914train_loss: 0.42687427843474635 test_loss: 0.46046610685066547\n",
      "iteration 8915train_loss: 0.42687427843201964 test_loss: 0.4604661194961565\n",
      "iteration 8916train_loss: 0.42687427842929626 test_loss: 0.46046613213409804\n",
      "iteration 8917train_loss: 0.42687427842657627 test_loss: 0.4604661447644944\n",
      "iteration 8918train_loss: 0.4268742784238594 test_loss: 0.4604661573873503\n",
      "iteration 8919train_loss: 0.4268742784211459 test_loss: 0.4604661700026703\n",
      "iteration 8920train_loss: 0.42687427841843545 test_loss: 0.4604661826104587\n",
      "iteration 8921train_loss: 0.42687427841572834 test_loss: 0.4604661952107201\n",
      "iteration 8922train_loss: 0.42687427841302444 test_loss: 0.4604662078034589\n",
      "iteration 8923train_loss: 0.4268742784103239 test_loss: 0.4604662203886798\n",
      "iteration 8924train_loss: 0.42687427840762643 test_loss: 0.46046623296638706\n",
      "iteration 8925train_loss: 0.42687427840493225 test_loss: 0.46046624553658533\n",
      "iteration 8926train_loss: 0.4268742784022412 test_loss: 0.460466258099279\n",
      "iteration 8927train_loss: 0.4268742783995535 test_loss: 0.4604662706544725\n",
      "iteration 8928train_loss: 0.42687427839686887 test_loss: 0.4604662832021704\n",
      "iteration 8929train_loss: 0.42687427839418757 test_loss: 0.46046629574237724\n",
      "iteration 8930train_loss: 0.4268742783915094 test_loss: 0.4604663082750974\n",
      "iteration 8931train_loss: 0.42687427838883446 test_loss: 0.4604663208003353\n",
      "iteration 8932train_loss: 0.4268742783861626 test_loss: 0.46046633331809544\n",
      "iteration 8933train_loss: 0.4268742783834942 test_loss: 0.4604663458283824\n",
      "iteration 8934train_loss: 0.42687427838082875 test_loss: 0.4604663583312005\n",
      "iteration 8935train_loss: 0.4268742783781665 test_loss: 0.46046637082655417\n",
      "iteration 8936train_loss: 0.42687427837550745 test_loss: 0.46046638331444806\n",
      "iteration 8937train_loss: 0.4268742783728516 test_loss: 0.46046639579488635\n",
      "iteration 8938train_loss: 0.4268742783701989 test_loss: 0.4604664082678738\n",
      "iteration 8939train_loss: 0.4268742783675495 test_loss: 0.4604664207334147\n",
      "iteration 8940train_loss: 0.4268742783649031 test_loss: 0.4604664331915134\n",
      "iteration 8941train_loss: 0.42687427836225994 test_loss: 0.4604664456421745\n",
      "iteration 8942train_loss: 0.42687427835961983 test_loss: 0.4604664580854024\n",
      "iteration 8943train_loss: 0.426874278356983 test_loss: 0.46046647052120154\n",
      "iteration 8944train_loss: 0.42687427835434927 test_loss: 0.4604664829495762\n",
      "iteration 8945train_loss: 0.4268742783517186 test_loss: 0.4604664953705311\n",
      "iteration 8946train_loss: 0.42687427834909125 test_loss: 0.4604665077840704\n",
      "iteration 8947train_loss: 0.42687427834646696 test_loss: 0.4604665201901986\n",
      "iteration 8948train_loss: 0.4268742783438458 test_loss: 0.46046653258892034\n",
      "iteration 8949train_loss: 0.42687427834122765 test_loss: 0.4604665449802398\n",
      "iteration 8950train_loss: 0.4268742783386128 test_loss: 0.46046655736416153\n",
      "iteration 8951train_loss: 0.426874278336001 test_loss: 0.46046656974068967\n",
      "iteration 8952train_loss: 0.4268742783333922 test_loss: 0.46046658210982905\n",
      "iteration 8953train_loss: 0.4268742783307867 test_loss: 0.4604665944715838\n",
      "iteration 8954train_loss: 0.42687427832818436 test_loss: 0.46046660682595847\n",
      "iteration 8955train_loss: 0.4268742783255849 test_loss: 0.4604666191729572\n",
      "iteration 8956train_loss: 0.4268742783229887 test_loss: 0.4604666315125848\n",
      "iteration 8957train_loss: 0.42687427832039554 test_loss: 0.4604666438448454\n",
      "iteration 8958train_loss: 0.42687427831780556 test_loss: 0.4604666561697434\n",
      "iteration 8959train_loss: 0.42687427831521857 test_loss: 0.4604666684872834\n",
      "iteration 8960train_loss: 0.4268742783126347 test_loss: 0.4604666807974696\n",
      "iteration 8961train_loss: 0.4268742783100539 test_loss: 0.46046669310030636\n",
      "iteration 8962train_loss: 0.42687427830747626 test_loss: 0.46046670539579826\n",
      "iteration 8963train_loss: 0.4268742783049016 test_loss: 0.4604667176839494\n",
      "iteration 8964train_loss: 0.4268742783023301 test_loss: 0.4604667299647645\n",
      "iteration 8965train_loss: 0.42687427829976166 test_loss: 0.46046674223824774\n",
      "iteration 8966train_loss: 0.4268742782971962 test_loss: 0.4604667545044035\n",
      "iteration 8967train_loss: 0.42687427829463376 test_loss: 0.46046676676323617\n",
      "iteration 8968train_loss: 0.42687427829207464 test_loss: 0.4604667790147501\n",
      "iteration 8969train_loss: 0.4268742782895184 test_loss: 0.46046679125894985\n",
      "iteration 8970train_loss: 0.4268742782869652 test_loss: 0.4604668034958395\n",
      "iteration 8971train_loss: 0.4268742782844151 test_loss: 0.4604668157254236\n",
      "iteration 8972train_loss: 0.426874278281868 test_loss: 0.4604668279477063\n",
      "iteration 8973train_loss: 0.426874278279324 test_loss: 0.4604668401626923\n",
      "iteration 8974train_loss: 0.4268742782767829 test_loss: 0.46046685237038576\n",
      "iteration 8975train_loss: 0.42687427827424496 test_loss: 0.46046686457079106\n",
      "iteration 8976train_loss: 0.4268742782717101 test_loss: 0.4604668767639124\n",
      "iteration 8977train_loss: 0.4268742782691781 test_loss: 0.4604668889497544\n",
      "iteration 8978train_loss: 0.4268742782666493 test_loss: 0.4604669011283212\n",
      "iteration 8979train_loss: 0.4268742782641234 test_loss: 0.46046691329961725\n",
      "iteration 8980train_loss: 0.4268742782616005 test_loss: 0.46046692546364687\n",
      "iteration 8981train_loss: 0.4268742782590807 test_loss: 0.4604669376204143\n",
      "iteration 8982train_loss: 0.4268742782565638 test_loss: 0.460466949769924\n",
      "iteration 8983train_loss: 0.4268742782540501 test_loss: 0.46046696191218023\n",
      "iteration 8984train_loss: 0.4268742782515392 test_loss: 0.46046697404718734\n",
      "iteration 8985train_loss: 0.4268742782490314 test_loss: 0.46046698617494963\n",
      "iteration 8986train_loss: 0.4268742782465265 test_loss: 0.46046699829547155\n",
      "iteration 8987train_loss: 0.4268742782440247 test_loss: 0.46046701040875737\n",
      "iteration 8988train_loss: 0.42687427824152585 test_loss: 0.46046702251481125\n",
      "iteration 8989train_loss: 0.42687427823902996 test_loss: 0.4604670346136377\n",
      "iteration 8990train_loss: 0.42687427823653706 test_loss: 0.4604670467052409\n",
      "iteration 8991train_loss: 0.4268742782340473 test_loss: 0.46046705878962535\n",
      "iteration 8992train_loss: 0.4268742782315602 test_loss: 0.4604670708667952\n",
      "iteration 8993train_loss: 0.4268742782290763 test_loss: 0.4604670829367547\n",
      "iteration 8994train_loss: 0.4268742782265954 test_loss: 0.4604670949995084\n",
      "iteration 8995train_loss: 0.4268742782241173 test_loss: 0.4604671070550603\n",
      "iteration 8996train_loss: 0.42687427822164226 test_loss: 0.4604671191034148\n",
      "iteration 8997train_loss: 0.4268742782191702 test_loss: 0.4604671311445763\n",
      "iteration 8998train_loss: 0.4268742782167009 test_loss: 0.4604671431785492\n",
      "iteration 8999train_loss: 0.42687427821423474 test_loss: 0.4604671552053375\n",
      "iteration 9000train_loss: 0.4268742782117714 test_loss: 0.46046716722494563\n",
      "iteration 9001train_loss: 0.42687427820931106 test_loss: 0.46046717923737784\n",
      "iteration 9002train_loss: 0.42687427820685364 test_loss: 0.4604671912426384\n",
      "iteration 9003train_loss: 0.42687427820439916 test_loss: 0.46046720324073165\n",
      "iteration 9004train_loss: 0.4268742782019476 test_loss: 0.46046721523166195\n",
      "iteration 9005train_loss: 0.4268742781994991 test_loss: 0.4604672272154334\n",
      "iteration 9006train_loss: 0.4268742781970533 test_loss: 0.46046723919205035\n",
      "iteration 9007train_loss: 0.4268742781946106 test_loss: 0.46046725116151704\n",
      "iteration 9008train_loss: 0.4268742781921707 test_loss: 0.4604672631238379\n",
      "iteration 9009train_loss: 0.4268742781897338 test_loss: 0.46046727507901686\n",
      "iteration 9010train_loss: 0.4268742781872998 test_loss: 0.46046728702705847\n",
      "iteration 9011train_loss: 0.42687427818486867 test_loss: 0.46046729896796695\n",
      "iteration 9012train_loss: 0.42687427818244045 test_loss: 0.46046731090174653\n",
      "iteration 9013train_loss: 0.42687427818001517 test_loss: 0.46046732282840136\n",
      "iteration 9014train_loss: 0.4268742781775927 test_loss: 0.46046733474793583\n",
      "iteration 9015train_loss: 0.42687427817517315 test_loss: 0.4604673466603542\n",
      "iteration 9016train_loss: 0.4268742781727565 test_loss: 0.4604673585656606\n",
      "iteration 9017train_loss: 0.42687427817034285 test_loss: 0.4604673704638594\n",
      "iteration 9018train_loss: 0.4268742781679319 test_loss: 0.4604673823549546\n",
      "iteration 9019train_loss: 0.4268742781655239 test_loss: 0.4604673942389509\n",
      "iteration 9020train_loss: 0.4268742781631189 test_loss: 0.4604674061158521\n",
      "iteration 9021train_loss: 0.4268742781607166 test_loss: 0.4604674179856626\n",
      "iteration 9022train_loss: 0.42687427815831724 test_loss: 0.4604674298483867\n",
      "iteration 9023train_loss: 0.42687427815592066 test_loss: 0.4604674417040284\n",
      "iteration 9024train_loss: 0.4268742781535271 test_loss: 0.4604674535525923\n",
      "iteration 9025train_loss: 0.42687427815113627 test_loss: 0.46046746539408223\n",
      "iteration 9026train_loss: 0.4268742781487483 test_loss: 0.4604674772285026\n",
      "iteration 9027train_loss: 0.42687427814636314 test_loss: 0.46046748905585777\n",
      "iteration 9028train_loss: 0.426874278143981 test_loss: 0.46046750087615174\n",
      "iteration 9029train_loss: 0.4268742781416016 test_loss: 0.4604675126893888\n",
      "iteration 9030train_loss: 0.42687427813922507 test_loss: 0.4604675244955731\n",
      "iteration 9031train_loss: 0.4268742781368513 test_loss: 0.460467536294709\n",
      "iteration 9032train_loss: 0.4268742781344805 test_loss: 0.46046754808680057\n",
      "iteration 9033train_loss: 0.42687427813211243 test_loss: 0.4604675598718521\n",
      "iteration 9034train_loss: 0.42687427812974715 test_loss: 0.4604675716498677\n",
      "iteration 9035train_loss: 0.4268742781273848 test_loss: 0.46046758342085164\n",
      "iteration 9036train_loss: 0.4268742781250252 test_loss: 0.4604675951848082\n",
      "iteration 9037train_loss: 0.4268742781226685 test_loss: 0.4604676069417414\n",
      "iteration 9038train_loss: 0.42687427812031453 test_loss: 0.4604676186916555\n",
      "iteration 9039train_loss: 0.4268742781179633 test_loss: 0.46046763043455463\n",
      "iteration 9040train_loss: 0.42687427811561507 test_loss: 0.46046764217044317\n",
      "iteration 9041train_loss: 0.4268742781132696 test_loss: 0.4604676538993252\n",
      "iteration 9042train_loss: 0.42687427811092676 test_loss: 0.4604676656212049\n",
      "iteration 9043train_loss: 0.4268742781085869 test_loss: 0.46046767733608635\n",
      "iteration 9044train_loss: 0.4268742781062497 test_loss: 0.46046768904397384\n",
      "iteration 9045train_loss: 0.4268742781039154 test_loss: 0.46046770074487153\n",
      "iteration 9046train_loss: 0.42687427810158385 test_loss: 0.4604677124387836\n",
      "iteration 9047train_loss: 0.4268742780992551 test_loss: 0.46046772412571413\n",
      "iteration 9048train_loss: 0.4268742780969292 test_loss: 0.46046773580566747\n",
      "iteration 9049train_loss: 0.4268742780946059 test_loss: 0.4604677474786476\n",
      "iteration 9050train_loss: 0.42687427809228545 test_loss: 0.4604677591446588\n",
      "iteration 9051train_loss: 0.4268742780899678 test_loss: 0.46046777080370516\n",
      "iteration 9052train_loss: 0.42687427808765294 test_loss: 0.4604677824557909\n",
      "iteration 9053train_loss: 0.42687427808534084 test_loss: 0.46046779410092015\n",
      "iteration 9054train_loss: 0.42687427808303136 test_loss: 0.4604678057390969\n",
      "iteration 9055train_loss: 0.42687427808072476 test_loss: 0.4604678173703257\n",
      "iteration 9056train_loss: 0.42687427807842093 test_loss: 0.4604678289946103\n",
      "iteration 9057train_loss: 0.42687427807611983 test_loss: 0.4604678406119551\n",
      "iteration 9058train_loss: 0.42687427807382144 test_loss: 0.4604678522223641\n",
      "iteration 9059train_loss: 0.42687427807152584 test_loss: 0.4604678638258414\n",
      "iteration 9060train_loss: 0.426874278069233 test_loss: 0.4604678754223913\n",
      "iteration 9061train_loss: 0.4268742780669428 test_loss: 0.46046788701201785\n",
      "iteration 9062train_loss: 0.42687427806465544 test_loss: 0.4604678985947252\n",
      "iteration 9063train_loss: 0.4268742780623708 test_loss: 0.4604679101705174\n",
      "iteration 9064train_loss: 0.42687427806008876 test_loss: 0.46046792173939877\n",
      "iteration 9065train_loss: 0.4268742780578096 test_loss: 0.46046793330137326\n",
      "iteration 9066train_loss: 0.426874278055533 test_loss: 0.460467944856445\n",
      "iteration 9067train_loss: 0.4268742780532593 test_loss: 0.4604679564046183\n",
      "iteration 9068train_loss: 0.42687427805098827 test_loss: 0.46046796794589695\n",
      "iteration 9069train_loss: 0.42687427804871986 test_loss: 0.4604679794802852\n",
      "iteration 9070train_loss: 0.4268742780464542 test_loss: 0.46046799100778735\n",
      "iteration 9071train_loss: 0.42687427804419126 test_loss: 0.4604680025284074\n",
      "iteration 9072train_loss: 0.4268742780419311 test_loss: 0.46046801404214943\n",
      "iteration 9073train_loss: 0.4268742780396736 test_loss: 0.4604680255490175\n",
      "iteration 9074train_loss: 0.4268742780374187 test_loss: 0.46046803704901573\n",
      "iteration 9075train_loss: 0.42687427803516653 test_loss: 0.4604680485421484\n",
      "iteration 9076train_loss: 0.42687427803291716 test_loss: 0.46046806002841933\n",
      "iteration 9077train_loss: 0.4268742780306703 test_loss: 0.4604680715078328\n",
      "iteration 9078train_loss: 0.42687427802842626 test_loss: 0.4604680829803928\n",
      "iteration 9079train_loss: 0.42687427802618483 test_loss: 0.4604680944461036\n",
      "iteration 9080train_loss: 0.42687427802394606 test_loss: 0.46046810590496906\n",
      "iteration 9081train_loss: 0.42687427802171 test_loss: 0.4604681173569933\n",
      "iteration 9082train_loss: 0.4268742780194766 test_loss: 0.46046812880218063\n",
      "iteration 9083train_loss: 0.4268742780172459 test_loss: 0.4604681402405349\n",
      "iteration 9084train_loss: 0.4268742780150178 test_loss: 0.46046815167206023\n",
      "iteration 9085train_loss: 0.42687427801279243 test_loss: 0.4604681630967608\n",
      "iteration 9086train_loss: 0.4268742780105697 test_loss: 0.46046817451464056\n",
      "iteration 9087train_loss: 0.4268742780083495 test_loss: 0.46046818592570365\n",
      "iteration 9088train_loss: 0.42687427800613215 test_loss: 0.4604681973299542\n",
      "iteration 9089train_loss: 0.42687427800391736 test_loss: 0.46046820872739613\n",
      "iteration 9090train_loss: 0.42687427800170513 test_loss: 0.4604682201180336\n",
      "iteration 9091train_loss: 0.42687427799949573 test_loss: 0.46046823150187066\n",
      "iteration 9092train_loss: 0.42687427799728883 test_loss: 0.4604682428789113\n",
      "iteration 9093train_loss: 0.42687427799508454 test_loss: 0.4604682542491597\n",
      "iteration 9094train_loss: 0.42687427799288297 test_loss: 0.46046826561261983\n",
      "iteration 9095train_loss: 0.426874277990684 test_loss: 0.46046827696929576\n",
      "iteration 9096train_loss: 0.4268742779884877 test_loss: 0.4604682883191916\n",
      "iteration 9097train_loss: 0.426874277986294 test_loss: 0.4604682996623113\n",
      "iteration 9098train_loss: 0.4268742779841028 test_loss: 0.46046831099865887\n",
      "iteration 9099train_loss: 0.4268742779819143 test_loss: 0.4604683223282385\n",
      "iteration 9100train_loss: 0.42687427797972854 test_loss: 0.4604683336510541\n",
      "iteration 9101train_loss: 0.42687427797754524 test_loss: 0.4604683449671098\n",
      "iteration 9102train_loss: 0.42687427797536454 test_loss: 0.46046835627640975\n",
      "iteration 9103train_loss: 0.4268742779731865 test_loss: 0.46046836757895765\n",
      "iteration 9104train_loss: 0.4268742779710111 test_loss: 0.4604683788747578\n",
      "iteration 9105train_loss: 0.4268742779688382 test_loss: 0.4604683901638141\n",
      "iteration 9106train_loss: 0.4268742779666679 test_loss: 0.4604684014461306\n",
      "iteration 9107train_loss: 0.4268742779645002 test_loss: 0.4604684127217115\n",
      "iteration 9108train_loss: 0.4268742779623352 test_loss: 0.4604684239905604\n",
      "iteration 9109train_loss: 0.42687427796017263 test_loss: 0.46046843525268183\n",
      "iteration 9110train_loss: 0.4268742779580127 test_loss: 0.4604684465080795\n",
      "iteration 9111train_loss: 0.42687427795585536 test_loss: 0.4604684577567574\n",
      "iteration 9112train_loss: 0.42687427795370064 test_loss: 0.4604684689987197\n",
      "iteration 9113train_loss: 0.4268742779515484 test_loss: 0.46046848023397025\n",
      "iteration 9114train_loss: 0.42687427794939875 test_loss: 0.46046849146251323\n",
      "iteration 9115train_loss: 0.4268742779472518 test_loss: 0.4604685026843525\n",
      "iteration 9116train_loss: 0.42687427794510735 test_loss: 0.46046851389949217\n",
      "iteration 9117train_loss: 0.42687427794296534 test_loss: 0.4604685251079362\n",
      "iteration 9118train_loss: 0.426874277940826 test_loss: 0.46046853630968854\n",
      "iteration 9119train_loss: 0.4268742779386892 test_loss: 0.4604685475047532\n",
      "iteration 9120train_loss: 0.4268742779365549 test_loss: 0.4604685586931341\n",
      "iteration 9121train_loss: 0.42687427793442323 test_loss: 0.4604685698748355\n",
      "iteration 9122train_loss: 0.42687427793229404 test_loss: 0.46046858104986116\n",
      "iteration 9123train_loss: 0.42687427793016747 test_loss: 0.46046859221821496\n",
      "iteration 9124train_loss: 0.42687427792804333 test_loss: 0.46046860337990125\n",
      "iteration 9125train_loss: 0.4268742779259218 test_loss: 0.4604686145349237\n",
      "iteration 9126train_loss: 0.42687427792380284 test_loss: 0.4604686256832864\n",
      "iteration 9127train_loss: 0.4268742779216863 test_loss: 0.4604686368249933\n",
      "iteration 9128train_loss: 0.42687427791957233 test_loss: 0.4604686479600484\n",
      "iteration 9129train_loss: 0.4268742779174608 test_loss: 0.46046865908845563\n",
      "iteration 9130train_loss: 0.42687427791535204 test_loss: 0.4604686702102191\n",
      "iteration 9131train_loss: 0.4268742779132456 test_loss: 0.4604686813253424\n",
      "iteration 9132train_loss: 0.4268742779111417 test_loss: 0.46046869243382993\n",
      "iteration 9133train_loss: 0.4268742779090403 test_loss: 0.4604687035356855\n",
      "iteration 9134train_loss: 0.4268742779069415 test_loss: 0.460468714630913\n",
      "iteration 9135train_loss: 0.4268742779048451 test_loss: 0.4604687257195164\n",
      "iteration 9136train_loss: 0.42687427790275123 test_loss: 0.4604687368014997\n",
      "iteration 9137train_loss: 0.42687427790065996 test_loss: 0.46046874787686687\n",
      "iteration 9138train_loss: 0.4268742778985711 test_loss: 0.46046875894562184\n",
      "iteration 9139train_loss: 0.42687427789648474 test_loss: 0.4604687700077685\n",
      "iteration 9140train_loss: 0.4268742778944008 test_loss: 0.46046878106331085\n",
      "iteration 9141train_loss: 0.4268742778923195 test_loss: 0.4604687921122528\n",
      "iteration 9142train_loss: 0.42687427789024057 test_loss: 0.4604688031545983\n",
      "iteration 9143train_loss: 0.42687427788816407 test_loss: 0.46046881419035135\n",
      "iteration 9144train_loss: 0.42687427788609017 test_loss: 0.4604688252195159\n",
      "iteration 9145train_loss: 0.4268742778840187 test_loss: 0.46046883624209567\n",
      "iteration 9146train_loss: 0.4268742778819498 test_loss: 0.46046884725809484\n",
      "iteration 9147train_loss: 0.4268742778798833 test_loss: 0.46046885826751727\n",
      "iteration 9148train_loss: 0.42687427787781923 test_loss: 0.46046886927036673\n",
      "iteration 9149train_loss: 0.4268742778757576 test_loss: 0.4604688802666474\n",
      "iteration 9150train_loss: 0.42687427787369847 test_loss: 0.460468891256363\n",
      "iteration 9151train_loss: 0.42687427787164184 test_loss: 0.4604689022395174\n",
      "iteration 9152train_loss: 0.4268742778695876 test_loss: 0.4604689132161149\n",
      "iteration 9153train_loss: 0.42687427786753585 test_loss: 0.4604689241861589\n",
      "iteration 9154train_loss: 0.4268742778654866 test_loss: 0.4604689351496538\n",
      "iteration 9155train_loss: 0.4268742778634397 test_loss: 0.46046894610660316\n",
      "iteration 9156train_loss: 0.4268742778613953 test_loss: 0.4604689570570109\n",
      "iteration 9157train_loss: 0.42687427785935333 test_loss: 0.46046896800088116\n",
      "iteration 9158train_loss: 0.4268742778573138 test_loss: 0.46046897893821764\n",
      "iteration 9159train_loss: 0.42687427785527676 test_loss: 0.46046898986902435\n",
      "iteration 9160train_loss: 0.426874277853242 test_loss: 0.4604690007933051\n",
      "iteration 9161train_loss: 0.42687427785120985 test_loss: 0.46046901171106386\n",
      "iteration 9162train_loss: 0.4268742778491801 test_loss: 0.46046902262230444\n",
      "iteration 9163train_loss: 0.4268742778471527 test_loss: 0.46046903352703084\n",
      "iteration 9164train_loss: 0.4268742778451277 test_loss: 0.460469044425247\n",
      "iteration 9165train_loss: 0.42687427784310517 test_loss: 0.4604690553169565\n",
      "iteration 9166train_loss: 0.42687427784108495 test_loss: 0.46046906620216355\n",
      "iteration 9167train_loss: 0.42687427783906734 test_loss: 0.46046907708087187\n",
      "iteration 9168train_loss: 0.4268742778370519 test_loss: 0.46046908795308533\n",
      "iteration 9169train_loss: 0.42687427783503906 test_loss: 0.4604690988188079\n",
      "iteration 9170train_loss: 0.4268742778330285 test_loss: 0.4604691096780435\n",
      "iteration 9171train_loss: 0.4268742778310205 test_loss: 0.4604691205307957\n",
      "iteration 9172train_loss: 0.4268742778290147 test_loss: 0.4604691313770688\n",
      "iteration 9173train_loss: 0.42687427782701143 test_loss: 0.46046914221686636\n",
      "iteration 9174train_loss: 0.42687427782501053 test_loss: 0.46046915305019226\n",
      "iteration 9175train_loss: 0.42687427782301207 test_loss: 0.4604691638770506\n",
      "iteration 9176train_loss: 0.4268742778210159 test_loss: 0.46046917469744497\n",
      "iteration 9177train_loss: 0.42687427781902215 test_loss: 0.46046918551137933\n",
      "iteration 9178train_loss: 0.4268742778170308 test_loss: 0.4604691963188577\n",
      "iteration 9179train_loss: 0.4268742778150417 test_loss: 0.4604692071198836\n",
      "iteration 9180train_loss: 0.42687427781305515 test_loss: 0.46046921791446105\n",
      "iteration 9181train_loss: 0.42687427781107085 test_loss: 0.46046922870259405\n",
      "iteration 9182train_loss: 0.426874277809089 test_loss: 0.46046923948428625\n",
      "iteration 9183train_loss: 0.42687427780710946 test_loss: 0.46046925025954155\n",
      "iteration 9184train_loss: 0.42687427780513226 test_loss: 0.46046926102836383\n",
      "iteration 9185train_loss: 0.42687427780315756 test_loss: 0.46046927179075686\n",
      "iteration 9186train_loss: 0.4268742778011851 test_loss: 0.46046928254672453\n",
      "iteration 9187train_loss: 0.426874277799215 test_loss: 0.46046929329627057\n",
      "iteration 9188train_loss: 0.4268742777972473 test_loss: 0.46046930403939906\n",
      "iteration 9189train_loss: 0.4268742777952819 test_loss: 0.4604693147761135\n",
      "iteration 9190train_loss: 0.4268742777933189 test_loss: 0.460469325506418\n",
      "iteration 9191train_loss: 0.4268742777913582 test_loss: 0.4604693362303163\n",
      "iteration 9192train_loss: 0.4268742777893999 test_loss: 0.4604693469478122\n",
      "iteration 9193train_loss: 0.42687427778744397 test_loss: 0.4604693576589095\n",
      "iteration 9194train_loss: 0.4268742777854902 test_loss: 0.460469368363612\n",
      "iteration 9195train_loss: 0.4268742777835388 test_loss: 0.46046937906192364\n",
      "iteration 9196train_loss: 0.4268742777815898 test_loss: 0.4604693897538482\n",
      "iteration 9197train_loss: 0.42687427777964315 test_loss: 0.4604694004393893\n",
      "iteration 9198train_loss: 0.4268742777776988 test_loss: 0.46046941111855105\n",
      "iteration 9199train_loss: 0.4268742777757568 test_loss: 0.46046942179133704\n",
      "iteration 9200train_loss: 0.42687427777381715 test_loss: 0.4604694324577512\n",
      "iteration 9201train_loss: 0.4268742777718797 test_loss: 0.4604694431177972\n",
      "iteration 9202train_loss: 0.4268742777699446 test_loss: 0.4604694537714789\n",
      "iteration 9203train_loss: 0.4268742777680119 test_loss: 0.4604694644188003\n",
      "iteration 9204train_loss: 0.4268742777660814 test_loss: 0.4604694750597649\n",
      "iteration 9205train_loss: 0.4268742777641532 test_loss: 0.46046948569437657\n",
      "iteration 9206train_loss: 0.42687427776222736 test_loss: 0.4604694963226393\n",
      "iteration 9207train_loss: 0.4268742777603038 test_loss: 0.4604695069445565\n",
      "iteration 9208train_loss: 0.42687427775838244 test_loss: 0.4604695175601324\n",
      "iteration 9209train_loss: 0.42687427775646347 test_loss: 0.4604695281693705\n",
      "iteration 9210train_loss: 0.42687427775454684 test_loss: 0.4604695387722746\n",
      "iteration 9211train_loss: 0.4268742777526324 test_loss: 0.46046954936884854\n",
      "iteration 9212train_loss: 0.42687427775072034 test_loss: 0.4604695599590962\n",
      "iteration 9213train_loss: 0.4268742777488106 test_loss: 0.4604695705430212\n",
      "iteration 9214train_loss: 0.426874277746903 test_loss: 0.46046958112062736\n",
      "iteration 9215train_loss: 0.4268742777449977 test_loss: 0.46046959169191837\n",
      "iteration 9216train_loss: 0.4268742777430948 test_loss: 0.4604696022568982\n",
      "iteration 9217train_loss: 0.426874277741194 test_loss: 0.4604696128155705\n",
      "iteration 9218train_loss: 0.42687427773929565 test_loss: 0.460469623367939\n",
      "iteration 9219train_loss: 0.4268742777373994 test_loss: 0.46046963391400747\n",
      "iteration 9220train_loss: 0.42687427773550546 test_loss: 0.46046964445377975\n",
      "iteration 9221train_loss: 0.42687427773361386 test_loss: 0.46046965498725956\n",
      "iteration 9222train_loss: 0.4268742777317244 test_loss: 0.4604696655144506\n",
      "iteration 9223train_loss: 0.4268742777298374 test_loss: 0.46046967603535677\n",
      "iteration 9224train_loss: 0.42687427772795244 test_loss: 0.4604696865499817\n",
      "iteration 9225train_loss: 0.42687427772606984 test_loss: 0.4604696970583292\n",
      "iteration 9226train_loss: 0.42687427772418934 test_loss: 0.4604697075604029\n",
      "iteration 9227train_loss: 0.42687427772231124 test_loss: 0.4604697180562067\n",
      "iteration 9228train_loss: 0.4268742777204354 test_loss: 0.4604697285457442\n",
      "iteration 9229train_loss: 0.42687427771856173 test_loss: 0.4604697390290192\n",
      "iteration 9230train_loss: 0.42687427771669023 test_loss: 0.4604697495060355\n",
      "iteration 9231train_loss: 0.42687427771482106 test_loss: 0.4604697599767968\n",
      "iteration 9232train_loss: 0.42687427771295416 test_loss: 0.46046977044130677\n",
      "iteration 9233train_loss: 0.42687427771108943 test_loss: 0.46046978089956936\n",
      "iteration 9234train_loss: 0.4268742777092269 test_loss: 0.46046979135158794\n",
      "iteration 9235train_loss: 0.42687427770736663 test_loss: 0.4604698017973665\n",
      "iteration 9236train_loss: 0.42687427770550856 test_loss: 0.46046981223690864\n",
      "iteration 9237train_loss: 0.4268742777036527 test_loss: 0.4604698226702182\n",
      "iteration 9238train_loss: 0.42687427770179914 test_loss: 0.46046983309729883\n",
      "iteration 9239train_loss: 0.4268742776999478 test_loss: 0.4604698435181543\n",
      "iteration 9240train_loss: 0.4268742776980985 test_loss: 0.4604698539327882\n",
      "iteration 9241train_loss: 0.4268742776962516 test_loss: 0.46046986434120446\n",
      "iteration 9242train_loss: 0.4268742776944068 test_loss: 0.4604698747434066\n",
      "iteration 9243train_loss: 0.42687427769256414 test_loss: 0.4604698851393983\n",
      "iteration 9244train_loss: 0.42687427769072384 test_loss: 0.46046989552918344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9245train_loss: 0.42687427768888564 test_loss: 0.46046990591276565\n",
      "iteration 9246train_loss: 0.4268742776870496 test_loss: 0.4604699162901486\n",
      "iteration 9247train_loss: 0.42687427768521574 test_loss: 0.46046992666133607\n",
      "iteration 9248train_loss: 0.42687427768338415 test_loss: 0.46046993702633154\n",
      "iteration 9249train_loss: 0.4268742776815548 test_loss: 0.4604699473851391\n",
      "iteration 9250train_loss: 0.4268742776797276 test_loss: 0.4604699577377621\n",
      "iteration 9251train_loss: 0.4268742776779026 test_loss: 0.4604699680842043\n",
      "iteration 9252train_loss: 0.42687427767607966 test_loss: 0.46046997842446946\n",
      "iteration 9253train_loss: 0.42687427767425895 test_loss: 0.4604699887585614\n",
      "iteration 9254train_loss: 0.4268742776724404 test_loss: 0.4604699990864835\n",
      "iteration 9255train_loss: 0.4268742776706241 test_loss: 0.4604700094082396\n",
      "iteration 9256train_loss: 0.4268742776688099 test_loss: 0.46047001972383345\n",
      "iteration 9257train_loss: 0.42687427766699787 test_loss: 0.4604700300332686\n",
      "iteration 9258train_loss: 0.4268742776651881 test_loss: 0.4604700403365488\n",
      "iteration 9259train_loss: 0.4268742776633803 test_loss: 0.4604700506336777\n",
      "iteration 9260train_loss: 0.4268742776615748 test_loss: 0.4604700609246589\n",
      "iteration 9261train_loss: 0.42687427765977143 test_loss: 0.46047007120949623\n",
      "iteration 9262train_loss: 0.42687427765797026 test_loss: 0.46047008148819324\n",
      "iteration 9263train_loss: 0.42687427765617103 test_loss: 0.4604700917607537\n",
      "iteration 9264train_loss: 0.42687427765437425 test_loss: 0.46047010202718114\n",
      "iteration 9265train_loss: 0.4268742776525794 test_loss: 0.46047011228747925\n",
      "iteration 9266train_loss: 0.4268742776507868 test_loss: 0.4604701225416518\n",
      "iteration 9267train_loss: 0.42687427764899627 test_loss: 0.46047013278970245\n",
      "iteration 9268train_loss: 0.4268742776472079 test_loss: 0.4604701430316345\n",
      "iteration 9269train_loss: 0.4268742776454217 test_loss: 0.4604701532674521\n",
      "iteration 9270train_loss: 0.42687427764363756 test_loss: 0.46047016349715864\n",
      "iteration 9271train_loss: 0.42687427764185565 test_loss: 0.4604701737207578\n",
      "iteration 9272train_loss: 0.42687427764007585 test_loss: 0.4604701839382532\n",
      "iteration 9273train_loss: 0.4268742776382981 test_loss: 0.46047019414964857\n",
      "iteration 9274train_loss: 0.42687427763652247 test_loss: 0.46047020435494745\n",
      "iteration 9275train_loss: 0.426874277634749 test_loss: 0.46047021455415355\n",
      "iteration 9276train_loss: 0.42687427763297764 test_loss: 0.4604702247472705\n",
      "iteration 9277train_loss: 0.4268742776312085 test_loss: 0.460470234934302\n",
      "iteration 9278train_loss: 0.4268742776294413 test_loss: 0.4604702451152515\n",
      "iteration 9279train_loss: 0.4268742776276763 test_loss: 0.4604702552901227\n",
      "iteration 9280train_loss: 0.42687427762591346 test_loss: 0.4604702654589193\n",
      "iteration 9281train_loss: 0.4268742776241526 test_loss: 0.4604702756216449\n",
      "iteration 9282train_loss: 0.42687427762239394 test_loss: 0.4604702857783031\n",
      "iteration 9283train_loss: 0.42687427762063734 test_loss: 0.4604702959288976\n",
      "iteration 9284train_loss: 0.42687427761888286 test_loss: 0.46047030607343187\n",
      "iteration 9285train_loss: 0.4268742776171305 test_loss: 0.46047031621190965\n",
      "iteration 9286train_loss: 0.4268742776153801 test_loss: 0.4604703263443345\n",
      "iteration 9287train_loss: 0.4268742776136319 test_loss: 0.4604703364707101\n",
      "iteration 9288train_loss: 0.42687427761188584 test_loss: 0.46047034659104\n",
      "iteration 9289train_loss: 0.42687427761014185 test_loss: 0.46047035670532777\n",
      "iteration 9290train_loss: 0.42687427760839974 test_loss: 0.46047036681357706\n",
      "iteration 9291train_loss: 0.42687427760665997 test_loss: 0.4604703769157916\n",
      "iteration 9292train_loss: 0.4268742776049221 test_loss: 0.4604703870119748\n",
      "iteration 9293train_loss: 0.4268742776031863 test_loss: 0.46047039710213034\n",
      "iteration 9294train_loss: 0.42687427760145275 test_loss: 0.4604704071862618\n",
      "iteration 9295train_loss: 0.4268742775997211 test_loss: 0.46047041726437277\n",
      "iteration 9296train_loss: 0.42687427759799146 test_loss: 0.4604704273364669\n",
      "iteration 9297train_loss: 0.4268742775962641 test_loss: 0.4604704374025478\n",
      "iteration 9298train_loss: 0.4268742775945387 test_loss: 0.46047044746261895\n",
      "iteration 9299train_loss: 0.4268742775928154 test_loss: 0.4604704575166841\n",
      "iteration 9300train_loss: 0.4268742775910942 test_loss: 0.4604704675647465\n",
      "iteration 9301train_loss: 0.42687427758937485 test_loss: 0.4604704776068102\n",
      "iteration 9302train_loss: 0.42687427758765767 test_loss: 0.46047048764287846\n",
      "iteration 9303train_loss: 0.4268742775859426 test_loss: 0.46047049767295495\n",
      "iteration 9304train_loss: 0.4268742775842295 test_loss: 0.4604705076970432\n",
      "iteration 9305train_loss: 0.4268742775825185 test_loss: 0.4604705177151469\n",
      "iteration 9306train_loss: 0.42687427758080954 test_loss: 0.4604705277272695\n",
      "iteration 9307train_loss: 0.42687427757910257 test_loss: 0.4604705377334146\n",
      "iteration 9308train_loss: 0.42687427757739765 test_loss: 0.46047054773358587\n",
      "iteration 9309train_loss: 0.4268742775756948 test_loss: 0.46047055772778667\n",
      "iteration 9310train_loss: 0.426874277573994 test_loss: 0.4604705677160208\n",
      "iteration 9311train_loss: 0.4268742775722952 test_loss: 0.4604705776982917\n",
      "iteration 9312train_loss: 0.4268742775705983 test_loss: 0.46047058767460286\n",
      "iteration 9313train_loss: 0.4268742775689036 test_loss: 0.46047059764495807\n",
      "iteration 9314train_loss: 0.4268742775672109 test_loss: 0.46047060760936065\n",
      "iteration 9315train_loss: 0.4268742775655202 test_loss: 0.46047061756781416\n",
      "iteration 9316train_loss: 0.4268742775638314 test_loss: 0.4604706275203223\n",
      "iteration 9317train_loss: 0.4268742775621449 test_loss: 0.4604706374668886\n",
      "iteration 9318train_loss: 0.42687427756046026 test_loss: 0.46047064740751653\n",
      "iteration 9319train_loss: 0.42687427755877755 test_loss: 0.4604706573422096\n",
      "iteration 9320train_loss: 0.4268742775570969 test_loss: 0.4604706672709714\n",
      "iteration 9321train_loss: 0.42687427755541824 test_loss: 0.46047067719380563\n",
      "iteration 9322train_loss: 0.4268742775537417 test_loss: 0.46047068711071554\n",
      "iteration 9323train_loss: 0.426874277552067 test_loss: 0.46047069702170484\n",
      "iteration 9324train_loss: 0.42687427755039437 test_loss: 0.46047070692677705\n",
      "iteration 9325train_loss: 0.42687427754872365 test_loss: 0.4604707168259357\n",
      "iteration 9326train_loss: 0.42687427754705515 test_loss: 0.4604707267191843\n",
      "iteration 9327train_loss: 0.42687427754538854 test_loss: 0.4604707366065264\n",
      "iteration 9328train_loss: 0.4268742775437238 test_loss: 0.4604707464879656\n",
      "iteration 9329train_loss: 0.4268742775420612 test_loss: 0.4604707563635053\n",
      "iteration 9330train_loss: 0.4268742775404004 test_loss: 0.4604707662331489\n",
      "iteration 9331train_loss: 0.42687427753874185 test_loss: 0.4604707760969002\n",
      "iteration 9332train_loss: 0.42687427753708507 test_loss: 0.46047078595476254\n",
      "iteration 9333train_loss: 0.42687427753543034 test_loss: 0.46047079580673955\n",
      "iteration 9334train_loss: 0.4268742775337776 test_loss: 0.46047080565283466\n",
      "iteration 9335train_loss: 0.42687427753212687 test_loss: 0.4604708154930514\n",
      "iteration 9336train_loss: 0.42687427753047813 test_loss: 0.4604708253273932\n",
      "iteration 9337train_loss: 0.4268742775288311 test_loss: 0.4604708351558639\n",
      "iteration 9338train_loss: 0.4268742775271863 test_loss: 0.4604708449784666\n",
      "iteration 9339train_loss: 0.4268742775255434 test_loss: 0.46047085479520494\n",
      "iteration 9340train_loss: 0.42687427752390245 test_loss: 0.46047086460608244\n",
      "iteration 9341train_loss: 0.42687427752226337 test_loss: 0.46047087441110257\n",
      "iteration 9342train_loss: 0.4268742775206264 test_loss: 0.460470884210269\n",
      "iteration 9343train_loss: 0.42687427751899126 test_loss: 0.460470894003585\n",
      "iteration 9344train_loss: 0.4268742775173581 test_loss: 0.4604709037910541\n",
      "iteration 9345train_loss: 0.42687427751572704 test_loss: 0.4604709135726799\n",
      "iteration 9346train_loss: 0.42687427751409773 test_loss: 0.4604709233484658\n",
      "iteration 9347train_loss: 0.42687427751247053 test_loss: 0.4604709331184152\n",
      "iteration 9348train_loss: 0.42687427751084517 test_loss: 0.46047094288253193\n",
      "iteration 9349train_loss: 0.4268742775092218 test_loss: 0.46047095264081905\n",
      "iteration 9350train_loss: 0.4268742775076002 test_loss: 0.4604709623932803\n",
      "iteration 9351train_loss: 0.42687427750598067 test_loss: 0.460470972139919\n",
      "iteration 9352train_loss: 0.4268742775043632 test_loss: 0.46047098188073887\n",
      "iteration 9353train_loss: 0.4268742775027475 test_loss: 0.4604709916157431\n",
      "iteration 9354train_loss: 0.42687427750113377 test_loss: 0.46047100134493535\n",
      "iteration 9355train_loss: 0.426874277499522 test_loss: 0.46047101106831895\n",
      "iteration 9356train_loss: 0.426874277497912 test_loss: 0.4604710207858975\n",
      "iteration 9357train_loss: 0.4268742774963041 test_loss: 0.46047103049767446\n",
      "iteration 9358train_loss: 0.42687427749469803 test_loss: 0.4604710402036531\n",
      "iteration 9359train_loss: 0.4268742774930939 test_loss: 0.46047104990383714\n",
      "iteration 9360train_loss: 0.42687427749149176 test_loss: 0.4604710595982298\n",
      "iteration 9361train_loss: 0.4268742774898914 test_loss: 0.46047106928683484\n",
      "iteration 9362train_loss: 0.426874277488293 test_loss: 0.4604710789696555\n",
      "iteration 9363train_loss: 0.42687427748669654 test_loss: 0.46047108864669517\n",
      "iteration 9364train_loss: 0.4268742774851019 test_loss: 0.46047109831795746\n",
      "iteration 9365train_loss: 0.4268742774835093 test_loss: 0.4604711079834457\n",
      "iteration 9366train_loss: 0.4268742774819186 test_loss: 0.46047111764316345\n",
      "iteration 9367train_loss: 0.42687427748032963 test_loss: 0.4604711272971141\n",
      "iteration 9368train_loss: 0.42687427747874274 test_loss: 0.46047113694530106\n",
      "iteration 9369train_loss: 0.4268742774771577 test_loss: 0.4604711465877278\n",
      "iteration 9370train_loss: 0.4268742774755745 test_loss: 0.46047115622439777\n",
      "iteration 9371train_loss: 0.4268742774739931 test_loss: 0.46047116585531433\n",
      "iteration 9372train_loss: 0.4268742774724138 test_loss: 0.46047117548048117\n",
      "iteration 9373train_loss: 0.42687427747083634 test_loss: 0.4604711850999014\n",
      "iteration 9374train_loss: 0.4268742774692606 test_loss: 0.46047119471357867\n",
      "iteration 9375train_loss: 0.42687427746768686 test_loss: 0.4604712043215162\n",
      "iteration 9376train_loss: 0.426874277466115 test_loss: 0.4604712139237177\n",
      "iteration 9377train_loss: 0.42687427746454504 test_loss: 0.46047122352018627\n",
      "iteration 9378train_loss: 0.42687427746297696 test_loss: 0.46047123311092564\n",
      "iteration 9379train_loss: 0.4268742774614107 test_loss: 0.46047124269593903\n",
      "iteration 9380train_loss: 0.4268742774598462 test_loss: 0.46047125227522995\n",
      "iteration 9381train_loss: 0.42687427745828366 test_loss: 0.4604712618488018\n",
      "iteration 9382train_loss: 0.4268742774567231 test_loss: 0.46047127141665795\n",
      "iteration 9383train_loss: 0.4268742774551643 test_loss: 0.4604712809788018\n",
      "iteration 9384train_loss: 0.4268742774536074 test_loss: 0.46047129053523683\n",
      "iteration 9385train_loss: 0.42687427745205225 test_loss: 0.46047130008596643\n",
      "iteration 9386train_loss: 0.42687427745049916 test_loss: 0.460471309630994\n",
      "iteration 9387train_loss: 0.4268742774489478 test_loss: 0.46047131917032286\n",
      "iteration 9388train_loss: 0.4268742774473983 test_loss: 0.46047132870395663\n",
      "iteration 9389train_loss: 0.4268742774458506 test_loss: 0.46047133823189845\n",
      "iteration 9390train_loss: 0.42687427744430484 test_loss: 0.4604713477541519\n",
      "iteration 9391train_loss: 0.42687427744276085 test_loss: 0.46047135727072036\n",
      "iteration 9392train_loss: 0.4268742774412188 test_loss: 0.46047136678160705\n",
      "iteration 9393train_loss: 0.42687427743967854 test_loss: 0.46047137628681556\n",
      "iteration 9394train_loss: 0.4268742774381401 test_loss: 0.4604713857863493\n",
      "iteration 9395train_loss: 0.42687427743660344 test_loss: 0.4604713952802114\n",
      "iteration 9396train_loss: 0.4268742774350688 test_loss: 0.4604714047684055\n",
      "iteration 9397train_loss: 0.4268742774335358 test_loss: 0.4604714142509349\n",
      "iteration 9398train_loss: 0.4268742774320047 test_loss: 0.46047142372780303\n",
      "iteration 9399train_loss: 0.42687427743047546 test_loss: 0.4604714331990131\n",
      "iteration 9400train_loss: 0.4268742774289479 test_loss: 0.4604714426645687\n",
      "iteration 9401train_loss: 0.42687427742742234 test_loss: 0.46047145212447305\n",
      "iteration 9402train_loss: 0.42687427742589856 test_loss: 0.46047146157872965\n",
      "iteration 9403train_loss: 0.42687427742437656 test_loss: 0.4604714710273418\n",
      "iteration 9404train_loss: 0.42687427742285644 test_loss: 0.4604714804703127\n",
      "iteration 9405train_loss: 0.426874277421338 test_loss: 0.4604714899076462\n",
      "iteration 9406train_loss: 0.4268742774198216 test_loss: 0.4604714993393451\n",
      "iteration 9407train_loss: 0.4268742774183068 test_loss: 0.4604715087654132\n",
      "iteration 9408train_loss: 0.42687427741679385 test_loss: 0.4604715181858536\n",
      "iteration 9409train_loss: 0.4268742774152828 test_loss: 0.4604715276006698\n",
      "iteration 9410train_loss: 0.42687427741377354 test_loss: 0.46047153700986504\n",
      "iteration 9411train_loss: 0.42687427741226597 test_loss: 0.46047154641344284\n",
      "iteration 9412train_loss: 0.4268742774107603 test_loss: 0.46047155581140636\n",
      "iteration 9413train_loss: 0.4268742774092565 test_loss: 0.4604715652037591\n",
      "iteration 9414train_loss: 0.42687427740775435 test_loss: 0.46047157459050425\n",
      "iteration 9415train_loss: 0.426874277406254 test_loss: 0.4604715839716453\n",
      "iteration 9416train_loss: 0.4268742774047554 test_loss: 0.4604715933471856\n",
      "iteration 9417train_loss: 0.42687427740325873 test_loss: 0.46047160271712845\n",
      "iteration 9418train_loss: 0.4268742774017638 test_loss: 0.4604716120814772\n",
      "iteration 9419train_loss: 0.42687427740027056 test_loss: 0.4604716214402352\n",
      "iteration 9420train_loss: 0.4268742773987793 test_loss: 0.46047163079340575\n",
      "iteration 9421train_loss: 0.4268742773972896 test_loss: 0.46047164014099223\n",
      "iteration 9422train_loss: 0.4268742773958018 test_loss: 0.46047164948299785\n",
      "iteration 9423train_loss: 0.42687427739431577 test_loss: 0.46047165881942614\n",
      "iteration 9424train_loss: 0.42687427739283146 test_loss: 0.46047166815028023\n",
      "iteration 9425train_loss: 0.42687427739134903 test_loss: 0.46047167747556367\n",
      "iteration 9426train_loss: 0.4268742773898683 test_loss: 0.4604716867952796\n",
      "iteration 9427train_loss: 0.42687427738838934 test_loss: 0.46047169610943145\n",
      "iteration 9428train_loss: 0.42687427738691214 test_loss: 0.46047170541802235\n",
      "iteration 9429train_loss: 0.4268742773854367 test_loss: 0.4604717147210559\n",
      "iteration 9430train_loss: 0.42687427738396305 test_loss: 0.46047172401853526\n",
      "iteration 9431train_loss: 0.42687427738249123 test_loss: 0.46047173331046376\n",
      "iteration 9432train_loss: 0.42687427738102096 test_loss: 0.46047174259684476\n",
      "iteration 9433train_loss: 0.4268742773795525 test_loss: 0.4604717518776816\n",
      "iteration 9434train_loss: 0.4268742773780859 test_loss: 0.4604717611529774\n",
      "iteration 9435train_loss: 0.42687427737662115 test_loss: 0.46047177042273557\n",
      "iteration 9436train_loss: 0.426874277375158 test_loss: 0.46047177968695957\n",
      "iteration 9437train_loss: 0.42687427737369654 test_loss: 0.4604717889456525\n",
      "iteration 9438train_loss: 0.4268742773722369 test_loss: 0.46047179819881784\n",
      "iteration 9439train_loss: 0.42687427737077904 test_loss: 0.46047180744645866\n",
      "iteration 9440train_loss: 0.4268742773693228 test_loss: 0.46047181668857845\n",
      "iteration 9441train_loss: 0.4268742773678684 test_loss: 0.46047182592518043\n",
      "iteration 9442train_loss: 0.42687427736641576 test_loss: 0.46047183515626783\n",
      "iteration 9443train_loss: 0.42687427736496475 test_loss: 0.46047184438184413\n",
      "iteration 9444train_loss: 0.4268742773635156 test_loss: 0.46047185360191245\n",
      "iteration 9445train_loss: 0.426874277362068 test_loss: 0.4604718628164762\n",
      "iteration 9446train_loss: 0.42687427736062233 test_loss: 0.46047187202553846\n",
      "iteration 9447train_loss: 0.4268742773591782 test_loss: 0.46047188122910276\n",
      "iteration 9448train_loss: 0.426874277357736 test_loss: 0.4604718904271723\n",
      "iteration 9449train_loss: 0.4268742773562953 test_loss: 0.4604718996197503\n",
      "iteration 9450train_loss: 0.4268742773548565 test_loss: 0.46047190880684\n",
      "iteration 9451train_loss: 0.4268742773534193 test_loss: 0.46047191798844495\n",
      "iteration 9452train_loss: 0.4268742773519839 test_loss: 0.4604719271645681\n",
      "iteration 9453train_loss: 0.42687427735055017 test_loss: 0.4604719363352129\n",
      "iteration 9454train_loss: 0.42687427734911815 test_loss: 0.46047194550038245\n",
      "iteration 9455train_loss: 0.42687427734768785 test_loss: 0.46047195466008023\n",
      "iteration 9456train_loss: 0.42687427734625927 test_loss: 0.46047196381430944\n",
      "iteration 9457train_loss: 0.42687427734483235 test_loss: 0.46047197296307324\n",
      "iteration 9458train_loss: 0.4268742773434072 test_loss: 0.4604719821063751\n",
      "iteration 9459train_loss: 0.4268742773419837 test_loss: 0.46047199124421806\n",
      "iteration 9460train_loss: 0.4268742773405619 test_loss: 0.46047200037660546\n",
      "iteration 9461train_loss: 0.4268742773391418 test_loss: 0.4604720095035406\n",
      "iteration 9462train_loss: 0.42687427733772343 test_loss: 0.4604720186250268\n",
      "iteration 9463train_loss: 0.42687427733630673 test_loss: 0.46047202774106705\n",
      "iteration 9464train_loss: 0.42687427733489175 test_loss: 0.4604720368516649\n",
      "iteration 9465train_loss: 0.4268742773334783 test_loss: 0.4604720459568234\n",
      "iteration 9466train_loss: 0.42687427733206684 test_loss: 0.4604720550565459\n",
      "iteration 9467train_loss: 0.42687427733065686 test_loss: 0.4604720641508355\n",
      "iteration 9468train_loss: 0.4268742773292486 test_loss: 0.4604720732396957\n",
      "iteration 9469train_loss: 0.426874277327842 test_loss: 0.4604720823231296\n",
      "iteration 9470train_loss: 0.42687427732643707 test_loss: 0.46047209140114026\n",
      "iteration 9471train_loss: 0.42687427732503397 test_loss: 0.4604721004737314\n",
      "iteration 9472train_loss: 0.4268742773236323 test_loss: 0.46047210954090567\n",
      "iteration 9473train_loss: 0.4268742773222325 test_loss: 0.4604721186026666\n",
      "iteration 9474train_loss: 0.42687427732083427 test_loss: 0.4604721276590175\n",
      "iteration 9475train_loss: 0.4268742773194377 test_loss: 0.4604721367099615\n",
      "iteration 9476train_loss: 0.42687427731804284 test_loss: 0.4604721457555017\n",
      "iteration 9477train_loss: 0.4268742773166498 test_loss: 0.4604721547956417\n",
      "iteration 9478train_loss: 0.4268742773152582 test_loss: 0.46047216383038425\n",
      "iteration 9479train_loss: 0.4268742773138683 test_loss: 0.46047217285973296\n",
      "iteration 9480train_loss: 0.42687427731248007 test_loss: 0.4604721818836908\n",
      "iteration 9481train_loss: 0.4268742773110935 test_loss: 0.4604721909022611\n",
      "iteration 9482train_loss: 0.42687427730970856 test_loss: 0.4604721999154472\n",
      "iteration 9483train_loss: 0.42687427730832533 test_loss: 0.460472208923252\n",
      "iteration 9484train_loss: 0.4268742773069438 test_loss: 0.46047221792567894\n",
      "iteration 9485train_loss: 0.4268742773055638 test_loss: 0.46047222692273115\n",
      "iteration 9486train_loss: 0.4268742773041855 test_loss: 0.4604722359144119\n",
      "iteration 9487train_loss: 0.4268742773028088 test_loss: 0.4604722449007243\n",
      "iteration 9488train_loss: 0.4268742773014339 test_loss: 0.4604722538816717\n",
      "iteration 9489train_loss: 0.42687427730006056 test_loss: 0.4604722628572571\n",
      "iteration 9490train_loss: 0.4268742772986888 test_loss: 0.46047227182748396\n",
      "iteration 9491train_loss: 0.42687427729731864 test_loss: 0.4604722807923552\n",
      "iteration 9492train_loss: 0.4268742772959502 test_loss: 0.4604722897518743\n",
      "iteration 9493train_loss: 0.4268742772945834 test_loss: 0.4604722987060443\n",
      "iteration 9494train_loss: 0.42687427729321825 test_loss: 0.46047230765486835\n",
      "iteration 9495train_loss: 0.4268742772918546 test_loss: 0.46047231659834964\n",
      "iteration 9496train_loss: 0.42687427729049277 test_loss: 0.4604723255364915\n",
      "iteration 9497train_loss: 0.42687427728913246 test_loss: 0.46047233446929703\n",
      "iteration 9498train_loss: 0.4268742772877738 test_loss: 0.46047234339676935\n",
      "iteration 9499train_loss: 0.42687427728641675 test_loss: 0.4604723523189118\n",
      "iteration 9500train_loss: 0.4268742772850613 test_loss: 0.4604723612357274\n",
      "iteration 9501train_loss: 0.4268742772837074 test_loss: 0.4604723701472194\n",
      "iteration 9502train_loss: 0.4268742772823552 test_loss: 0.46047237905339106\n",
      "iteration 9503train_loss: 0.4268742772810047 test_loss: 0.4604723879542454\n",
      "iteration 9504train_loss: 0.42687427727965577 test_loss: 0.46047239684978575\n",
      "iteration 9505train_loss: 0.4268742772783083 test_loss: 0.46047240574001513\n",
      "iteration 9506train_loss: 0.4268742772769626 test_loss: 0.46047241462493677\n",
      "iteration 9507train_loss: 0.4268742772756184 test_loss: 0.4604724235045539\n",
      "iteration 9508train_loss: 0.4268742772742759 test_loss: 0.46047243237886976\n",
      "iteration 9509train_loss: 0.4268742772729349 test_loss: 0.4604724412478872\n",
      "iteration 9510train_loss: 0.42687427727159555 test_loss: 0.46047245011160975\n",
      "iteration 9511train_loss: 0.4268742772702579 test_loss: 0.4604724589700403\n",
      "iteration 9512train_loss: 0.42687427726892174 test_loss: 0.4604724678231822\n",
      "iteration 9513train_loss: 0.42687427726758714 test_loss: 0.4604724766710383\n",
      "iteration 9514train_loss: 0.4268742772662542 test_loss: 0.4604724855136122\n",
      "iteration 9515train_loss: 0.42687427726492283 test_loss: 0.4604724943509067\n",
      "iteration 9516train_loss: 0.42687427726359306 test_loss: 0.46047250318292515\n",
      "iteration 9517train_loss: 0.42687427726226496 test_loss: 0.4604725120096706\n",
      "iteration 9518train_loss: 0.4268742772609383 test_loss: 0.46047252083114626\n",
      "iteration 9519train_loss: 0.4268742772596133 test_loss: 0.46047252964735513\n",
      "iteration 9520train_loss: 0.42687427725828986 test_loss: 0.46047253845830066\n",
      "iteration 9521train_loss: 0.426874277256968 test_loss: 0.4604725472639857\n",
      "iteration 9522train_loss: 0.42687427725564775 test_loss: 0.4604725560644134\n",
      "iteration 9523train_loss: 0.426874277254329 test_loss: 0.4604725648595871\n",
      "iteration 9524train_loss: 0.42687427725301186 test_loss: 0.4604725736495097\n",
      "iteration 9525train_loss: 0.42687427725169635 test_loss: 0.4604725824341846\n",
      "iteration 9526train_loss: 0.4268742772503824 test_loss: 0.46047259121361467\n",
      "iteration 9527train_loss: 0.42687427724906996 test_loss: 0.46047259998780327\n",
      "iteration 9528train_loss: 0.4268742772477591 test_loss: 0.4604726087567534\n",
      "iteration 9529train_loss: 0.4268742772464499 test_loss: 0.4604726175204681\n",
      "iteration 9530train_loss: 0.42687427724514204 test_loss: 0.4604726262789508\n",
      "iteration 9531train_loss: 0.426874277243836 test_loss: 0.46047263503220437\n",
      "iteration 9532train_loss: 0.42687427724253135 test_loss: 0.46047264378023184\n",
      "iteration 9533train_loss: 0.42687427724122834 test_loss: 0.46047265252303665\n",
      "iteration 9534train_loss: 0.4268742772399268 test_loss: 0.4604726612606217\n",
      "iteration 9535train_loss: 0.4268742772386269 test_loss: 0.46047266999299\n",
      "iteration 9536train_loss: 0.42687427723732857 test_loss: 0.46047267872014497\n",
      "iteration 9537train_loss: 0.4268742772360318 test_loss: 0.4604726874420896\n",
      "iteration 9538train_loss: 0.4268742772347365 test_loss: 0.460472696158827\n",
      "iteration 9539train_loss: 0.4268742772334427 test_loss: 0.4604727048703602\n",
      "iteration 9540train_loss: 0.42687427723215055 test_loss: 0.46047271357669217\n",
      "iteration 9541train_loss: 0.4268742772308598 test_loss: 0.46047272227782643\n",
      "iteration 9542train_loss: 0.4268742772295708 test_loss: 0.4604727309737658\n",
      "iteration 9543train_loss: 0.42687427722828314 test_loss: 0.4604727396645134\n",
      "iteration 9544train_loss: 0.4268742772269972 test_loss: 0.4604727483500724\n",
      "iteration 9545train_loss: 0.4268742772257127 test_loss: 0.4604727570304458\n",
      "iteration 9546train_loss: 0.42687427722442967 test_loss: 0.46047276570563694\n",
      "iteration 9547train_loss: 0.42687427722314836 test_loss: 0.4604727743756486\n",
      "iteration 9548train_loss: 0.42687427722186844 test_loss: 0.46047278304048395\n",
      "iteration 9549train_loss: 0.42687427722059007 test_loss: 0.4604727917001462\n",
      "iteration 9550train_loss: 0.4268742772193132 test_loss: 0.4604728003546384\n",
      "iteration 9551train_loss: 0.42687427721803795 test_loss: 0.46047280900396365\n",
      "iteration 9552train_loss: 0.42687427721676413 test_loss: 0.46047281764812487\n",
      "iteration 9553train_loss: 0.42687427721549187 test_loss: 0.46047282628712527\n",
      "iteration 9554train_loss: 0.4268742772142211 test_loss: 0.46047283492096797\n",
      "iteration 9555train_loss: 0.42687427721295185 test_loss: 0.46047284354965606\n",
      "iteration 9556train_loss: 0.42687427721168414 test_loss: 0.4604728521731925\n",
      "iteration 9557train_loss: 0.42687427721041793 test_loss: 0.46047286079158045\n",
      "iteration 9558train_loss: 0.42687427720915333 test_loss: 0.460472869404823\n",
      "iteration 9559train_loss: 0.42687427720789006 test_loss: 0.46047287801292314\n",
      "iteration 9560train_loss: 0.4268742772066284 test_loss: 0.46047288661588404\n",
      "iteration 9561train_loss: 0.42687427720536825 test_loss: 0.46047289521370866\n",
      "iteration 9562train_loss: 0.4268742772041096 test_loss: 0.46047290380640005\n",
      "iteration 9563train_loss: 0.42687427720285237 test_loss: 0.4604729123939615\n",
      "iteration 9564train_loss: 0.4268742772015967 test_loss: 0.4604729209763959\n",
      "iteration 9565train_loss: 0.4268742772003426 test_loss: 0.46047292955370617\n",
      "iteration 9566train_loss: 0.42687427719908994 test_loss: 0.4604729381258957\n",
      "iteration 9567train_loss: 0.4268742771978387 test_loss: 0.46047294669296734\n",
      "iteration 9568train_loss: 0.426874277196589 test_loss: 0.46047295525492404\n",
      "iteration 9569train_loss: 0.4268742771953408 test_loss: 0.4604729638117692\n",
      "iteration 9570train_loss: 0.42687427719409415 test_loss: 0.4604729723635055\n",
      "iteration 9571train_loss: 0.426874277192849 test_loss: 0.4604729809101362\n",
      "iteration 9572train_loss: 0.4268742771916053 test_loss: 0.4604729894516644\n",
      "iteration 9573train_loss: 0.426874277190363 test_loss: 0.460472997988093\n",
      "iteration 9574train_loss: 0.42687427718912224 test_loss: 0.46047300651942497\n",
      "iteration 9575train_loss: 0.426874277187883 test_loss: 0.46047301504566357\n",
      "iteration 9576train_loss: 0.4268742771866451 test_loss: 0.4604730235668118\n",
      "iteration 9577train_loss: 0.4268742771854089 test_loss: 0.4604730320828726\n",
      "iteration 9578train_loss: 0.42687427718417414 test_loss: 0.4604730405938491\n",
      "iteration 9579train_loss: 0.42687427718294074 test_loss: 0.46047304909974424\n",
      "iteration 9580train_loss: 0.4268742771817088 test_loss: 0.46047305760056106\n",
      "iteration 9581train_loss: 0.4268742771804785 test_loss: 0.4604730660963026\n",
      "iteration 9582train_loss: 0.4268742771792495 test_loss: 0.460473074586972\n",
      "iteration 9583train_loss: 0.4268742771780219 test_loss: 0.46047308307257223\n",
      "iteration 9584train_loss: 0.4268742771767959 test_loss: 0.4604730915531063\n",
      "iteration 9585train_loss: 0.4268742771755714 test_loss: 0.46047310002857716\n",
      "iteration 9586train_loss: 0.42687427717434834 test_loss: 0.46047310849898804\n",
      "iteration 9587train_loss: 0.4268742771731267 test_loss: 0.4604731169643417\n",
      "iteration 9588train_loss: 0.4268742771719065 test_loss: 0.46047312542464114\n",
      "iteration 9589train_loss: 0.4268742771706877 test_loss: 0.4604731338798898\n",
      "iteration 9590train_loss: 0.42687427716947046 test_loss: 0.46047314233009035\n",
      "iteration 9591train_loss: 0.42687427716825477 test_loss: 0.4604731507752458\n",
      "iteration 9592train_loss: 0.42687427716704046 test_loss: 0.46047315921535936\n",
      "iteration 9593train_loss: 0.4268742771658275 test_loss: 0.4604731676504337\n",
      "iteration 9594train_loss: 0.42687427716461607 test_loss: 0.4604731760804723\n",
      "iteration 9595train_loss: 0.42687427716340604 test_loss: 0.46047318450547764\n",
      "iteration 9596train_loss: 0.42687427716219745 test_loss: 0.46047319292545313\n",
      "iteration 9597train_loss: 0.4268742771609903 test_loss: 0.46047320134040165\n",
      "iteration 9598train_loss: 0.42687427715978465 test_loss: 0.4604732097503262\n",
      "iteration 9599train_loss: 0.42687427715858045 test_loss: 0.4604732181552297\n",
      "iteration 9600train_loss: 0.42687427715737764 test_loss: 0.46047322655511524\n",
      "iteration 9601train_loss: 0.42687427715617626 test_loss: 0.4604732349499858\n",
      "iteration 9602train_loss: 0.42687427715497633 test_loss: 0.4604732433398444\n",
      "iteration 9603train_loss: 0.4268742771537778 test_loss: 0.46047325172469394\n",
      "iteration 9604train_loss: 0.4268742771525807 test_loss: 0.46047326010453743\n",
      "iteration 9605train_loss: 0.42687427715138504 test_loss: 0.4604732684793779\n",
      "iteration 9606train_loss: 0.426874277150191 test_loss: 0.4604732768492184\n",
      "iteration 9607train_loss: 0.4268742771489981 test_loss: 0.4604732852140619\n",
      "iteration 9608train_loss: 0.42687427714780674 test_loss: 0.46047329357391126\n",
      "iteration 9609train_loss: 0.42687427714661685 test_loss: 0.4604733019287695\n",
      "iteration 9610train_loss: 0.42687427714542836 test_loss: 0.4604733102786396\n",
      "iteration 9611train_loss: 0.4268742771442412 test_loss: 0.46047331862352464\n",
      "iteration 9612train_loss: 0.42687427714305554 test_loss: 0.4604733269634275\n",
      "iteration 9613train_loss: 0.4268742771418713 test_loss: 0.4604733352983511\n",
      "iteration 9614train_loss: 0.4268742771406884 test_loss: 0.46047334362829867\n",
      "iteration 9615train_loss: 0.426874277139507 test_loss: 0.4604733519532729\n",
      "iteration 9616train_loss: 0.42687427713832693 test_loss: 0.46047336027327684\n",
      "iteration 9617train_loss: 0.4268742771371483 test_loss: 0.4604733685883135\n",
      "iteration 9618train_loss: 0.4268742771359712 test_loss: 0.46047337689838586\n",
      "iteration 9619train_loss: 0.42687427713479537 test_loss: 0.46047338520349684\n",
      "iteration 9620train_loss: 0.4268742771336209 test_loss: 0.4604733935036494\n",
      "iteration 9621train_loss: 0.4268742771324479 test_loss: 0.4604734017988464\n",
      "iteration 9622train_loss: 0.4268742771312763 test_loss: 0.46047341008909093\n",
      "iteration 9623train_loss: 0.4268742771301061 test_loss: 0.4604734183743861\n",
      "iteration 9624train_loss: 0.4268742771289373 test_loss: 0.4604734266547346\n",
      "iteration 9625train_loss: 0.42687427712776993 test_loss: 0.4604734349301394\n",
      "iteration 9626train_loss: 0.42687427712660386 test_loss: 0.46047344320060357\n",
      "iteration 9627train_loss: 0.4268742771254393 test_loss: 0.46047345146613006\n",
      "iteration 9628train_loss: 0.42687427712427595 test_loss: 0.46047345972672166\n",
      "iteration 9629train_loss: 0.42687427712311427 test_loss: 0.46047346798238153\n",
      "iteration 9630train_loss: 0.4268742771219537 test_loss: 0.4604734762331125\n",
      "iteration 9631train_loss: 0.4268742771207946 test_loss: 0.46047348447891745\n",
      "iteration 9632train_loss: 0.42687427711963694 test_loss: 0.4604734927197994\n",
      "iteration 9633train_loss: 0.42687427711848064 test_loss: 0.4604735009557614\n",
      "iteration 9634train_loss: 0.42687427711732573 test_loss: 0.4604735091868062\n",
      "iteration 9635train_loss: 0.42687427711617226 test_loss: 0.46047351741293674\n",
      "iteration 9636train_loss: 0.4268742771150201 test_loss: 0.46047352563415617\n",
      "iteration 9637train_loss: 0.4268742771138693 test_loss: 0.46047353385046724\n",
      "iteration 9638train_loss: 0.42687427711271986 test_loss: 0.4604735420618728\n",
      "iteration 9639train_loss: 0.4268742771115718 test_loss: 0.46047355026837594\n",
      "iteration 9640train_loss: 0.42687427711042514 test_loss: 0.46047355846997956\n",
      "iteration 9641train_loss: 0.4268742771092799 test_loss: 0.46047356666668654\n",
      "iteration 9642train_loss: 0.4268742771081359 test_loss: 0.4604735748584999\n",
      "iteration 9643train_loss: 0.42687427710699327 test_loss: 0.4604735830454223\n",
      "iteration 9644train_loss: 0.42687427710585213 test_loss: 0.460473591227457\n",
      "iteration 9645train_loss: 0.4268742771047122 test_loss: 0.4604735994046068\n",
      "iteration 9646train_loss: 0.42687427710357373 test_loss: 0.4604736075768745\n",
      "iteration 9647train_loss: 0.4268742771024366 test_loss: 0.46047361574426315\n",
      "iteration 9648train_loss: 0.4268742771013009 test_loss: 0.46047362390677554\n",
      "iteration 9649train_loss: 0.4268742771001665 test_loss: 0.46047363206441466\n",
      "iteration 9650train_loss: 0.42687427709903336 test_loss: 0.46047364021718346\n",
      "iteration 9651train_loss: 0.4268742770979017 test_loss: 0.4604736483650848\n",
      "iteration 9652train_loss: 0.4268742770967713 test_loss: 0.4604736565081216\n",
      "iteration 9653train_loss: 0.4268742770956423 test_loss: 0.4604736646462967\n",
      "iteration 9654train_loss: 0.42687427709451464 test_loss: 0.46047367277961304\n",
      "iteration 9655train_loss: 0.4268742770933884 test_loss: 0.46047368090807345\n",
      "iteration 9656train_loss: 0.42687427709226344 test_loss: 0.46047368903168107\n",
      "iteration 9657train_loss: 0.4268742770911398 test_loss: 0.4604736971504385\n",
      "iteration 9658train_loss: 0.4268742770900175 test_loss: 0.46047370526434894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9659train_loss: 0.4268742770888965 test_loss: 0.4604737133734149\n",
      "iteration 9660train_loss: 0.426874277087777 test_loss: 0.46047372147763965\n",
      "iteration 9661train_loss: 0.42687427708665876 test_loss: 0.46047372957702587\n",
      "iteration 9662train_loss: 0.42687427708554176 test_loss: 0.4604737376715765\n",
      "iteration 9663train_loss: 0.42687427708442616 test_loss: 0.4604737457612944\n",
      "iteration 9664train_loss: 0.42687427708331194 test_loss: 0.46047375384618244\n",
      "iteration 9665train_loss: 0.42687427708219894 test_loss: 0.4604737619262437\n",
      "iteration 9666train_loss: 0.4268742770810874 test_loss: 0.4604737700014808\n",
      "iteration 9667train_loss: 0.42687427707997705 test_loss: 0.46047377807189666\n",
      "iteration 9668train_loss: 0.4268742770788682 test_loss: 0.46047378613749435\n",
      "iteration 9669train_loss: 0.4268742770777605 test_loss: 0.4604737941982765\n",
      "iteration 9670train_loss: 0.4268742770766542 test_loss: 0.4604738022542462\n",
      "iteration 9671train_loss: 0.42687427707554926 test_loss: 0.4604738103054061\n",
      "iteration 9672train_loss: 0.42687427707444553 test_loss: 0.4604738183517594\n",
      "iteration 9673train_loss: 0.42687427707334313 test_loss: 0.4604738263933086\n",
      "iteration 9674train_loss: 0.42687427707224224 test_loss: 0.46047383443005674\n",
      "iteration 9675train_loss: 0.4268742770711424 test_loss: 0.4604738424620067\n",
      "iteration 9676train_loss: 0.42687427707004405 test_loss: 0.46047385048916134\n",
      "iteration 9677train_loss: 0.426874277068947 test_loss: 0.4604738585115235\n",
      "iteration 9678train_loss: 0.42687427706785114 test_loss: 0.4604738665290961\n",
      "iteration 9679train_loss: 0.4268742770667567 test_loss: 0.460473874541882\n",
      "iteration 9680train_loss: 0.42687427706566355 test_loss: 0.46047388254988386\n",
      "iteration 9681train_loss: 0.42687427706457176 test_loss: 0.4604738905531047\n",
      "iteration 9682train_loss: 0.42687427706348113 test_loss: 0.4604738985515474\n",
      "iteration 9683train_loss: 0.4268742770623919 test_loss: 0.46047390654521486\n",
      "iteration 9684train_loss: 0.42687427706130393 test_loss: 0.4604739145341097\n",
      "iteration 9685train_loss: 0.42687427706021724 test_loss: 0.46047392251823493\n",
      "iteration 9686train_loss: 0.42687427705913195 test_loss: 0.4604739304975935\n",
      "iteration 9687train_loss: 0.4268742770580478 test_loss: 0.46047393847218804\n",
      "iteration 9688train_loss: 0.426874277056965 test_loss: 0.46047394644202144\n",
      "iteration 9689train_loss: 0.42687427705588354 test_loss: 0.4604739544070966\n",
      "iteration 9690train_loss: 0.42687427705480346 test_loss: 0.46047396236741645\n",
      "iteration 9691train_loss: 0.4268742770537246 test_loss: 0.46047397032298365\n",
      "iteration 9692train_loss: 0.42687427705264697 test_loss: 0.46047397827380115\n",
      "iteration 9693train_loss: 0.42687427705157055 test_loss: 0.4604739862198717\n",
      "iteration 9694train_loss: 0.4268742770504956 test_loss: 0.4604739941611982\n",
      "iteration 9695train_loss: 0.4268742770494218 test_loss: 0.4604740020977835\n",
      "iteration 9696train_loss: 0.4268742770483494 test_loss: 0.46047401002963034\n",
      "iteration 9697train_loss: 0.4268742770472782 test_loss: 0.4604740179567417\n",
      "iteration 9698train_loss: 0.42687427704620834 test_loss: 0.4604740258791203\n",
      "iteration 9699train_loss: 0.4268742770451396 test_loss: 0.4604740337967689\n",
      "iteration 9700train_loss: 0.42687427704407227 test_loss: 0.4604740417096904\n",
      "iteration 9701train_loss: 0.4268742770430061 test_loss: 0.4604740496178877\n",
      "iteration 9702train_loss: 0.4268742770419413 test_loss: 0.46047405752136356\n",
      "iteration 9703train_loss: 0.4268742770408778 test_loss: 0.46047406542012076\n",
      "iteration 9704train_loss: 0.42687427703981556 test_loss: 0.46047407331416207\n",
      "iteration 9705train_loss: 0.42687427703875463 test_loss: 0.4604740812034905\n",
      "iteration 9706train_loss: 0.42687427703769476 test_loss: 0.4604740890881086\n",
      "iteration 9707train_loss: 0.42687427703663633 test_loss: 0.46047409696801933\n",
      "iteration 9708train_loss: 0.42687427703557923 test_loss: 0.46047410484322565\n",
      "iteration 9709train_loss: 0.4268742770345232 test_loss: 0.4604741127137301\n",
      "iteration 9710train_loss: 0.42687427703346853 test_loss: 0.46047412057953563\n",
      "iteration 9711train_loss: 0.42687427703241515 test_loss: 0.460474128440645\n",
      "iteration 9712train_loss: 0.426874277031363 test_loss: 0.460474136297061\n",
      "iteration 9713train_loss: 0.42687427703031205 test_loss: 0.46047414414878657\n",
      "iteration 9714train_loss: 0.4268742770292624 test_loss: 0.46047415199582425\n",
      "iteration 9715train_loss: 0.42687427702821407 test_loss: 0.46047415983817713\n",
      "iteration 9716train_loss: 0.42687427702716685 test_loss: 0.4604741676758477\n",
      "iteration 9717train_loss: 0.42687427702612096 test_loss: 0.4604741755088391\n",
      "iteration 9718train_loss: 0.42687427702507635 test_loss: 0.4604741833371539\n",
      "iteration 9719train_loss: 0.4268742770240329 test_loss: 0.46047419116079485\n",
      "iteration 9720train_loss: 0.42687427702299063 test_loss: 0.46047419897976494\n",
      "iteration 9721train_loss: 0.42687427702194986 test_loss: 0.46047420679406664\n",
      "iteration 9722train_loss: 0.4268742770209102 test_loss: 0.4604742146037031\n",
      "iteration 9723train_loss: 0.42687427701987163 test_loss: 0.460474222408677\n",
      "iteration 9724train_loss: 0.4268742770188346 test_loss: 0.460474230208991\n",
      "iteration 9725train_loss: 0.4268742770177986 test_loss: 0.46047423800464793\n",
      "iteration 9726train_loss: 0.4268742770167639 test_loss: 0.46047424579565055\n",
      "iteration 9727train_loss: 0.4268742770157304 test_loss: 0.4604742535820017\n",
      "iteration 9728train_loss: 0.4268742770146981 test_loss: 0.46047426136370423\n",
      "iteration 9729train_loss: 0.42687427701366726 test_loss: 0.46047426914076084\n",
      "iteration 9730train_loss: 0.4268742770126374 test_loss: 0.4604742769131741\n",
      "iteration 9731train_loss: 0.42687427701160885 test_loss: 0.46047428468094714\n",
      "iteration 9732train_loss: 0.42687427701058145 test_loss: 0.4604742924440825\n",
      "iteration 9733train_loss: 0.42687427700955544 test_loss: 0.4604743002025831\n",
      "iteration 9734train_loss: 0.42687427700853053 test_loss: 0.46047430795645145\n",
      "iteration 9735train_loss: 0.4268742770075069 test_loss: 0.46047431570569064\n",
      "iteration 9736train_loss: 0.4268742770064845 test_loss: 0.46047432345030315\n",
      "iteration 9737train_loss: 0.42687427700546327 test_loss: 0.46047433119029185\n",
      "iteration 9738train_loss: 0.42687427700444325 test_loss: 0.46047433892565964\n",
      "iteration 9739train_loss: 0.42687427700342445 test_loss: 0.4604743466564091\n",
      "iteration 9740train_loss: 0.42687427700240693 test_loss: 0.460474354382543\n",
      "iteration 9741train_loss: 0.42687427700139063 test_loss: 0.46047436210406417\n",
      "iteration 9742train_loss: 0.42687427700037556 test_loss: 0.46047436982097545\n",
      "iteration 9743train_loss: 0.42687427699936165 test_loss: 0.46047437753327924\n",
      "iteration 9744train_loss: 0.4268742769983489 test_loss: 0.4604743852409788\n",
      "iteration 9745train_loss: 0.4268742769973374 test_loss: 0.4604743929440763\n",
      "iteration 9746train_loss: 0.4268742769963271 test_loss: 0.46047440064257494\n",
      "iteration 9747train_loss: 0.42687427699531805 test_loss: 0.4604744083364773\n",
      "iteration 9748train_loss: 0.4268742769943102 test_loss: 0.4604744160257862\n",
      "iteration 9749train_loss: 0.4268742769933035 test_loss: 0.46047442371050423\n",
      "iteration 9750train_loss: 0.42687427699229813 test_loss: 0.4604744313906343\n",
      "iteration 9751train_loss: 0.4268742769912938 test_loss: 0.4604744390661791\n",
      "iteration 9752train_loss: 0.4268742769902906 test_loss: 0.4604744467371412\n",
      "iteration 9753train_loss: 0.4268742769892888 test_loss: 0.4604744544035236\n",
      "iteration 9754train_loss: 0.42687427698828817 test_loss: 0.46047446206532877\n",
      "iteration 9755train_loss: 0.4268742769872887 test_loss: 0.46047446972255973\n",
      "iteration 9756train_loss: 0.42687427698629044 test_loss: 0.4604744773752191\n",
      "iteration 9757train_loss: 0.42687427698529334 test_loss: 0.4604744850233095\n",
      "iteration 9758train_loss: 0.4268742769842974 test_loss: 0.46047449266683377\n",
      "iteration 9759train_loss: 0.4268742769833027 test_loss: 0.46047450030579445\n",
      "iteration 9760train_loss: 0.4268742769823092 test_loss: 0.4604745079401945\n",
      "iteration 9761train_loss: 0.42687427698131686 test_loss: 0.46047451557003655\n",
      "iteration 9762train_loss: 0.4268742769803257 test_loss: 0.46047452319532334\n",
      "iteration 9763train_loss: 0.42687427697933583 test_loss: 0.46047453081605755\n",
      "iteration 9764train_loss: 0.42687427697834696 test_loss: 0.46047453843224195\n",
      "iteration 9765train_loss: 0.42687427697735936 test_loss: 0.4604745460438793\n",
      "iteration 9766train_loss: 0.4268742769763729 test_loss: 0.4604745536509721\n",
      "iteration 9767train_loss: 0.4268742769753876 test_loss: 0.4604745612535232\n",
      "iteration 9768train_loss: 0.42687427697440355 test_loss: 0.4604745688515355\n",
      "iteration 9769train_loss: 0.4268742769734206 test_loss: 0.46047457644501133\n",
      "iteration 9770train_loss: 0.42687427697243885 test_loss: 0.4604745840339537\n",
      "iteration 9771train_loss: 0.4268742769714584 test_loss: 0.4604745916183651\n",
      "iteration 9772train_loss: 0.426874276970479 test_loss: 0.46047459919824857\n",
      "iteration 9773train_loss: 0.4268742769695008 test_loss: 0.4604746067736064\n",
      "iteration 9774train_loss: 0.4268742769685237 test_loss: 0.46047461434444154\n",
      "iteration 9775train_loss: 0.42687427696754787 test_loss: 0.46047462191075667\n",
      "iteration 9776train_loss: 0.42687427696657315 test_loss: 0.4604746294725543\n",
      "iteration 9777train_loss: 0.4268742769655996 test_loss: 0.4604746370298375\n",
      "iteration 9778train_loss: 0.42687427696462715 test_loss: 0.46047464458260856\n",
      "iteration 9779train_loss: 0.426874276963656 test_loss: 0.4604746521308704\n",
      "iteration 9780train_loss: 0.4268742769626859 test_loss: 0.46047465967462564\n",
      "iteration 9781train_loss: 0.426874276961717 test_loss: 0.46047466721387714\n",
      "iteration 9782train_loss: 0.4268742769607493 test_loss: 0.46047467474862736\n",
      "iteration 9783train_loss: 0.4268742769597826 test_loss: 0.460474682278879\n",
      "iteration 9784train_loss: 0.42687427695881724 test_loss: 0.46047468980463496\n",
      "iteration 9785train_loss: 0.4268742769578529 test_loss: 0.46047469732589774\n",
      "iteration 9786train_loss: 0.4268742769568898 test_loss: 0.46047470484266995\n",
      "iteration 9787train_loss: 0.42687427695592783 test_loss: 0.4604747123549545\n",
      "iteration 9788train_loss: 0.42687427695496705 test_loss: 0.46047471986275396\n",
      "iteration 9789train_loss: 0.42687427695400726 test_loss: 0.460474727366071\n",
      "iteration 9790train_loss: 0.4268742769530487 test_loss: 0.46047473486490825\n",
      "iteration 9791train_loss: 0.4268742769520913 test_loss: 0.4604747423592685\n",
      "iteration 9792train_loss: 0.4268742769511351 test_loss: 0.4604747498491542\n",
      "iteration 9793train_loss: 0.42687427695018 test_loss: 0.4604747573345684\n",
      "iteration 9794train_loss: 0.426874276949226 test_loss: 0.4604747648155134\n",
      "iteration 9795train_loss: 0.4268742769482731 test_loss: 0.4604747722919921\n",
      "iteration 9796train_loss: 0.4268742769473214 test_loss: 0.460474779764007\n",
      "iteration 9797train_loss: 0.4268742769463708 test_loss: 0.4604747872315609\n",
      "iteration 9798train_loss: 0.42687427694542135 test_loss: 0.46047479469465646\n",
      "iteration 9799train_loss: 0.4268742769444731 test_loss: 0.46047480215329617\n",
      "iteration 9800train_loss: 0.4268742769435259 test_loss: 0.460474809607483\n",
      "iteration 9801train_loss: 0.42687427694257996 test_loss: 0.46047481705721927\n",
      "iteration 9802train_loss: 0.42687427694163504 test_loss: 0.4604748245025079\n",
      "iteration 9803train_loss: 0.42687427694069124 test_loss: 0.46047483194335126\n",
      "iteration 9804train_loss: 0.42687427693974855 test_loss: 0.46047483937975225\n",
      "iteration 9805train_loss: 0.4268742769388071 test_loss: 0.46047484681171347\n",
      "iteration 9806train_loss: 0.42687427693786667 test_loss: 0.4604748542392376\n",
      "iteration 9807train_loss: 0.4268742769369274 test_loss: 0.46047486166232715\n",
      "iteration 9808train_loss: 0.42687427693598917 test_loss: 0.46047486908098484\n",
      "iteration 9809train_loss: 0.42687427693505214 test_loss: 0.4604748764952134\n",
      "iteration 9810train_loss: 0.4268742769341163 test_loss: 0.4604748839050155\n",
      "iteration 9811train_loss: 0.42687427693318153 test_loss: 0.46047489131039354\n",
      "iteration 9812train_loss: 0.4268742769322478 test_loss: 0.46047489871135044\n",
      "iteration 9813train_loss: 0.42687427693131524 test_loss: 0.4604749061078885\n",
      "iteration 9814train_loss: 0.4268742769303837 test_loss: 0.46047491350001074\n",
      "iteration 9815train_loss: 0.42687427692945346 test_loss: 0.4604749208877196\n",
      "iteration 9816train_loss: 0.4268742769285242 test_loss: 0.4604749282710177\n",
      "iteration 9817train_loss: 0.42687427692759605 test_loss: 0.4604749356499077\n",
      "iteration 9818train_loss: 0.42687427692666907 test_loss: 0.4604749430243923\n",
      "iteration 9819train_loss: 0.42687427692574315 test_loss: 0.460474950394474\n",
      "iteration 9820train_loss: 0.42687427692481833 test_loss: 0.46047495776015557\n",
      "iteration 9821train_loss: 0.4268742769238947 test_loss: 0.4604749651214396\n",
      "iteration 9822train_loss: 0.42687427692297203 test_loss: 0.4604749724783286\n",
      "iteration 9823train_loss: 0.4268742769220506 test_loss: 0.4604749798308253\n",
      "iteration 9824train_loss: 0.42687427692113017 test_loss: 0.46047498717893237\n",
      "iteration 9825train_loss: 0.4268742769202109 test_loss: 0.4604749945226522\n",
      "iteration 9826train_loss: 0.42687427691929275 test_loss: 0.4604750018619877\n",
      "iteration 9827train_loss: 0.4268742769183756 test_loss: 0.46047500919694134\n",
      "iteration 9828train_loss: 0.4268742769174595 test_loss: 0.46047501652751577\n",
      "iteration 9829train_loss: 0.4268742769165447 test_loss: 0.46047502385371364\n",
      "iteration 9830train_loss: 0.42687427691563085 test_loss: 0.4604750311755374\n",
      "iteration 9831train_loss: 0.4268742769147182 test_loss: 0.46047503849298976\n",
      "iteration 9832train_loss: 0.42687427691380647 test_loss: 0.46047504580607335\n",
      "iteration 9833train_loss: 0.426874276912896 test_loss: 0.46047505311479087\n",
      "iteration 9834train_loss: 0.4268742769119865 test_loss: 0.4604750604191447\n",
      "iteration 9835train_loss: 0.4268742769110781 test_loss: 0.4604750677191376\n",
      "iteration 9836train_loss: 0.42687427691017077 test_loss: 0.46047507501477225\n",
      "iteration 9837train_loss: 0.4268742769092646 test_loss: 0.4604750823060511\n",
      "iteration 9838train_loss: 0.42687427690835944 test_loss: 0.4604750895929768\n",
      "iteration 9839train_loss: 0.42687427690745544 test_loss: 0.46047509687555194\n",
      "iteration 9840train_loss: 0.42687427690655244 test_loss: 0.46047510415377907\n",
      "iteration 9841train_loss: 0.4268742769056506 test_loss: 0.4604751114276609\n",
      "iteration 9842train_loss: 0.42687427690474977 test_loss: 0.4604751186971999\n",
      "iteration 9843train_loss: 0.42687427690385005 test_loss: 0.46047512596239876\n",
      "iteration 9844train_loss: 0.4268742769029514 test_loss: 0.4604751332232601\n",
      "iteration 9845train_loss: 0.42687427690205376 test_loss: 0.4604751404797864\n",
      "iteration 9846train_loss: 0.4268742769011572 test_loss: 0.4604751477319803\n",
      "iteration 9847train_loss: 0.4268742769002618 test_loss: 0.46047515497984437\n",
      "iteration 9848train_loss: 0.4268742768993674 test_loss: 0.46047516222338125\n",
      "iteration 9849train_loss: 0.4268742768984742 test_loss: 0.4604751694625934\n",
      "iteration 9850train_loss: 0.42687427689758184 test_loss: 0.46047517669748356\n",
      "iteration 9851train_loss: 0.42687427689669066 test_loss: 0.4604751839280542\n",
      "iteration 9852train_loss: 0.42687427689580065 test_loss: 0.46047519115430785\n",
      "iteration 9853train_loss: 0.4268742768949116 test_loss: 0.46047519837624734\n",
      "iteration 9854train_loss: 0.4268742768940235 test_loss: 0.46047520559387495\n",
      "iteration 9855train_loss: 0.4268742768931367 test_loss: 0.46047521280719345\n",
      "iteration 9856train_loss: 0.42687427689225077 test_loss: 0.46047522001620517\n",
      "iteration 9857train_loss: 0.4268742768913659 test_loss: 0.46047522722091305\n",
      "iteration 9858train_loss: 0.4268742768904822 test_loss: 0.4604752344213194\n",
      "iteration 9859train_loss: 0.4268742768895995 test_loss: 0.46047524161742687\n",
      "iteration 9860train_loss: 0.42687427688871776 test_loss: 0.460475248809238\n",
      "iteration 9861train_loss: 0.42687427688783725 test_loss: 0.4604752559967553\n",
      "iteration 9862train_loss: 0.42687427688695767 test_loss: 0.46047526317998155\n",
      "iteration 9863train_loss: 0.42687427688607915 test_loss: 0.46047527035891894\n",
      "iteration 9864train_loss: 0.42687427688520174 test_loss: 0.4604752775335703\n",
      "iteration 9865train_loss: 0.4268742768843254 test_loss: 0.4604752847039383\n",
      "iteration 9866train_loss: 0.42687427688345 test_loss: 0.4604752918700251\n",
      "iteration 9867train_loss: 0.42687427688257573 test_loss: 0.46047529903183365\n",
      "iteration 9868train_loss: 0.4268742768817024 test_loss: 0.4604753061893663\n",
      "iteration 9869train_loss: 0.4268742768808303 test_loss: 0.46047531334262565\n",
      "iteration 9870train_loss: 0.4268742768799591 test_loss: 0.4604753204916142\n",
      "iteration 9871train_loss: 0.42687427687908885 test_loss: 0.4604753276363346\n",
      "iteration 9872train_loss: 0.4268742768782199 test_loss: 0.4604753347767894\n",
      "iteration 9873train_loss: 0.4268742768773517 test_loss: 0.4604753419129809\n",
      "iteration 9874train_loss: 0.4268742768764847 test_loss: 0.46047534904491205\n",
      "iteration 9875train_loss: 0.42687427687561874 test_loss: 0.460475356172585\n",
      "iteration 9876train_loss: 0.42687427687475377 test_loss: 0.4604753632960026\n",
      "iteration 9877train_loss: 0.4268742768738898 test_loss: 0.46047537041516723\n",
      "iteration 9878train_loss: 0.4268742768730269 test_loss: 0.46047537753008144\n",
      "iteration 9879train_loss: 0.42687427687216506 test_loss: 0.4604753846407478\n",
      "iteration 9880train_loss: 0.4268742768713042 test_loss: 0.4604753917471689\n",
      "iteration 9881train_loss: 0.42687427687044444 test_loss: 0.46047539884934713\n",
      "iteration 9882train_loss: 0.4268742768695857 test_loss: 0.4604754059472851\n",
      "iteration 9883train_loss: 0.4268742768687278 test_loss: 0.4604754130409854\n",
      "iteration 9884train_loss: 0.42687427686787127 test_loss: 0.4604754201304504\n",
      "iteration 9885train_loss: 0.4268742768670154 test_loss: 0.4604754272156828\n",
      "iteration 9886train_loss: 0.42687427686616075 test_loss: 0.46047543429668514\n",
      "iteration 9887train_loss: 0.4268742768653071 test_loss: 0.46047544137345964\n",
      "iteration 9888train_loss: 0.42687427686445456 test_loss: 0.4604754484460093\n",
      "iteration 9889train_loss: 0.4268742768636028 test_loss: 0.4604754555143362\n",
      "iteration 9890train_loss: 0.42687427686275226 test_loss: 0.4604754625784432\n",
      "iteration 9891train_loss: 0.4268742768619027 test_loss: 0.46047546963833264\n",
      "iteration 9892train_loss: 0.42687427686105406 test_loss: 0.460475476694007\n",
      "iteration 9893train_loss: 0.4268742768602064 test_loss: 0.46047548374546887\n",
      "iteration 9894train_loss: 0.42687427685935997 test_loss: 0.4604754907927209\n",
      "iteration 9895train_loss: 0.4268742768585144 test_loss: 0.46047549783576525\n",
      "iteration 9896train_loss: 0.42687427685766993 test_loss: 0.4604755048746047\n",
      "iteration 9897train_loss: 0.4268742768568264 test_loss: 0.4604755119092418\n",
      "iteration 9898train_loss: 0.4268742768559838 test_loss: 0.460475518939679\n",
      "iteration 9899train_loss: 0.42687427685514234 test_loss: 0.4604755259659187\n",
      "iteration 9900train_loss: 0.42687427685430185 test_loss: 0.46047553298796334\n",
      "iteration 9901train_loss: 0.4268742768534623 test_loss: 0.46047554000581575\n",
      "iteration 9902train_loss: 0.4268742768526238 test_loss: 0.4604755470194782\n",
      "iteration 9903train_loss: 0.4268742768517863 test_loss: 0.46047555402895324\n",
      "iteration 9904train_loss: 0.4268742768509498 test_loss: 0.46047556103424325\n",
      "iteration 9905train_loss: 0.4268742768501143 test_loss: 0.46047556803535095\n",
      "iteration 9906train_loss: 0.4268742768492797 test_loss: 0.46047557503227876\n",
      "iteration 9907train_loss: 0.4268742768484463 test_loss: 0.4604755820250291\n",
      "iteration 9908train_loss: 0.4268742768476138 test_loss: 0.4604755890136045\n",
      "iteration 9909train_loss: 0.42687427684678225 test_loss: 0.4604755959980075\n",
      "iteration 9910train_loss: 0.42687427684595175 test_loss: 0.46047560297824064\n",
      "iteration 9911train_loss: 0.4268742768451222 test_loss: 0.4604756099543062\n",
      "iteration 9912train_loss: 0.42687427684429363 test_loss: 0.46047561692620675\n",
      "iteration 9913train_loss: 0.42687427684346607 test_loss: 0.4604756238939449\n",
      "iteration 9914train_loss: 0.4268742768426395 test_loss: 0.4604756308575231\n",
      "iteration 9915train_loss: 0.426874276841814 test_loss: 0.46047563781694373\n",
      "iteration 9916train_loss: 0.42687427684098933 test_loss: 0.4604756447722093\n",
      "iteration 9917train_loss: 0.4268742768401657 test_loss: 0.4604756517233224\n",
      "iteration 9918train_loss: 0.42687427683934315 test_loss: 0.46047565867028534\n",
      "iteration 9919train_loss: 0.4268742768385214 test_loss: 0.4604756656131008\n",
      "iteration 9920train_loss: 0.4268742768377008 test_loss: 0.460475672551771\n",
      "iteration 9921train_loss: 0.4268742768368811 test_loss: 0.46047567948629864\n",
      "iteration 9922train_loss: 0.4268742768360624 test_loss: 0.46047568641668607\n",
      "iteration 9923train_loss: 0.4268742768352447 test_loss: 0.4604756933429359\n",
      "iteration 9924train_loss: 0.42687427683442797 test_loss: 0.46047570026505036\n",
      "iteration 9925train_loss: 0.4268742768336122 test_loss: 0.4604757071830322\n",
      "iteration 9926train_loss: 0.42687427683279733 test_loss: 0.46047571409688376\n",
      "iteration 9927train_loss: 0.42687427683198353 test_loss: 0.4604757210066074\n",
      "iteration 9928train_loss: 0.4268742768311707 test_loss: 0.46047572791220576\n",
      "iteration 9929train_loss: 0.4268742768303588 test_loss: 0.4604757348136811\n",
      "iteration 9930train_loss: 0.4268742768295478 test_loss: 0.4604757417110361\n",
      "iteration 9931train_loss: 0.42687427682873796 test_loss: 0.4604757486042731\n",
      "iteration 9932train_loss: 0.42687427682792883 test_loss: 0.4604757554933946\n",
      "iteration 9933train_loss: 0.4268742768271209 test_loss: 0.46047576237840293\n",
      "iteration 9934train_loss: 0.4268742768263139 test_loss: 0.4604757692593007\n",
      "iteration 9935train_loss: 0.42687427682550777 test_loss: 0.46047577613609036\n",
      "iteration 9936train_loss: 0.42687427682470275 test_loss: 0.46047578300877434\n",
      "iteration 9937train_loss: 0.42687427682389856 test_loss: 0.46047578987735505\n",
      "iteration 9938train_loss: 0.42687427682309526 test_loss: 0.460475796741835\n",
      "iteration 9939train_loss: 0.42687427682229306 test_loss: 0.4604758036022165\n",
      "iteration 9940train_loss: 0.42687427682149176 test_loss: 0.4604758104585022\n",
      "iteration 9941train_loss: 0.4268742768206914 test_loss: 0.4604758173106944\n",
      "iteration 9942train_loss: 0.42687427681989204 test_loss: 0.46047582415879557\n",
      "iteration 9943train_loss: 0.42687427681909357 test_loss: 0.4604758310028082\n",
      "iteration 9944train_loss: 0.42687427681829615 test_loss: 0.4604758378427346\n",
      "iteration 9945train_loss: 0.4268742768174997 test_loss: 0.46047584467857755\n",
      "iteration 9946train_loss: 0.42687427681670415 test_loss: 0.46047585151033904\n",
      "iteration 9947train_loss: 0.42687427681590945 test_loss: 0.46047585833802174\n",
      "iteration 9948train_loss: 0.4268742768151158 test_loss: 0.4604758651616281\n",
      "iteration 9949train_loss: 0.4268742768143231 test_loss: 0.4604758719811605\n",
      "iteration 9950train_loss: 0.4268742768135313 test_loss: 0.46047587879662133\n",
      "iteration 9951train_loss: 0.42687427681274054 test_loss: 0.46047588560801317\n",
      "iteration 9952train_loss: 0.42687427681195067 test_loss: 0.4604758924153382\n",
      "iteration 9953train_loss: 0.4268742768111616 test_loss: 0.46047589921859916\n",
      "iteration 9954train_loss: 0.42687427681037365 test_loss: 0.4604759060177983\n",
      "iteration 9955train_loss: 0.42687427680958656 test_loss: 0.46047591281293804\n",
      "iteration 9956train_loss: 0.4268742768088005 test_loss: 0.4604759196040208\n",
      "iteration 9957train_loss: 0.4268742768080153 test_loss: 0.460475926391049\n",
      "iteration 9958train_loss: 0.4268742768072311 test_loss: 0.4604759331740252\n",
      "iteration 9959train_loss: 0.4268742768064477 test_loss: 0.46047593995295166\n",
      "iteration 9960train_loss: 0.4268742768056654 test_loss: 0.4604759467278309\n",
      "iteration 9961train_loss: 0.426874276804884 test_loss: 0.4604759534986651\n",
      "iteration 9962train_loss: 0.42687427680410345 test_loss: 0.46047596026545695\n",
      "iteration 9963train_loss: 0.42687427680332396 test_loss: 0.4604759670282088\n",
      "iteration 9964train_loss: 0.4268742768025453 test_loss: 0.4604759737869231\n",
      "iteration 9965train_loss: 0.42687427680176754 test_loss: 0.4604759805416021\n",
      "iteration 9966train_loss: 0.4268742768009907 test_loss: 0.46047598729224826\n",
      "iteration 9967train_loss: 0.4268742768002149 test_loss: 0.46047599403886413\n",
      "iteration 9968train_loss: 0.42687427679944 test_loss: 0.4604760007814519\n",
      "iteration 9969train_loss: 0.4268742767986661 test_loss: 0.4604760075200142\n",
      "iteration 9970train_loss: 0.4268742767978929 test_loss: 0.46047601425455337\n",
      "iteration 9971train_loss: 0.4268742767971207 test_loss: 0.4604760209850717\n",
      "iteration 9972train_loss: 0.4268742767963495 test_loss: 0.4604760277115716\n",
      "iteration 9973train_loss: 0.4268742767955792 test_loss: 0.46047603443405555\n",
      "iteration 9974train_loss: 0.4268742767948098 test_loss: 0.46047604115252594\n",
      "iteration 9975train_loss: 0.42687427679404133 test_loss: 0.46047604786698515\n",
      "iteration 9976train_loss: 0.4268742767932737 test_loss: 0.46047605457743557\n",
      "iteration 9977train_loss: 0.4268742767925071 test_loss: 0.46047606128387963\n",
      "iteration 9978train_loss: 0.4268742767917414 test_loss: 0.46047606798631974\n",
      "iteration 9979train_loss: 0.4268742767909766 test_loss: 0.46047607468475804\n",
      "iteration 9980train_loss: 0.4268742767902127 test_loss: 0.4604760813791974\n",
      "iteration 9981train_loss: 0.42687427678944967 test_loss: 0.46047608806963974\n",
      "iteration 9982train_loss: 0.42687427678868767 test_loss: 0.46047609475608753\n",
      "iteration 9983train_loss: 0.42687427678792644 test_loss: 0.46047610143854345\n",
      "iteration 9984train_loss: 0.4268742767871662 test_loss: 0.46047610811700973\n",
      "iteration 9985train_loss: 0.4268742767864069 test_loss: 0.4604761147914885\n",
      "iteration 9986train_loss: 0.4268742767856484 test_loss: 0.4604761214619825\n",
      "iteration 9987train_loss: 0.4268742767848909 test_loss: 0.4604761281284938\n",
      "iteration 9988train_loss: 0.42687427678413425 test_loss: 0.46047613479102517\n",
      "iteration 9989train_loss: 0.42687427678337847 test_loss: 0.4604761414495786\n",
      "iteration 9990train_loss: 0.42687427678262363 test_loss: 0.4604761481041567\n",
      "iteration 9991train_loss: 0.4268742767818698 test_loss: 0.4604761547547617\n",
      "iteration 9992train_loss: 0.4268742767811167 test_loss: 0.460476161401396\n",
      "iteration 9993train_loss: 0.4268742767803647 test_loss: 0.4604761680440621\n",
      "iteration 9994train_loss: 0.4268742767796134 test_loss: 0.46047617468276236\n",
      "iteration 9995train_loss: 0.426874276778863 test_loss: 0.4604761813174989\n",
      "iteration 9996train_loss: 0.42687427677811357 test_loss: 0.46047618794827433\n",
      "iteration 9997train_loss: 0.42687427677736506 test_loss: 0.4604761945750908\n",
      "iteration 9998train_loss: 0.4268742767766174 test_loss: 0.46047620119795096\n",
      "iteration 9999train_loss: 0.4268742767758707 test_loss: 0.460476207816857\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coeff,loss_dict = Gradient_Descent(trainx,trainy.values,testx,testy.values,lr=0.05,reg=0,iters=10000)\n",
    "loss_dict.plot()\n",
    "\n",
    "\n",
    "#setting cutoff for class prediction from probability values\n",
    "cutoff=np.mean(trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(data,coeff):\n",
    "    return sigmoid(data.dot(coeff))\n",
    "\n",
    "def predict(data,coeff,cutoff):\n",
    "    return (sigmoid(data.dot(coeff))>cutoff)*1.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction= predict(trainx,coeff,cutoff)\n",
    "test_prediction=  predict(testx,coeff,cutoff)\n",
    "\n",
    "def accuracy(datay,prediction):\n",
    "    m= datay.shape[0]\n",
    "    return np.mean(datay.reshape(m,)==prediction.reshape(m,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_accuracy= accuracy(trainy.values,train_prediction)\n",
    "test_accuracy= accuracy(testy.values,test_prediction)\n",
    "\n",
    "def confusion_matrix(datay,prediction):\n",
    "    m=datay.shape[0]\n",
    "    confmat=pd.crosstab(datay.reshape(m,),prediction.reshape(m,))\n",
    "    return confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat=confusion_matrix(testy.values,test_prediction)\n",
    "#Finding specificty and sensitivity of model at specified cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(confmat):\n",
    "    confmat=confmat.values\n",
    "    specificity=confmat[0,0]/np.sum(confmat[0,:])\n",
    "    sensitivity=confmat[1,1]/np.sum(confmat[1,:])\n",
    "    return sensitivity,specificity\n",
    "\n",
    "sens,spec=metric(confmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Roc curves and attempting to find auc value\n",
    "\n",
    "def plot_roccurve(data,datay,coeff,num_points=1000):\n",
    "    Sensitivity=[]\n",
    "    Specificity=[]\n",
    "    p = np.linspace(0.05,0.95,num_points)\n",
    "    for index in np.arange(len(p)):\n",
    "        data_prediction=predict(data,coeff,p[index])\n",
    "        confmat= confusion_matrix(datay,data_prediction)\n",
    "        sens,spec=metric(confmat)\n",
    "        Sensitivity.append(sens)\n",
    "        Specificity.append(spec)\n",
    "    #ROC Curves\n",
    "    plt.scatter(1-np.array(Specificity),np.array(Sensitivity))\n",
    "    x= np.linspace(0,1,50)\n",
    "    plt.plot(x,x, linestyle='solid',color='red')\n",
    "    plt.xlabel('1-Specificity') \n",
    "    plt.ylabel('Sensitivity') \n",
    "    plt.title('ROC_CURVE')      \n",
    "    return np.array(Specificity),np.array(Sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xnc1XP6x/HX5VYJJURDSg1ZUqbMjcIYDD8ZpgzGiGyDGLKPwWSMyZYte6gYWcYaCSUhhEp3oo1IogWFylJar98fn3Pfc7o7932f++58z/cs7+fjcT/uc77ne865vi3nOp/t+pi7IyIiArBB3AGIiEjuUFIQEZEKSgoiIlJBSUFERCooKYiISAUlBRERqaCkICIiFZQURESkgpKC5DUzm21my8zsRzP7ysweNLNNkx7fx8xeM7MfzGyJmT1vZm0rvUZjM7vNzL5IvM7MxP2mabz/8WZWlnjel2Y2wsz2Szz2oJldU+n8VmbmZrZhTfGb2eVm9maK92xqZivMrJ2ZnWJmqxPPT/7Ztm5/olLslBSkEPzB3TcFOgAdgcsBzKwz8DLwHLAt0Br4AHjbzH6ZOKc+8CqwG9AFaAzsA3wL7FXdm5rZRcBtwHVAM6Al0B/olon4gYeBfcysdaXzjwOmuPvUxP2x7r5ppZ/5tYxBBIAN4w5AJFPc/SszG0n4cAW4EXjI3W9POu0KM/s1cBVwUuKnJXCgu/+YOGcBcHV172VmmwF9gFPd/Zmkh55P/Kx3/O4+18xeA05MvFe5k4DBdXkPkZqopSAFw8y2Aw4DZprZxoRv/E+lOPVJ4JDE7YOBl5ISQro6AxsBz9Yx3HUkx590eDAhKZSfszMhaTyWqfcVSaakIIVgqJn9AMwhfMv/F7AF4d/3lynO/xIoHy/YsopzarIl8I27r6rDcytLFX+5Z4FmZrZP4v5JwAh3X5h0TiczW5z082kGYpIipaQgheBId28EHADsQvjAXwSsAbZJcf42wDeJ299WcU5NvgWalg8YV2EVUK/SsXqJuNYkHUsVPwDuvpTQ2jnJzAw4gXW7jsa5e5Oknx3qcD0igJKCFBB3fwN4ELjZ3X8CxgJ/SnHqsYTBZYBXgEPNbJNavt1Y4GfgyGrO+QJoVelYa2COu6+pfHJy/JUeGpyI+RCgEfBCLWMVSZuSghSa24BDzKwDcBlwspmdZ2aNzGzzxBTRzsC/E+c/TOi2GWJmu5jZBma2pZn9w8x+X9WbuPsS4ErgbjM70sw2NrN6ZnaYmd2YOG0IcLiZ/Z+ZlSSmiV4BPJ5m/OXGAIuBAcDj7r6itn8oIulSUpCCkuhrfwj4p7u/BRwKHEUYN/icMOVzP3f/JHH+csJg80fAKOB74F1CF874Gt6rH3AR4YN+ISG59AKGJh6fBnQHrge+I7QuxvO/hFRt/EnHPHFs+8TvyjqnWKewZ3Wxi1TFtPOaiIiUU0tBREQqaPGaSBXMrCUwvYqH27r7F9mMRyQb1H0kIiIV8q6l0LRpU2/VqlXcYYiI5JWJEyd+4+5b1XRe3iWFVq1aUVZWFncYIiJ5xcw+T+c8DTSLiEgFJQUREamgpCAiIhWUFEREpIKSgoiIVFBSEBGRCpElBTN7wMwWmNnUKh43M7sjsUn6ZDPbI6pYREQkPVG2FB4kbIRelcOANomfnsA9EcYiIiJpiGzxmru/aWatqjmlG2FTdQfGmVkTM9vG3euyNaKISLWGTprHTSNnMG/xMkrMWO2+zu9UmjWqzzc/rlzr8RIz6m0AP69Ov0xQ8yYNabVlQ8bNWrTWazVv0pBLDt2ZIzs2r/rJixfDZZfBxRdDmzZpv2ddxLmiuTmh/ny5uYlj6yQFM+tJaE3QsmXLrAQnIoVj6KR5XP7MFJatXA1Q8aFc+XcqX/+w7p5Gq91Zvbp2McxbvIx5i5elPH75M1MAUieGoUPh7LNhwQLYc8+CTgqW4ljKvxl3H0DYdYrS0lJV8BMpMOXf4ucvXsa2TRpy4C5bMfqjhWt9q0/rG3UVbho5oyIh5KJlK1dz08gZa1/b11/DuefCU09Bhw7wwguwR/RDr3EmhblAi6T72wHzY4pFRGJS+Vv8vMXLeGTc/6qSl3+Lr/EbdTXmp/iGnmsqYnSHhx6CCy+EpUvhuuvgb3+DevWyEkecSWEY0MvMHgf2BpZoPEGkcAydNI9/Pz+NRUtXAtCkYT1227ZRRZ96iRnd927B6I8Wpv0tPuU36jRs26Rhyq6bXLJtk4bw+edw5pkwciTsuy8MGgS77JLVOCJLCmb2GHAA0NTM5gL/AuoBuPu9wHDg98BMYClwalSxiEh2DZ00j4uefJ81SZ29i5et5O1Pv6u4v9p9rRZBuuryrf+SQ3deqzWSazbe0Lhn0duwW9dw4K674K9/hQ2yv5QsytlH3Wt43IFzonp/EYnPVcOmrZUQMmnbJg1r/ZzylkUuzj7aZ8UC7nypP1t+UAZdusC998L229f6GjMl7/ZTEJHsqG7wN1mbrTdh6Yo169U907BeSVrf4hvWK+GSQ3eu03sc2bF5nQapI7NyJdx4I/TpA5tuCg8/DCecAJZqDk72qMyFiKyjfPB33uJlOP8b/E31wf/Jgp/WKyGUmHH9Ue1p3qQhRvhG3aNTS5onWgQliQ/J5k0acv1R7XPrg72uJk6E0lK44gr44x/hww+hR4/YEwKopSBSFK4YOoXHxs9Za4D3miPbr/NYueq6UzKt+94tcu9bfFSWLYOrroKbb4ZmzcIahG7d4o5qLUoKIgXuiqFT1pnimXw/1WBvVAmhzdabMGvh0pTJqeC98QacfjrMnAlnnBG6jpo0iTuqdSgpiOSwqr7hp1OyoXyx12Pj56R87aqOR6HEjFuO/VVxtAYqW7IELr0U7rsPfvlLePVVOOiguKOqknmWmoiZUlpa6mVlZXGHIRKJ5CRgpF7iv+8OW/DeF0vSHpiNexpmw3olhTMWUFsvvABnnQVffhkWo/XpAxtvHEsoZjbR3UtrOk8DzSI5orybp7zrpqqva29/+l2tFntVpyQDA5tttt6kYlC4soIaHK6NhQvh+OPhD3+AzTeHsWPDOEJMCaE21H0kkiOy2Z1TrvveLWq1gKxHp5bFMwZQF+7w2GNw3nnw/fehZXDppVC/ftyRpU0tBZEcka3ZPuVKzLjmyPb06NRynRZD8yYN2XeHLSqOl5gpIdRkzpzQMjjhhFDJdNIk+Oc/8yohgFoKIllzSL/X+WTBT2sdKx8Mfqos/W/rDTbcgA3M0h5T2KPlZmuVlyjXfe9Qj/KaI9vrw359rFkDAwbA3/8Oq1fDbbdBr15QUhJ3ZHWipCCSBakSAoRFYZVrBFWnZAPjhqN3B2ou2ZBcarq6dQqyHj7+OEwvffNNOPjgkBxat447qvWi2UciWdDqshfr/NwSM9a4s+167CcgGbZqFdxyC/zrX9CwIfTrB6eckhMrkquS7uwjtRREclhRT+fMVe+/D6edBu+9B0cdFSqabrNN3FFljAaaRXKYEkIO+fln6N071CyaNw+efhqGDCmohABqKYhUOGHg2HUGZGuqAbQ+W0SW28BIOaaw7w5bKCHkirfeCiUqZsyAk08O3UVbbBF3VJFQS0GE1AkBap4mWr5F5NBJ8+r83v2O7cC+O6z9AbPvDlvw6Bmd6/yakiE//BBmEv3mN7B8edgR7cEHCzYhgFoKUqBSVf5sXs2eAOujrltEliuaCqH5ZsSIsDXm3Llw/vlwzTVh34MCp6QgBadyVdBylTeEz6R82Bhe0vTtt6FO0cMPw667wttvQ+fiabUpKUhe2fvaUXz9w4q4w1hHXbaIlBzjDk89FbqLFi0Kq5F794YGDeKOLKuUFCRv5GpCSGeLyGaN6qeMvVmj/CqBULDmz4ezz4bnnguzi155BXbfPe6oYqGBZskbcSSEmqqIplsFdHzvQ9ZJAM0a1Wd870PWO0ZZD+4wcCC0bRsGkW+6KVQ0LdKEAGopSIxSDQaXM+CzvofT+rIXqywhHaU2W2/CqIsOyOhrKgHkmE8/DSUqRo+GAw4IyWHHHeOOKnZqKUgsKu8dUJkTSkNkMiFU3hC+KlEkBMkh5SUq2reHiRNDvaLXXlNCSFBLQbKuquJwmaa5/rKOKVNCiYoJE6BrV+jfH5prOnAytRQkq5QQJBbLl4fidXvsAbNnwxNPwNChSggpqKUgWRVVQpjd9/BIXlcKwLhxoXUwfTqceCLceitsuWXcUeUstRQk7/Xo1DLuECQX/fRTWIS2zz6hXMXw4fDQQ0oINVBSkJxkhG//1U0I1RaRUqVXXoF27cIuaGedBVOnwmGHxR1VXlD3kWRMdVNM05GqC+gzdQtJbSxaBH/7GzzwAOy0U9gR7Te/iTuqvKKWgmRETVNMRSL3zDNhEdrgwXD55fDBB0oIdRBpS8HMugC3AyXAIHfvW+nxlsBgoEninMvcfXiUMUnmZGsmkUi1vvoq1CsaMgQ6dAhjBx07xh1V3oqspWBmJcDdwGFAW6C7mbWtdNoVwJPu3hE4DugfVTySWUoIEjv3sLdB27bwwgtw3XXw7rtKCOspypbCXsBMd58FYGaPA92A6UnnONA4cXszYH6E8Ugtrc9m8yKRmj0bevaEUaNgv/1g0CDYufqihJKeKMcUmgNzku7PTRxLdhXQw8zmAsOBcyOMR2oh2wmhcYOSrL6f5KnVq+GOO8LMorFj4e674Y03lBAyKMqWQqrZhJVHIbsDD7r7LWbWGXjYzNq5+5q1XsisJ9AToGVLzUnPpFzoBmrcoITJ/+4SawySB6ZPD4vQxo0L00vvvRf0eZBxUSaFuUCLpPvbsW730GlAFwB3H2tmGwFNgQXJJ7n7AGAAQGlpqaa3ZEg2EsJGJcZH1/4+0veQArdiBdxwQ9gOs1GjsCPaCSdADWXNpW6i7D6aALQxs9ZmVp8wkDys0jlfAL8DMLNdgY2AhRHGJEmUECTnTZgQNr258ko46qjQWujRQwkhQpG1FNx9lZn1AkYSpps+4O7TzKwPUObuw4CLgYFmdiGha+kUd010r4tc2ZWsfB8EkfWydGkoYNevH/ziF2FHtK5d446qKES6TiGx5mB4pWNXJt2eDuwbZQzFQAlBCsro0XD66TBrVphhdOONsNlmcUdVNFTmogBkOyGoIqlEYskSuOSSsAPaDjuEjW8OPDDuqIqOylxIrSghSCSGDQuL0O6/PySGyZOVEGKiloKkpA9/yYoFC+C888KmN+3bh7GD0tK4oypqainIOjSvQyLnDo88ArvuCs8+C1dfDWVlSgg5QC0FWYsGiyVyX3wR9jgYMQI6dw5dRrvuGndUkqCkkOdqKkehbiDJGWvWhFXIl14aWgp33AFnnw0lKnGSS5QU8pgK1knemDEjTDN96y045BAYMABatYo7KklBSSFPKAFIXlq5Em65Ba66CjbeOJS6PukkrUjOYUoKeUAJQfLSpEnwl7/A++/DMcfAnXeG1cmS05QUsihVAbrmTRoyb/GySN5voxJ9G5MYLFsGffrATTfBVluFHdGOOiruqCRNSgpZUlVF0igTgorRSdaNGRPGDj7+OLQSbr4ZNt887qikFrROIUuyuWdB8yYNlRAku77/Hs45B/bfP5S6HjUqTDVVQsg7ailE6ISBY3n70++y+p4N65VwyaHahUqyaPjwsO5g7ly44IKw78Emm8QdldSRkkJE4kgIzZs05JJDd+bIjpV3PRWJwDffwIUXhpXJbdvCO+9Ap05xRyXrSUkhItlICBo3kFi4h1pF554LixeHDXD+8Q9o0CDuyCQDlBTylBKCxGLevLAKedgw2HPPMG7Qvn3cUUkGKSlk0I6Xv8iqOuwbp1IUkvPWrIFBg0JZ6/IFaeefrxIVBUhJIUPqmhDabK0BOclxM2fCGWfA66+HPQ7KN8GRgqQpqRlS14Qw6qIDMh6LSEasWhXWGbRvH1YnDxoEr76qhFDg1FKIgbqLJOdNngynnRb2OOjWDfr3h223jTsqyQIlhfWgmkRScJYvD+sM+vaFLbaAJ58MdYtUwK5oKCnUUV0Twob6vyW56p13QomKDz+EE0+EW2+FLbeMOyrJMo0pZNGGBjOvV9eR5JgffwwzifbbD376KeyI9tBDSghFSi2FOtil9/Bana8xBMlZL78MPXvC559Dr15w3XXQqFHcUUmM1FKopV16D+fn1XWYaiSSS777Dk49FQ49FDbaKFQ3vfNOJQRRUqit2iaEZo3qRxSJSB0NGRJqFT38cChP8f77oetIBHUfRapZo/qM731I3GGIBF9+GbqInnkG9tgDXnoJOnSIOyrJMUoKEdAYguQUd/jPf+Dii+Hnn+GGG+Cii2BD/feXdelfRS1tVGIaU5D8MWsWnHkmvPJK2ABn4EDYaae4o5IcpjGFWvro2t9Xu/exWgmSE1avhttuCyUqxo+He+6B0aOVEKRGabUUzGwI8AAwwt3XRBtS7lPJaslp06aFEhXjx8Phh4eE0KJF3FFJnki3pXAPcDzwiZn1NbNd0nmSmXUxsxlmNtPMLqvinGPNbLqZTTOz/6YZj4hUtmIF9OkDHTuGyqaPPgrPP6+EILWSVkvB3V8BXjGzzYDuwCgzmwMMBB5x95WVn2NmJcDdwCHAXGCCmQ1z9+lJ57QBLgf2dfdFZrb1el9RFux97Si+/mFFxX3NMpLYvftuaB1MnQrdu8Ptt8NWW8UdleShtMcUzGxL4BTgdGAScDuwBzCqiqfsBcx091nuvgJ4HOhW6ZwzgLvdfRGAuy+oVfQxqJwQAL7+YQV7X1vVH4NIhJYuDbOKOneGRYtCy+C//1VCkDpLKymY2TPAGGBj4A/u3tXdn3D3c4FNq3hac2BO0v25iWPJdgJ2MrO3zWycmXWp4v17mlmZmZUtXLgwnZAjUzkh1HRcJDKjR4eB5H79QqmKadPgiCPijkryXLpTUge5+1oFf8ysgbsvd/fSKp6TaopO5bmcGwJtgAOA7YAxZtbO3Rev9ST3AcAAgNLS0tjmg6o1IDlh8eKwLeagQbDjjmFHtN/+Nu6opECk2310TYpjY2t4zlwgeYRrO2B+inOec/eV7v4ZMIOQJHJOqm4jkax77rlQouKBB+Dvfw+b4SghSAZV21Iws18QunwamllH/vftvzGhK6k6E4A2ZtYamAccR5jBlGwoYeD6QTNrSuhOmlWrK8gSJQSJ1YIFcN558MQTsPvuMGwYlFbVSBepu5q6jw4lDC5vB/RLOv4D8I/qnujuq8ysFzASKAEecPdpZtYHKHP3YYnH/s/MpgOrgUvc/ds6XUmEhk6aF3cIUqzc4ZFH4IILwr4H11wTWgj16sUdmRSoapOCuw8GBpvZ0e4+pLYvnhiHGF7p2JVJtx24KPGTs24aOaPGc0q0XaFk2hdfhBIVL70E++wTxhB23TXuqKTA1dR91MPdHwFamdk6H9zu3i/F0wrOvMXLajyn+95aICQZsmZNWIV82WWhpXD77XDOOVBSEndkUgRq6j7aJPG7qmmnAvTo1JJrjmwfdxhSCGbMCPskv/VW2ADnvvtg++3jjkqKSE3dR/clbvZ393gXCOQwJQRZbytXwk03hTIVG28MgwfDiSeCuiUly9Jdp/COmX0GPAE8U74CWVQVVTLgvffgL3+BDz6AP/0pbIvZrFncUUmRSmudgru3Aa4AdgMmmtkLZtYj0shyhBasSWSWLQvjBnvtFaacPvssPPmkEoLEKu3aR+7+rrtfRKhp9B0wOLKocoQWrElk3nwTfvWrsAvaKafA9Olw5JFxRyWSdu2jxmZ2spmNAN4BviQkh4KmhCAZ9/33cPbZYRXy6tVhR7RBg6BJk7gjEwHSH1P4gLD6uI+711TeQkRSefFFOOssmD8/7JHcpw9ssknNzxPJonSTwi8TC81EpLYWLgwrkv/7X9htN3j6adh777ijEkmppsVrt7n7BcAwM1snKbh718giywM9OrWMOwTJZe7w+OOhZtGSJXDVVXD55VC/ftyRiVSpppbCw4nfN0cdSL7RgjWp1ty58Ne/wgsvhNlF998P7drFHZVIjaodaHb3iYmbHdz9jeQfoEP04eUuJQRJac2asAq5bVt47bWwAc477yghSN5Id0rqySmOnZLBOETy3yefwEEHhcHkPfeEKVPgwgtVs0jySk1jCt0JeyC0NrNhSQ81AnKuxLVILFatgltvhSuvhAYNQlfRqaeqRIXkpZrGFMrXJDQFbkk6/gMwOaqgRPLGBx/AaafBxIlh8dndd8O228YdlUid1VQQ73Pgc6BzdsLJHdpYR6q1fHnY8KZvX9hiC3jqKTj6aLUOJO/V1H30lrvvZ2Y/AMlTUo2wR07jSKOLydBJ87j8mSlxhyG56p13Quvgo4/g5JPhlltgyy3jjkokI2pqKeyX+N0oO+HkhptGzmDZytVxhyG55scf4R//gLvughYtwo5ohx4ad1QiGZVu7aMdzKxB4vYBZnaemRVssZZ0dlqTIjNyZJhWetdd0KsXTJ2qhCAFKd0pqUOA1Wa2I3A/0Br4b2RRieSK774LVUy7dIGGDWHMGLjjDmhUVI1nKSLpJoU17r4K+CNwm7tfCGwTXVi5rc3WKmJW8NzD4PGuu8Kjj4Zuo0mTYN99445MJFLpFsRbmVizcDLwh8SxetGElNvabL0Joy46IO4wJErz58M558DQobDHHqHrqENRL+CXIpJuS+FUwrTUa939MzNrDTwSXVi5SwmhgLmHhWdt24ZB5BtvhPHjlRCkqKTVUnD36cB5Sfc/A/pGFZRI1s2aBWecEeoV7b9/2PimTZu4oxLJunRnH+1rZqPM7GMzm2Vmn5nZrKiDi8MJA7WHUFFZvTqUqGjXDiZMgHvugdGjlRCkaKU7pnA/cCEwESjYCfwnDBzL259+F3cYki1Tp4ZFaO++C4cfDvfeC9ttF3dUIrFKNykscfcRkUaSA5QQisSKFXD99XDttbDZZmFHtOOOU4kKEdJPCqPN7CbgGWB5+UF3fy+SqGKw97Wj4g5BsmH8+NA6mDYNjj8ebrsNttoq7qhEcka6SaF8Q9nSpGMOHJTZcOKx97Wj+PqHFXGHIVH66Sf45z9DEth2W3j+eTjiiLijEsk56c4+OjDqQOJSm4TQrJH21s1Lr74aZhZ99lnYIrNvX2hckLUcRdZburOPmpnZ/WY2InG/rZmdlsbzupjZDDObaWaXVXPeMWbmZlZa1TlRqG1CGN/7kIgjkoxavBhOPx0OPhg23BDeeAP691dCEKlGuovXHgRGAuW7h3wMXFDdE8ysBLgbOAxoC3Q3s7YpzmtEWAMxPs1YMqY2XUZKCHlm6NCwCO3BB+HSS8NmOPvvH3dUIjkv3aTQ1N2fBNYAJOog1TQ1dS9gprvPcvcVwONAtxTnXQ3cCPycZixZ17iB9tjNG19/DcceC3/8I2y9dRhY7ts3FLMTkRqlmxR+MrMtSWy0Y2adgCU1PKc5MCfp/tzEsQpm1hFo4e4vVPdCZtbTzMrMrGzhwoVphpwZjRuUMPnfXbL6nlIH7jB4cChgN2wYXHddWIz261/HHZlIXkl39tFFwDBgBzN7G9gKOKaG56Sa9F2xe5uZbQDcCpxS05u7+wBgAEBpaanXcHpGzO57eDbeRjJh9mw480x4+eVQxXTQINhll7ijEslL1bYUzGxPM/tFYj3Cb4F/ENYpvEz45l+duUCLpPvbAfOT7jcC2gGvm9lsoBMwLNuDzZLH1qyBO+8MJSreeSdsgPPmm0oIIuuhpu6j+4Dy0dh9gN6EweNFJL65V2MC0MbMWptZfeA4QmsDAHdf4u5N3b2Vu7cCxgFd3b2s9pchRefDD+E3v4Hzzgu/p04N5a43SLdHVERSqel/UIm7l9d++DMwwN2HuPs/gR2re2JiMLoXYdbSh8CT7j7NzPqYWdf1DVyK1MqVoTxFhw7w0Ufw0EMwfDhsv33ckYkUhJrGFErMbMPEB/zvgJ61eC7uPhwYXunYlVWce0BNrydFbuJE+MtfYPJk+POf4fbboVmzuKMSKSg1tRQeA94ws+eAZcAYgMRezTXNPhLJjGXLwlqDvfaCb74JaxAef1wJQSQC1X7bd/drzexVwn7ML7t7+cyfDYBzow5OhDfeCKuSZ84MpSpuvBGaNIk7KpGClU4X0LgUxz6OJhyRhCVLQuvgvvvgl78M9YsOKoj6iyI5TVM1JPe88ALsthsMHAgXXwxTpighiGSJkoLkjoULwx4Hf/gDbLEFjBsHN98MG28cd2QiRUNJQeLnHnY/23VXePpp+Pe/oawM9twz7shEik66ZS5EojFnTtjj4MUXoVOnUKJit93ijkqkaKmlIPFYswbuuSckgNGjw45ob72lhCASM7UUJPs+/jhMMx0zJmyAM2AAtG4dd1QigloKkk2rVsENN8Duu4cZRfffHyqbKiGI5IyibSm0uuzFuEMoLu+/D6edBu+9B0cdFSqabrNN3FGJSCVFlxRqsy+zZMDPP8PVV4cWQtOmYXbR0UfHHZWIVKGokoISQpa99VYYO5gxA049FW65BTbfPO6oRKQaRTWmoISQJT/8AL16hX0Oli8P4wYPPKCEIJIHiiopSBaMGBGmlfbvDxdcEAaUDzkk7qhEJE1F1X2UrsYNSuIOIf98+y1ceCE8/DC0bQtvvw2dO8cdlYjUkloKlTRuUMLkf3eJO4z84Q5PPhkSwWOPwZVXhhlGSggieUkthYR9d9iCR8/QB1mtzJ8PZ58Nzz0HpaUwalRYgyAieUsthQQlhFpwDzWK2rYNg8g33wxjxyohiBQAtRSkdj79NOyANno0HHBA2PNgxx3jjkpEMkQtBUnP6tVhnUH79jBxYqhX9NprSggiBUYtBanZlCmhRMWECdC1a5hu2rx53FGJSATUUpCqLV8O//oX7LEHzJ4NTzwBQ4cqIYgUMLUUJLVx40LrYPp0OPFEuPVW2HLLuKMSkYippSBr++mnsAhtn31CuYrhw+Ghh5QQRIqEWgryP6+8EmYWzZ4dtsjs2xcaN447KhHJIrUUBBYtCl1FhxwC9evDm2+GwWQlBJGio6RQ7J55JixCGzwYLr8cPvggVDcVkaKk7qNi9dVXobz1kCHQoUMYO+jYMe6oRCTZTZFDAAANdUlEQVRmaikUG3d48MHQOnjhBbjuOnj3XSUEEQEiTgpm1sXMZpjZTDO7LMXjF5nZdDObbGavmtn2UcZT9GbPhkMPDbug7bZb6Cq6/HKoVy/uyEQkR0SWFMysBLgbOAxoC3Q3s7aVTpsElLr77sDTwI1RxVPUVq+GO+6Adu1C4br+/eGNN2DnneOOTERyTJQthb2Ame4+y91XAI8D3ZJPcPfR7r40cXccsF2E8RSn6dNhv/3g/PNh//1h2rQw3XQD9RyKyLqi/GRoDsxJuj83cawqpwEjIoynuKxYAVdfHcYKPvkEHnkEXnwRWraMOzIRyWFRzj6yFMc85YlmPYBS4LdVPN4T6AnQUh9qNZswIaw7mDIFjjsObr8dtt467qhEJA9E2VKYC7RIur8dML/ySWZ2MNAb6Oruy1O9kLsPcPdSdy/daqutIgm2ICxdCpdcAp06hT2Tn3subJGphCAiaYoyKUwA2phZazOrDxwHDEs+wcw6AvcREsKCCGMpfKNHh53Pbr4ZTj89jCV07Rp3VCKSZyJLCu6+CugFjAQ+BJ5092lm1sfMyj+tbgI2BZ4ys/fNbFgVLydVWbIEzjwTDjoo3B89Gu67DzbbLN64RCQvRbqi2d2HA8MrHbsy6fbBUb5/wRs2LMwk+uqr0G101VWw8cZxRyUieUzzEvPRggVhALlbt1DSevx4uPFGJQQRWW9KCvnEPUwt3XVXePbZMOW0rAxKS+OOTEQKhAri5Ys5c+Css0Lhus6dYdCgUL9IRCSD1FLIdWvWhLIUbdvC66+HNQdjxighiEgk1FLIZTNmhJ3QxowJG+AMGACtWsUdlYgUMLUUctHKlWErzF/9KqxKfuABGDlSCUFEIqeWQq6ZNCmUqJg0CY45Bu68E37xi7ijEpEiUVQthRJLVY6p6uNZtWxZ2Ntgzz3hyy/DjmhPPaWEICJZVVRJofveLWp1PGvGjAlbYvbtCyefHEpUHHVUvDGJSFEqqqRwzZHt6dGpZUXLoMSMHp1acs2R7eMJ6Pvv4Zxzwj4HK1fCqFFw//2w+ebxxCMiRc/cU1azzlmlpaVeVlYWdxjrb/jwsO5g7ly44IKwEG2TTeKOSkQKlJlNdPcaV7pqoDnbvvkmJIFHHw1rDd55J5S6FhHJAUXVfRQrd3j88VCi4okn4Mor4b33lBBEJKeopZAN8+bB2WeHqqZ77hnGDdrHNI4hIlINtRSitGZNWIXctm0YRL7lFhg7VglBRHKWWgpRmTkzlKh4/XU48EAYOBB22CHuqEREqqWWQqatWhW2xGzfPowZDBwIr76qhCAieUEthUyaPDmUqCgrC/sj9+8PzZvHHZWISNrUUsiE5cvDbKJf/xo+/zzMLho6VAlBRPKOWgrra+zY0Dr48EM46STo1y9skSkikofUUqirH3+E88+HffeFn36CESNg8GAlBBHJa2op1MXLL0PPnvDFF2H9wfXXQ6NGcUclIrLe1FKoje++g1NPhUMPhY02CtVN77pLCUFECoaSQrqGDAmL0B5+GHr3hvffD11HIiIFRN1HNfnyy1De+tlnYY894KWXwt4HIiIFSC2FqrjDf/4TWgcjRoQNcMaPV0IQkYKmlkIqn30WBpJfeSVsgDNwIOy0U9xRiYhETi2FZKtXw223Qbt2oVVwzz0werQSgogUDbUUyk2bFhahjR8Phx8eEkKLmPduFhHJMrUUVqyAPn2gY8dQ2fTRR+H555UQRKQoFXdL4d13Q+tg6lQ4/vjQdbTVVnFHJSISm0hbCmbWxcxmmNlMM7ssxeMNzOyJxOPjzaxVlPFUWLoULr4YOneGRYtCy+DRR5UQRKToRZYUzKwEuBs4DGgLdDeztpVOOw1Y5O47ArcCN0QVT4XXXgt7HfTrF2YYTZsGRxwR+duKiOSDKFsKewEz3X2Wu68AHge6VTqnGzA4cftp4HdmZpFFdMEF8LvfwQYbhB3R7rkHNtsssrcTEck3USaF5sCcpPtzE8dSnuPuq4AlwDplRs2sp5mVmVnZwoUL6x7RDjvA3/8eNsP57W/r/joiIgUqyoHmVN/4vQ7n4O4DgAEApaWl6zyetnPPrfNTRUSKQZQthblA8rzO7YD5VZ1jZhsCmwHfRRiTiIhUI8qkMAFoY2atzaw+cBwwrNI5w4CTE7ePAV5z97q3BEREZL1E1n3k7qvMrBcwEigBHnD3aWbWByhz92HA/cDDZjaT0EI4Lqp4RESkZpEuXnP34cDwSseuTLr9M/CnKGMQEZH0qcyFiIhUUFIQEZEKSgoiIlJBSUFERCpYvs0ANbOFwOfr8RJNgW8yFE6+0DUXvmK7XtA119b27l5j1c+8Swrry8zK3L007jiySddc+IrtekHXHBV1H4mISAUlBRERqVCMSWFA3AHEQNdc+IrtekHXHImiG1MQEZGqFWNLQUREqqCkICIiFQo2KZhZFzObYWYzzeyyFI83MLMnEo+PN7NW2Y8yc9K43ovMbLqZTTazV81s+zjizKSarjnpvGPMzM0s76cvpnPNZnZs4u96mpn9N9sxZloa/7ZbmtloM5uU+Pf9+zjizBQze8DMFpjZ1CoeNzO7I/HnMdnM9shoAO5ecD+EUt2fAr8E6gMfAG0rnXM2cG/i9nHAE3HHHfH1HghsnLj913y+3nSvOXFeI+BNYBxQGnfcWfh7bgNMAjZP3N867rizcM0DgL8mbrcFZscd93pe8/7AHsDUKh7/PTCCsHNlJ2B8Jt+/UFsKewEz3X2Wu68AHge6VTqnGzA4cftp4Hdmlmp70HxQ4/W6+2h3X5q4O46wE14+S+fvGOBq4Ebg52wGF5F0rvkM4G53XwTg7guyHGOmpXPNDjRO3N6MdXd4zCvu/ibV70DZDXjIg3FAEzPbJlPvX6hJoTkwJ+n+3MSxlOe4+ypgCbBlVqLLvHSuN9lphG8a+azGazazjkALd38hm4FFKJ2/552AnczsbTMbZ2ZdshZdNNK55quAHmY2l7B/S6Fvxl7b/++1EukmOzFK9Y2/8tzbdM7JF2lfi5n1AEqB30YaUfSqvWYz2wC4FTglWwFlQTp/zxsSupAOILQGx5hZO3dfHHFsUUnnmrsDD7r7LWbWmbCbYzt3XxN9eLGI9LOrUFsKc4EWSfe3Y90mZcU5ZrYhodlZXZMtl6VzvZjZwUBvoKu7L89SbFGp6ZobAe2A181sNqHvdVieDzan++/6OXdf6e6fATMISSJfpXPNpwFPArj7WGAjQuG4QpXW//e6KtSkMAFoY2atzaw+YSB5WKVzhgEnJ24fA7zmiVGcPFTj9Sa6Uu4jJIR872eGGq7Z3Ze4e1N3b+XurQjjKF3dvSyecDMinX/XQwmTCjCzpoTupFlZjTKz0rnmL4DfAZjZroSksDCrUWbXMOCkxCykTsASd/8yUy9ekN1H7r7KzHoBIwmzFx5w92lm1gcoc/dhwP2EZuZMQgvhuPgiXj9pXu9NwKbAU4nx9C/cvWtsQa+nNK+5oKR5zSOB/zOz6cBq4BJ3/za+qNdPmtd8MTDQzC4kdKOcksdf8DCzxwjdf00T4yT/AuoBuPu9hHGT3wMzgaXAqRl9/zz+sxMRkQwr1O4jERGpAyUFERGpoKQgIiIVlBRERKSCkoKIiFRQUpCCVFOlycQ5vROVRCeb2ftmtneGYxhuZk0St88zsw/N7FEz61pdVdfE+e8kfrcys+MzGZdIdTQlVQqSme0P/EgoHNYuxeOdgX7AAe6+PLHQq767R1JMzcw+Ag5LrDKuzfMOAP7m7kdEEZdIZWopSEFKo9LkNsA35eU+3P2b8oRgZrPN7AYzezfxs2Pi+FZmNsTMJiR+9k0c39TM/mNmUxKtjqOTXqepmd1LKP08zMwuNLNTzOyuxDnNzOxZM/sg8bNP4viPiTj7Ar9JtGQuNLMxZtah/CIShe92z+AfnRQ5JQUpVi8DLczsYzPrb2aVCwR+7+57AXcBtyWO3Q7c6u57AkcDgxLH/0koNdDe3XcHXkt+IXc/i1Cb5kB3v7XS+9wBvOHuvyLU0J9W6fHLgDHu3iHx3EEkivyZ2U5AA3efXIfrF0lJSUGKkrv/CPwa6Emok/OEmZ2SdMpjSb87J24fDNxlZu8T6s80NrNGieN3J732olqEchBwT+J5q919SQ3nPwUcYWb1gL8AD9bivURqVJC1j0QqM7MWwPOJu/e6+73uvhp4nVBJdQqhQOKDiXOSB9vKb28AdHb3ZZVe28hS2XV3X2pmowgbrRxLKIMukjFqKUhRcPc5iS6YDu5+r5ntbGbJJaU7AJ8n3f9z0u+xidsvA73KT0jq2698fPNahPYqYXtUzKzEzBpXevwHQhnwZIMI3U4T3D1fy71LjlJSkIKUqDQ5FtjZzOaa2WmVTtkUGGxhg/vJhL19r0p6vIGZjQfOBy5MHDsPKE0MJk8HzkocvwbY3MymmtkHJEpXp+l84MBES2UisFulxycDqxKD0BcCuPtE4HvgP7V4H5G0aEqqSCUWNuUpdfdv4o4lFTPbltDttUsB7y4mMVFLQSSPmNlJwHigtxKCREEtBRERqaCWgoiIVFBSEBGRCkoKIiJSQUlBREQqKCmIiEiF/wcDHJCa3QC/SAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sens,Spec= plot_roccurve(testx,testy.values,coeff,num_points=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton's method yields same solution in 5 iterations \n",
    "LossNewton=[]\n",
    "def NewtonOptim(trainx,trainy,iters=10):\n",
    "    n = trainx.shape[1]\n",
    "    coeff= np.zeros((n,1))\n",
    "    m=trainx.shape[0]\n",
    "    trainy=trainy.reshape(m,1)\n",
    "    for iterations in range(10):\n",
    "        prediction= sigmoid(trainx.dot(coeff))\n",
    "        loss= -np.sum(trainy*np.log(prediction)+(1-trainy)*np.log(1-prediction))/m\n",
    "        vartrainy = prediction*(1-prediction).reshape(m,1)\n",
    "        vartrainy = np.eye(m)*vartrainy\n",
    "        hess = -(trainx.T).dot(vartrainy.dot(trainx))\n",
    "        coeff = coeff-np.linalg.inv(hess).dot((trainx.T).dot(trainy-prediction))\n",
    "        LossNewton.append(loss)\n",
    "        print(loss)\n",
    "    return coeff,LossNewton,hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454\n",
      "0.44631037910018445\n",
      "0.4279026153391865\n",
      "0.42688057342834207\n",
      "0.4268742767717537\n",
      "0.42687427615113305\n",
      "0.42687427615113305\n",
      "0.42687427615113305\n",
      "0.42687427615113305\n",
      "0.42687427615113305\n"
     ]
    }
   ],
   "source": [
    "NewCoeff,NewtonLoss,hess=NewtonOptim(trainx,trainy.values,iters=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pvalue(trainx,coeff,hess):\n",
    "    m=trainx.shape[0]\n",
    "    n=trainx.shape[1]\n",
    "    tstatistic= coeff/np.sqrt(np.diag(np.linalg.inv(-hess))).reshape(n,1)\n",
    "    df=m-n\n",
    "    pvalue = [2*(1-stats.t.cdf(tstat,df)) if tstat>0 else 2*stats.t.cdf(tstat,df) for tstat in tstatistic]\n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      True\n",
       "2      True\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7      True\n",
       "8      True\n",
       "9     False\n",
       "10     True\n",
       "11     True\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue=compute_pvalue(trainx,coeff,hess)\n",
    "\n",
    "#important features\n",
    "(pd.Series(pvalue) < 0.05)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'SibSp', 'Sex_male', 'Cabin_B', 'Cabin_D', 'Cabin_E',\n",
      "       'Embarked_S'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "important_features=data.columns[(pd.Series(pvalue) < 0.05)[1:]]\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
