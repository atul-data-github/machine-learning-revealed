{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing mnist using keras and reshaping and using first 1000 examples for train and test  \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train= X_train/255\n",
    "X_test = X_test/255\n",
    "temp=X_train.reshape(60000,28*28).T\n",
    "temp=temp.reshape(28,28,1,60000)\n",
    "temp= temp[:,:,:,:1000]\n",
    "ytemp = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test=X_test.reshape(10000,28*28).T\n",
    "temp_test=temp_test.reshape(28,28,1,10000)\n",
    "temp_test= temp_test[:,:,:,:1000]\n",
    "ytemp_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(feature_map):\n",
    "    ''' input: 4-d feature_map (H,W,C,m)\n",
    "        output: flattened vector (H*W*C,m) and shape'''\n",
    "    shape_x=feature_map.shape[0]\n",
    "    shape_y=feature_map.shape[1]\n",
    "    shape_z=feature_map.shape[2]\n",
    "    m=feature_map.shape[3]\n",
    "    flattened_vec = feature_map.reshape(shape_x*shape_y*shape_z,m)\n",
    "    return flattened_vec,shape_x,shape_y,shape_z,m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    y= np.maximum(0,x)\n",
    "    return y\n",
    "\n",
    "def relugradient(dact,act):\n",
    "    dhidden = dact\n",
    "    dhidden[act<=0]=0\n",
    "    return dhidden\n",
    "\n",
    "def softmax(x):\n",
    "    softm = np.exp(x)/np.sum(np.exp(x),axis=1,keepdims=True)\n",
    "    return softm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer_forward(feature_map,filtermat,pad=0,stride=1):\n",
    "    ''' input: feature_map (H,W,C,m), filtermat (h,w,C,num_filters),pad, stride\n",
    "        output: Convolved feature_map (output_H,Output_W,num_filters,m)'''     \n",
    "    pad = int(pad)\n",
    "    if (pad is not 0):\n",
    "        feature_map = np.pad(feature_map, ((pad,pad),(pad,pad),(0,0),(0,0)), 'constant')\n",
    "    n = int(feature_map.shape[0])\n",
    "    m = int(feature_map.shape[3])\n",
    "    f = int(filtermat.shape[0])\n",
    "    s = int(stride)\n",
    "    num_channels=int(feature_map.shape[2])\n",
    "    num_filters=filtermat.shape[3]\n",
    "    if (type((n-f)//s) is not int):\n",
    "         print((n-f)//s)\n",
    "         print('invalid padding or stride')\n",
    "         return\n",
    "    output_size = int((n-f)/s) +1\n",
    "    #print(output_size)\n",
    "    feat_map2 = np.zeros((output_size,output_size,num_filters,m))\n",
    "    for k in range(num_filters):\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                feat_map2[i,j,k,:]=np.sum(feature_map[i*s:(i*s+f),j*s:(j*s+f),:,:]*(filtermat[:,:,:,k].reshape(f,f,num_channels,-1)),axis=(0,1,2))\n",
    "                \n",
    "    return feat_map2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_averagepool_forward(feature_map,f=2,stride=2):\n",
    "    ''' input: feature map (H,W,C,m), f:filtersize stride:should be same as f\n",
    "        output: average pooled feature_map (H/f,W/f,C,m)'''       \n",
    "    n = int(feature_map.shape[0])\n",
    "    m = int(feature_map.shape[3])\n",
    "    s = int(stride)\n",
    "    num_filters=int(feature_map.shape[2])\n",
    "    output_size = int((n-f)/s) +1\n",
    "    feat_map2 = np.zeros((output_size,output_size,num_filters,m))\n",
    "    for k in range(num_filters):\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                feat_map2[i,j,k,:]=np.sum(feature_map[i*s:(i*s+f),j*s:(j*s+f),k,:],axis=(0,1))/(f*f)                \n",
    "    return feat_map2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_maxpool_forward(feature_map,f=2,stride=2):\n",
    "    ''' input: feature map (H,W,C,m), f:filtersize stride:should be same as f\n",
    "        output: max pooled feature_map (H/f,W/f,C,m)'''              \n",
    "    n = int(feature_map.shape[0])\n",
    "    m = int(feature_map.shape[3])\n",
    "    s = int(stride)\n",
    "    num_filters=int(feature_map.shape[2])\n",
    "    output_size = int((n-f)/s) +1\n",
    "    feat_map2 = np.zeros((output_size,output_size,num_filters,m))\n",
    "    for k in range(num_filters):\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                feat_map2[i,j,k,:]=np.max(feature_map[i*s:(i*s+f),j*s:(j*s+f),k,:],axis=(0,1))                \n",
    "    return feat_map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dH, myfilter,X,stride=1,pad=0):\n",
    "    ''' input: dH (H,W,C,m) gradient hidden layer myfilter: (h,w,C_prev,C), X: feature map of adjacent layer\n",
    "        output: dfilter: (h,w,k,C) filter gradient, dX: (H_prev,W_prev,C_prev,m)gradient next hidden layer\n",
    "        No bias term because of batchnorm'''  \n",
    "    (f, f, num_channels,num_filters) = myfilter.shape\n",
    "    \n",
    "    (n_H, n_W,_,_) = dH.shape\n",
    "    s=stride\n",
    "    dfilter = np.zeros(myfilter.shape)\n",
    "    dX = np.zeros(X.shape)\n",
    "    m = X.shape[3]\n",
    "    X = np.pad(X, ((pad,pad),(pad,pad),(0,0),(0,0)), 'constant')\n",
    "    dX = np.pad(dX, ((pad,pad),(pad,pad),(0,0),(0,0)), 'constant') \n",
    "\n",
    "    for k in range(num_filters):\n",
    "        shapedfilter=np.tile(myfilter[:,:,:,k].reshape(f,f,num_channels,1),(1,1,1,1,m))[0]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                   dX[h*s:h*s+f, w*s:w*s+f,:,:] += shapedfilter * dH[h,w,k,:]\n",
    "                   dfilter[:,:,:,k] += np.sum(X[h*s:h*s+f, w*s:w*s+f,:,:] * dH[h,w,k,:],axis=3)\n",
    "    dX = dX[pad:-pad,pad:-pad,:,:]\n",
    "    return dfilter,dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, actbeforepool,f):\n",
    "    '''helper function average pool packward: reshape gradient and scale'''\n",
    "    shapebeforepool = actbeforepool.shape\n",
    "    dz_reshape=np.repeat(np.repeat(dz,[f],axis=1),[f],axis=0)\n",
    "    average = dz_reshape / (f * f)\n",
    "    a = np.ones(shapebeforepool) * average    \n",
    "    return a\n",
    "\n",
    "def cnn_averagepool_backward(dfeat_map_pooled,feat_map_act,f=2,stride=2):\n",
    "    '''backwarprop for averagepooling layer'''\n",
    "    a= distribute_value(dfeat_map_pooled,feat_map_act,f)      \n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_maxpool_backward(dA_pooled,A_act,f=2,stride=2):\n",
    "    ''' input: dA_pooled: (H,W,C,m) gradient of pooled layer A_act:(f*H,f*W,C,m) layer before pooling\n",
    "        output: dA_act: (f*H,f*W,C,m)'''\n",
    "    dA_act = np.zeros(A_act.shape)\n",
    "    s = stride\n",
    "    (n_H,n_W,n_C,_)=dA_pooled.shape\n",
    "    dA_pooled_reshape=np.repeat(np.repeat(dA_pooled,[f],axis=1),[f],axis=0)\n",
    "    for c in range(n_C):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                mask = (A_act[h*s:h*s+f,w*s:w*s+f,c,:]==np.max(A_act[h*s:h*s+f,w*s:w*s+f,c,:],axis=(0,1)))\n",
    "                dA_act[h*s:h*s+f,w*s:w*s+f,c,:]=dA_pooled_reshape[h*s:h*s+f,w*s:w*s+f,c,:]*mask                   \n",
    "    return dA_act\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y,y_prob):\n",
    "    loss_train=0\n",
    "    m = y.shape[0]\n",
    "    for i in np.arange(m):\n",
    "        loss_train = loss_train+ (-np.log(y_prob[i,y[i]]))\n",
    "    loss_train = loss_train/m\n",
    "    return loss_train\n",
    "\n",
    "def accuracy_score(y,y_prob):\n",
    "    pred=pd.DataFrame(y_prob).idxmax(axis=1)\n",
    "    return np.mean(pred==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "  ''' input: x: (m,hidden_size),gamma:scale parameter, beta: shift parameter\n",
    "      output: out: (m,hidden_size), cache for backprop'''\n",
    "  N, D = x.shape\n",
    "  #step1: calculate mean\n",
    "  mu = 1./N * np.sum(x, axis = 0)\n",
    "  #step2: subtract mean vector of every trainings example\n",
    "  xmu = x - mu\n",
    "  #step3: following the lower branch - calculation denominator\n",
    "  sq = xmu ** 2\n",
    "  #step4: calculate variance\n",
    "  var = 1./N * np.sum(sq, axis = 0)\n",
    "  #step5: add eps for numerical stability, then sqrt\n",
    "  sqrtvar = np.sqrt(var + eps)\n",
    "  #step6: invert sqrtwar\n",
    "  ivar = 1./sqrtvar\n",
    "  #step7: execute normalization\n",
    "  xhat = xmu * ivar\n",
    "  #step8: Nor the two transformation steps\n",
    "  gammax = gamma * xhat\n",
    "  #step9\n",
    "  out = gammax + beta\n",
    "  #store intermediate\n",
    "  cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "  return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbatchnorm_forward(X, gamma, beta, eps = 1e-5):\n",
    "    ''' input: x: (H,W,C,m),gamma:scale parameter, beta: shift parameter\n",
    "        output: out: (H,W,C,m) cache for backprop'''\n",
    "\n",
    "    H, W, C, N = X.shape\n",
    "        # mini-batch mean\n",
    "    mu = np.mean(X, axis=(0, 1, 3))\n",
    "    xmu = X -mu.reshape(1,1,C,1)\n",
    "    sq = xmu**2\n",
    "        # mini-batch variance\n",
    "    var =(1/(H*W*N)) *np.sum(sq, axis=(0, 1, 3)).reshape((1,1,C,1))\n",
    "    sqrtvar = np.sqrt(var+eps)\n",
    "    ivar = 1./sqrtvar\n",
    "    xhat = xmu * ivar\n",
    "    #print(gamma.shape)\n",
    "    gammax = gamma.reshape(1,1,C,1) * xhat\n",
    "    out = gammax + beta.reshape(1,1,C,1)\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "    return out , cache   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "  ''' input: dout: (m,hidden_size) grdient from backprop, cache: cache for backprop\n",
    "      output: dx: (m,hidden_size)  gradient for input of batchnorm, dgamma: gradient of scale,dbeta: gradient of shift param'''\n",
    "  #unfold the variables stored in cache\n",
    "  xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "  #get the dimensions of the input/output\n",
    "  N,D = dout.shape\n",
    "  #step9\n",
    "  dbeta = np.sum(dout, axis=0)\n",
    "  dgammax = dout #not necessary, but more understandable\n",
    "  #step8\n",
    "  dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "  dxhat = dgammax * gamma\n",
    "  #step7\n",
    "  divar = np.sum(dxhat*xmu, axis=0)\n",
    "  dxmu1 = dxhat * ivar\n",
    "  #step6\n",
    "  dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "  #step5\n",
    "  dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "  #step4\n",
    "  dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "  #step3\n",
    "  dxmu2 = 2 * xmu * dsq\n",
    "  #step2\n",
    "  dx1 = (dxmu1 + dxmu2)\n",
    "  dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "  #step1\n",
    "  dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "  #step0\n",
    "  dx = dx1 + dx2\n",
    "  return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbatchnorm_backward(dout, cache):\n",
    "  ''' input:  dout: (H,W,C,m) grdient from backprop, cache: cache for backprop\n",
    "      output: dx: (H,W,C,m)  gradient for input of batchnorm, dgamma: gradient:(1,1,C,1) of scale\n",
    "              dbeta: (1,1,C,1) gradient of shift param'''\n",
    "  #unfold the variables stored in cache\n",
    "  xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "  #get the dimensions of the input/output\n",
    "  H,W,C,N = dout.shape\n",
    "  #step9\n",
    "  dbeta = np.sum(dout, axis=(0,1,3)).reshape(1,1,C,1)\n",
    "  dgammax = dout #not necessary, but more understandable\n",
    "  #step8\n",
    "  dgamma = np.sum(dgammax*xhat, axis=(0,1,3)).reshape(1,1,C,1)\n",
    "  dxhat = dgammax * gamma.reshape(1,1,C,1)\n",
    "  #step7\n",
    "  divar = np.sum(dxhat*xmu, axis=(0,1,3)).reshape(1,1,C,1)\n",
    "  dxmu1 = dxhat * ivar\n",
    "  #step6\n",
    "  dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "  #step5\n",
    "  dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "  #step4\n",
    "  dsq = (1. /(H*W*N)) * np.ones((H,W,C,N)) * dvar\n",
    "  #step3\n",
    "  dxmu2 = 2 * xmu * dsq\n",
    "  #step2\n",
    "  dx1 = (dxmu1 + dxmu2)\n",
    "  dmu = -1 * np.sum(dxmu1+dxmu2, axis=(0,1,3))\n",
    "  #step1\n",
    "  dx2 = (1. /(H*W*N)) * np.ones((H,W,C,N)) * dmu.reshape(1,1,C,1)\n",
    "  #step0\n",
    "  dx = dx1 + dx2\n",
    "  return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(temp,ytemp,parameters,fc1_neurons,num_class,pad=0):\n",
    "        '''input: temp: (H,W,C,m) training data,ytemp: (m,) digit, fc1_neurons: num_neurons in fc1\n",
    "        output: loss,accuracy,cache for backprop'''\n",
    "        (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)=parameters\n",
    "        feat_map = cnn_layer_forward(temp,myfilter1,pad=pad)\n",
    "        feat_map_bn,conv1_cache = convbatchnorm_forward(feat_map,conv1gam,conv1bet)\n",
    "        feat_map_act = relu(feat_map_bn)\n",
    "        feat_map_pool = cnn_maxpool_forward(feat_map_act,f=2,stride=2)\n",
    "        feat_map2= cnn_layer_forward(feat_map_pool,myfilter2,pad=pad)\n",
    "        feat_map2_bn,conv2_cache = convbatchnorm_forward(feat_map2,conv2gam,conv2bet)\n",
    "        feat_map2_act=relu(feat_map2_bn)\n",
    "        flattened_vec,shape_x,shape_y,shape_z,m = flatten(feat_map2_act)\n",
    "        shape = (shape_x,shape_y,shape_z,m)\n",
    "        fc1 = np.dot(flattened_vec.T,W1)\n",
    "        fc1_bn,fc_cache=batchnorm_forward(fc1, gamma=gam, beta=bet, eps = 1e-5)\n",
    "        fc1_act = relu(fc1_bn)\n",
    "        output = np.dot(fc1_act,W2)+b2\n",
    "        prob   = softmax(output)\n",
    "        loss=calculate_loss(ytemp,prob)\n",
    "        accuracy=accuracy_score(ytemp,prob)\n",
    "        cache =(feat_map_act,feat_map_pool,feat_map2_act,flattened_vec,shape,\n",
    "                fc1_act,prob,myfilter1,myfilter2,W1,W2,conv2_cache,conv1_cache,fc_cache)\n",
    "        \n",
    "        return loss,accuracy,cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate(temp,ytemp,cache,pad=0):\n",
    "        ''' input: temp: (H,W,C,m), ytemp: (m,) digit, cache from forprop\n",
    "            output: gradients for backprop'''\n",
    "        (feat_map_act,feat_map_pool,feat_map2_act,flattened_vec,shape,\n",
    "                fc1_act,prob,myfilter1,myfilter2,W1,W2,conv2_cache,conv1_cache,fc_cache)=cache\n",
    "        shape_x,shape_y,shape_z,m=shape\n",
    "        doutput = np.copy(prob)\n",
    "        m = ytemp.shape[0]\n",
    "        for i in np.arange(m):\n",
    "            doutput[i,ytemp[i]] = doutput[i,ytemp[i]]-1\n",
    "        doutput = doutput/m\n",
    "        dW2 = np.dot(fc1_act.T,doutput)\n",
    "        db2 = np.sum(doutput, axis=0, keepdims=True)\n",
    "        dfc1_act = np.dot(doutput,W2.T)\n",
    "        dfc1_bn = relugradient(dfc1_act,fc1_act)\n",
    "        dfc1,dgam,dbet = batchnorm_backward(dfc1_bn, fc_cache)\n",
    "        dW1 = np.dot(flattened_vec,dfc1)\n",
    "        dflattened_vec = np.dot(dfc1,W1.T).T\n",
    "        dfeat_map2_act=dflattened_vec.reshape(shape_x,shape_y,shape_z,m)\n",
    "        dfeat_map2_bn = relugradient(dfeat_map2_act,feat_map2_act)   \n",
    "        dfeat_map2,dconv2gam,dconv2bet = convbatchnorm_backward(dfeat_map2_bn,conv2_cache)\n",
    "        dfilt2,dfeat_map_pooled = conv_backward(dfeat_map2,myfilter2,feat_map_pool,pad=pad)\n",
    "        dfeat_map_act=cnn_maxpool_backward(dfeat_map_pooled,feat_map_act,f=2,stride=2)\n",
    "        dfeat_map_bn = relugradient(dfeat_map_act,feat_map_act)\n",
    "        dfeat_map,dconv1gam,dconv1bet = convbatchnorm_backward(dfeat_map_bn,conv1_cache)\n",
    "        dfilt1,dX = conv_backward(dfeat_map,myfilter1,temp,pad=pad)        \n",
    "        gradients =(dfilt1,dfilt2,dW1,dW2,db2,dconv1gam,dconv2gam,dconv1bet,dconv2bet,dgam,dbet)\n",
    "        return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tell_size(temp,ytemp,myfilter1,myfilter2,pad=0):\n",
    "        ''' function to find size of neurons in first fc layer'''\n",
    "        feat_map = cnn_layer_forward(temp,myfilter1,pad=pad)\n",
    "        feat_map_act = relu(feat_map)\n",
    "        feat_map_pool = cnn_averagepool_forward(feat_map_act,f=2,stride=2)\n",
    "        feat_map2= cnn_layer_forward(feat_map_pool,myfilter2,pad=pad)\n",
    "        feat_map2_act=relu(feat_map2)\n",
    "        flattened_vec,shape_x,shape_y,shape_z,m = flatten(feat_map2_act)\n",
    "        return flattened_vec.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_optimizer(parameters,gradients,alpha):\n",
    "        ''' input: parameters, gradients, alpha: learning_rate\n",
    "        output: updated parameters'''\n",
    "        (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)=parameters\n",
    "        (dfilt1,dfilt2,dW1,dW2,db2,dconv1gam,dconv2gam,dconv1bet,dconv2bet,dgam,dbet)=gradients\n",
    "        myfilter1 = myfilter1 -alpha*dfilt1\n",
    "        myfilter2 = myfilter2 -alpha*dfilt2\n",
    "        W1 = W1 - alpha * dW1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "        conv1gam = conv1gam -alpha * dconv1gam\n",
    "        conv1bet = conv1bet -alpha * dconv1bet\n",
    "        conv2gam = conv2gam -alpha * dconv2gam\n",
    "        conv2bet = conv2bet -alpha * dconv2bet        \n",
    "        gam = gam - alpha * dgam\n",
    "        bet = bet - alpha * dbet\n",
    "        updated_parameters = (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)\n",
    "        return updated_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(temp,ytemp,myfilter1,myfilter2,fc1_neurons,num_class,alpha=0.05,iterations=50,pad=0):\n",
    "    ''' input: temp: training data, ytemp: digits,myfilter1,myfilter2:filters,fc1_neurons:num_neurons in fc layer,num_class: number of class\n",
    "        output: Loss and Accuracy list'''  \n",
    "    Loss=[]\n",
    "    Accuracy=[]\n",
    "    alpha=alpha\n",
    "    iterations=iterations\n",
    "    gam=1\n",
    "    bet=0\n",
    "    conv1gam=np.ones(myfilter1.shape[3]).reshape(1,1,myfilter1.shape[3],1)\n",
    "    conv2gam=np.ones(myfilter2.shape[3]).reshape(1,1,myfilter2.shape[3],1)\n",
    "    conv1bet=np.zeros(myfilter1.shape[3]).reshape(1,1,myfilter1.shape[3],1)\n",
    "    conv2bet=np.zeros(myfilter2.shape[3]).reshape(1,1,myfilter2.shape[3],1)\n",
    "    size=tell_size(temp,ytemp,myfilter1,myfilter2,pad=pad)\n",
    "    W1 = np.random.normal(0,1,(size,fc1_neurons))*np.sqrt(2/size)\n",
    "    W2 = np.random.normal(0,1,(fc1_neurons,num_class))*np.sqrt(2/fc1_neurons)\n",
    "    b2 = np.zeros((1,num_class))       \n",
    "    parameters = (myfilter1,myfilter2,W1,W2,b2,conv1gam,conv2gam,conv1bet,conv2bet,gam,bet)\n",
    "    for i in range(iterations):\n",
    "        loss,accuracy,cache_train=forward_propagate(temp,ytemp,parameters,fc1_neurons,num_class,pad=pad)\n",
    "        Loss.append(loss)\n",
    "        Accuracy.append(accuracy)\n",
    "        print(str(i)+\": Loss:\"+str(loss)+\" Accuracy: \"+str(accuracy))\n",
    "        gradients=backward_propagate(temp,ytemp,cache_train,pad=pad)\n",
    "        parameters=gradient_descent_optimizer(parameters,gradients,alpha)\n",
    "    return Loss,Accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Loss:2.5793182118416036 Accuracy: 0.13\n",
      "1: Loss:1.4036494247815845 Accuracy: 0.56\n",
      "2: Loss:1.039193162529265 Accuracy: 0.772\n",
      "3: Loss:0.871091936306385 Accuracy: 0.825\n",
      "4: Loss:0.752256807764434 Accuracy: 0.854\n",
      "5: Loss:0.6532967958576497 Accuracy: 0.887\n",
      "6: Loss:0.5957505812263014 Accuracy: 0.887\n",
      "7: Loss:0.5413248942995825 Accuracy: 0.899\n",
      "8: Loss:0.5195867963441035 Accuracy: 0.902\n",
      "9: Loss:0.48320549439219707 Accuracy: 0.905\n",
      "10: Loss:0.3928034356053484 Accuracy: 0.936\n",
      "11: Loss:0.3436564445639857 Accuracy: 0.941\n",
      "12: Loss:0.3128217437315367 Accuracy: 0.952\n",
      "13: Loss:0.2977734456207918 Accuracy: 0.952\n",
      "14: Loss:0.25852697746703596 Accuracy: 0.967\n",
      "15: Loss:0.2302877990880721 Accuracy: 0.966\n",
      "16: Loss:0.20194231445402008 Accuracy: 0.977\n",
      "17: Loss:0.1821095204703444 Accuracy: 0.976\n",
      "18: Loss:0.1622351569345543 Accuracy: 0.985\n",
      "19: Loss:0.1476346902773255 Accuracy: 0.988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe4252fb278>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myfilter1 = np.random.normal(0,0.1,9*16).reshape(3,3,1,16)\n",
    "myfilter2 = np.random.normal(0,0.1,9*16*18).reshape(3,3,16,18)\n",
    "lossbn,accuracybn, parameters =train_CNN(temp,ytemp,myfilter1,myfilter2,fc1_neurons=20,num_class=10,alpha=0.5,iterations=20,pad=1)    \n",
    "pd.Series(lossbn).plot()    \n",
    "pd.Series(accuracybn).plot()\n",
    "#batch norm can support very higher learning rate without divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy,_ = forward_propagate(temp_test,ytemp_test,parameters,fc1_neurons=20,num_class=10,pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3986668663961918, 0.89)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
